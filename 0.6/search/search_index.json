{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Iter8 \u00b6","title":"Home"},{"location":"#iter8","text":"","title":"Iter8"},{"location":"news/","text":"News and Announcements \u00b6 Iter8 at KubeCon + CloudNativeCon Europe, May 6, 2021 Iter8 at Knative meetup, Mar 24, 2021 Kubeflow blog article by Animesh Singh and Dan Sun: Operationalize, Scale and Infuse Trust in AI Models using KFServing , Mar 8, 2021 Medium blog article by Michael Kalantar: Automated Canary Release of Microservices on Kubernetes using Tekton and iter8 , Oct 26, 2020 Medium blog article by Kusuma Chalasani: Better Performance with kruize and iter8 for your microservices application , Oct 12, 2020 Medium blog article by Srinivasan Parthasarathy: Automated Canary Release of TensorFlow Models on Kubernetes , Oct 5, 2020 Medium blog article by Sushma Ravichandran: Iter8: Take a look at the magic under the hood , Oct 1, 2020 Medium blog article by Fabio Oliveira: Iter8: Achieving Agility with Control , Aug 17 th , 2020","title":"News"},{"location":"news/#news-and-announcements","text":"Iter8 at KubeCon + CloudNativeCon Europe, May 6, 2021 Iter8 at Knative meetup, Mar 24, 2021 Kubeflow blog article by Animesh Singh and Dan Sun: Operationalize, Scale and Infuse Trust in AI Models using KFServing , Mar 8, 2021 Medium blog article by Michael Kalantar: Automated Canary Release of Microservices on Kubernetes using Tekton and iter8 , Oct 26, 2020 Medium blog article by Kusuma Chalasani: Better Performance with kruize and iter8 for your microservices application , Oct 12, 2020 Medium blog article by Srinivasan Parthasarathy: Automated Canary Release of TensorFlow Models on Kubernetes , Oct 5, 2020 Medium blog article by Sushma Ravichandran: Iter8: Take a look at the magic under the hood , Oct 1, 2020 Medium blog article by Fabio Oliveira: Iter8: Achieving Agility with Control , Aug 17 th , 2020","title":"News and Announcements"},{"location":"roadmap/","text":"Roadmap \u00b6 Enhanced experiments A/B/n, and Pareto testing patterns with single and multiple reward metrics Blue/green deployment pattern Experiments with support and confidence Metrics Support for more metric providers like MySQL, PostgreSQL, CouchDB, MongoDB, and Google Analytics. Enhanced MLOps experiments Customized experiments/metrics for serving frameworks like TorchServe and TFServing GitOps Integration with ArgoCD, Flux and other GitOps operators Notifications Integration with Slack, GitHub, and other RESTful services Enhancing Kubernetes and OpenShift integration Support for OpenShift Serverless Enhanced Knative metrics in tutorials using OpenTelemetry collector Support for Ambassador and Kong networking layers in KNative Support for experimenting with configuration and routes in Knative Git triggered workflows and CI/CD Integration with GitHub Actions and other pipeline providers Improved installation Iter8 Helm chart, Iter8 Operator","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Enhanced experiments A/B/n, and Pareto testing patterns with single and multiple reward metrics Blue/green deployment pattern Experiments with support and confidence Metrics Support for more metric providers like MySQL, PostgreSQL, CouchDB, MongoDB, and Google Analytics. Enhanced MLOps experiments Customized experiments/metrics for serving frameworks like TorchServe and TFServing GitOps Integration with ArgoCD, Flux and other GitOps operators Notifications Integration with Slack, GitHub, and other RESTful services Enhancing Kubernetes and OpenShift integration Support for OpenShift Serverless Enhanced Knative metrics in tutorials using OpenTelemetry collector Support for Ambassador and Kong networking layers in KNative Support for experimenting with configuration and routes in Knative Git triggered workflows and CI/CD Integration with GitHub Actions and other pipeline providers Improved installation Iter8 Helm chart, Iter8 Operator","title":"Roadmap"},{"location":"concepts/buildingblocks/","text":"Building Blocks \u00b6 Iter8 defines a Kubernetes resource called Experiment that automates SLO validation, A/B, and A/B/n testing experiments. During an experiment, Iter8 can compare multiple versions, find, and safely promote the winning version (winner) based on business metrics and performance metrics like latency and error-rate. We now introduce the building blocks of an Iter8 experiment. Objectives \u00b6 Objectives correspond to service-level objectives or SLOs. In Iter8 experiments, objectives are specified as metrics along with acceptable limits on their values. Iter8 will report how versions are performing with respect to these metrics and whether or not they satisfy the objectives. An example of an objective is as follows: the 99 th -percentile tail latency of the version should be under 50 msec. Reward \u00b6 Reward typically corresponds to a business metric which you wish to optimize during an experiment. In Iter8 experiments, reward is specified as a metrics along a preferred direction, which could be high or low . Examples of reward includes user-engagement, conversion rate, click-through rate, revenue, precision, recall, and accuracy (for ML models), all of which have a preferred direction high . The number of GPU cores consumed by an ML model version is an example of a reward with preferred direction low . Validation \u00b6 A version of your app/ML model is considered validated , if it satisfies the objectives specified in the experiment. Testing pattern \u00b6 Testing pattern defines the number of versions involved in the experiment (1, 2, or more), and determines how the winner is identified. Iter8 supports A/B , A/B/n , canary and conformance testing patterns. A/B A/B testing involves a baseline version, a candidate version, a reward metric, and objectives (optional). If both versions are validated, the version which optimizes the reward is the winner. If only a single version is validated, this version is the winner. If no version is validated, then there is no winner. A/B/n A/B/n testing involves a baseline version, two or more candidate versions, a reward metric, and objectives (optional). The winner of the experiment is the version which optimizes the reward among the subset of versions that are validated. If no version is validated, then there is no winner. Canary Canary testing involves a baseline version, a candidate version, and objectives. If the candidate is validated, then candidate is the winner; else, if baseline is validated, then baseline is the winner. If no version is validated, then there is no winner. Conformance Conformance testing involves a single version, a baseline. If it is validated, then baseline is the winner; else, there is no winner. Deployment pattern \u00b6 Deployment pattern determines how traffic is split between versions. Iter8 supports progressive and fixed-split deployment patterns. Progressive Progressive deployment incrementally shifts traffic towards the winner over multiple iterations. Fixed-split Fixed-split deployment does not shift traffic between versions. Traffic engineering \u00b6 Traffic engineering refers to features such as dark launch, traffic mirroring/shadowing, user segmentation and session affinity that provide fine-grained controls over how traffic is routed to and from app versions. Iter8 enables you to take total advantage of all the traffic engineering features available in the service mesh, ingress technology, or networking layer present in your Kubernetes cluster. Dark launch Dark launch enables you to deploy and experiment with a new version of your application/ML model in such a way that it is hidden from all (or most) of your end-users. Traffic mirroring/shadowing Traffic mirroring or shadowing enables experimenting with a dark launched version with zero-impact on end-users. Mirrored traffic is a replica of the real user requests 1 that is routed to the dark version. Metrics are collected and evaluated for the dark version, but responses from the dark version are ignored. User segmentation User segmentation is the ability to carve out a specific segment of users for an experiment, leaving the rest of the users unaffected by the experiment. Service meshes and ingress controllers often provide the ability to route requests dynamically to different versions based on request attributes such as user identity, URI, IP address prefixes, or origin. Iter8 can leverage this functionality in experiments to control the segment of the users that will participate in the experiment. For example, in the canary experiment depicted below, requests from the country Wakanda may be routed to baseline or candidate; requests that are not from Wakanda will not participate in the experiment and are routed only to the baseline. Session affinity During A/B or canary testing experiments, it is often necessary to ensure that the version to which a particular user's request is routed remains consistent throughout the duration of the experiment. This traffic engineering feature is called session affinity or session stickiness . Service meshes and ingress controllers can enable this feature based on HTTP cookies or request attributes such as user identity, URI, IP address prefixes, or origin. Iter8 can leverage this functionality in experiments to control how user requests are routed to versions. For example, in the A/B testing experiment depicted below, requests from user group 1 are always routed to the baseline while requests from user group 2 are always routed to the candidate during the experiment. Version promotion \u00b6 When two or more versions participate in an experiment, Iter8 recommends a version for promotion ; if the experiment yielded a winner, then the version recommended for promotion is the winner; otherwise, the version recommended for promotion is the baseline version of your app/ML model. Iter8 can promote the recommended version at the end of an experiment. It is possible to mirror only a certain percentage of the requests instead of all requests. \u21a9","title":"Experiment building blocks"},{"location":"concepts/buildingblocks/#building-blocks","text":"Iter8 defines a Kubernetes resource called Experiment that automates SLO validation, A/B, and A/B/n testing experiments. During an experiment, Iter8 can compare multiple versions, find, and safely promote the winning version (winner) based on business metrics and performance metrics like latency and error-rate. We now introduce the building blocks of an Iter8 experiment.","title":"Building Blocks"},{"location":"concepts/buildingblocks/#objectives","text":"Objectives correspond to service-level objectives or SLOs. In Iter8 experiments, objectives are specified as metrics along with acceptable limits on their values. Iter8 will report how versions are performing with respect to these metrics and whether or not they satisfy the objectives. An example of an objective is as follows: the 99 th -percentile tail latency of the version should be under 50 msec.","title":"Objectives"},{"location":"concepts/buildingblocks/#reward","text":"Reward typically corresponds to a business metric which you wish to optimize during an experiment. In Iter8 experiments, reward is specified as a metrics along a preferred direction, which could be high or low . Examples of reward includes user-engagement, conversion rate, click-through rate, revenue, precision, recall, and accuracy (for ML models), all of which have a preferred direction high . The number of GPU cores consumed by an ML model version is an example of a reward with preferred direction low .","title":"Reward"},{"location":"concepts/buildingblocks/#validation","text":"A version of your app/ML model is considered validated , if it satisfies the objectives specified in the experiment.","title":"Validation"},{"location":"concepts/buildingblocks/#testing-pattern","text":"Testing pattern defines the number of versions involved in the experiment (1, 2, or more), and determines how the winner is identified. Iter8 supports A/B , A/B/n , canary and conformance testing patterns. A/B A/B testing involves a baseline version, a candidate version, a reward metric, and objectives (optional). If both versions are validated, the version which optimizes the reward is the winner. If only a single version is validated, this version is the winner. If no version is validated, then there is no winner. A/B/n A/B/n testing involves a baseline version, two or more candidate versions, a reward metric, and objectives (optional). The winner of the experiment is the version which optimizes the reward among the subset of versions that are validated. If no version is validated, then there is no winner. Canary Canary testing involves a baseline version, a candidate version, and objectives. If the candidate is validated, then candidate is the winner; else, if baseline is validated, then baseline is the winner. If no version is validated, then there is no winner. Conformance Conformance testing involves a single version, a baseline. If it is validated, then baseline is the winner; else, there is no winner.","title":"Testing pattern"},{"location":"concepts/buildingblocks/#deployment-pattern","text":"Deployment pattern determines how traffic is split between versions. Iter8 supports progressive and fixed-split deployment patterns. Progressive Progressive deployment incrementally shifts traffic towards the winner over multiple iterations. Fixed-split Fixed-split deployment does not shift traffic between versions.","title":"Deployment pattern"},{"location":"concepts/buildingblocks/#traffic-engineering","text":"Traffic engineering refers to features such as dark launch, traffic mirroring/shadowing, user segmentation and session affinity that provide fine-grained controls over how traffic is routed to and from app versions. Iter8 enables you to take total advantage of all the traffic engineering features available in the service mesh, ingress technology, or networking layer present in your Kubernetes cluster. Dark launch Dark launch enables you to deploy and experiment with a new version of your application/ML model in such a way that it is hidden from all (or most) of your end-users. Traffic mirroring/shadowing Traffic mirroring or shadowing enables experimenting with a dark launched version with zero-impact on end-users. Mirrored traffic is a replica of the real user requests 1 that is routed to the dark version. Metrics are collected and evaluated for the dark version, but responses from the dark version are ignored. User segmentation User segmentation is the ability to carve out a specific segment of users for an experiment, leaving the rest of the users unaffected by the experiment. Service meshes and ingress controllers often provide the ability to route requests dynamically to different versions based on request attributes such as user identity, URI, IP address prefixes, or origin. Iter8 can leverage this functionality in experiments to control the segment of the users that will participate in the experiment. For example, in the canary experiment depicted below, requests from the country Wakanda may be routed to baseline or candidate; requests that are not from Wakanda will not participate in the experiment and are routed only to the baseline. Session affinity During A/B or canary testing experiments, it is often necessary to ensure that the version to which a particular user's request is routed remains consistent throughout the duration of the experiment. This traffic engineering feature is called session affinity or session stickiness . Service meshes and ingress controllers can enable this feature based on HTTP cookies or request attributes such as user identity, URI, IP address prefixes, or origin. Iter8 can leverage this functionality in experiments to control how user requests are routed to versions. For example, in the A/B testing experiment depicted below, requests from user group 1 are always routed to the baseline while requests from user group 2 are always routed to the candidate during the experiment.","title":"Traffic engineering"},{"location":"concepts/buildingblocks/#version-promotion","text":"When two or more versions participate in an experiment, Iter8 recommends a version for promotion ; if the experiment yielded a winner, then the version recommended for promotion is the winner; otherwise, the version recommended for promotion is the baseline version of your app/ML model. Iter8 can promote the recommended version at the end of an experiment. It is possible to mirror only a certain percentage of the requests instead of all requests. \u21a9","title":"Version promotion"},{"location":"concepts/whatisiter8/","text":"What is Iter8? \u00b6 Iter8 is an AI-powered platform for cloud native release automation and experimentation platform that enables SLO validation, A/B testing and progressive delivery. Iter8 makes it easy to unlock business value and guarantee SLOs by identifying the best performing version of your app/ML model and promoting it safely. Iter8 is designed for DevOps and MLOps teams interested in maximizing release velocity and business value with their apps/ML models while protecting end-user experience. What is an Iter8 experiment? \u00b6 Iter8 defines a Kubernetes resource called Experiment that automates SLO validation, A/B, and A/B/n testing experiments. During an experiment, Iter8 can compare multiple versions, find, and safely promote the winning version (winner) based on business metrics and performance metrics like latency and error-rate. How does Iter8 work? \u00b6 Iter8 consists of a Go-based Kubernetes controller that orchestrates (reconciles) experiments in conjunction with a Python-based analytics service , and a Go-based task runner .","title":"What is Iter8?"},{"location":"concepts/whatisiter8/#what-is-iter8","text":"Iter8 is an AI-powered platform for cloud native release automation and experimentation platform that enables SLO validation, A/B testing and progressive delivery. Iter8 makes it easy to unlock business value and guarantee SLOs by identifying the best performing version of your app/ML model and promoting it safely. Iter8 is designed for DevOps and MLOps teams interested in maximizing release velocity and business value with their apps/ML models while protecting end-user experience.","title":"What is Iter8?"},{"location":"concepts/whatisiter8/#what-is-an-iter8-experiment","text":"Iter8 defines a Kubernetes resource called Experiment that automates SLO validation, A/B, and A/B/n testing experiments. During an experiment, Iter8 can compare multiple versions, find, and safely promote the winning version (winner) based on business metrics and performance metrics like latency and error-rate.","title":"What is an Iter8 experiment?"},{"location":"concepts/whatisiter8/#how-does-iter8-work","text":"Iter8 consists of a Go-based Kubernetes controller that orchestrates (reconciles) experiments in conjunction with a Python-based analytics service , and a Go-based task runner .","title":"How does Iter8 work?"},{"location":"contributing/analytics/","text":"Extending Iter8's Analytics Functions \u00b6 Iter8's analytics functions are implemented in the iter8-analytics repo . Python virtual environment \u00b6 Use a Python 3+ virtual environment to locally develop iter8-analytics . You can create and activate a virtual environment as follows. git clone git@github.com:iter8-tools/iter8-analytics.git cd iter8-analytics python3 -m venv .venv source .venv/bin/activate Running iter8-analytics locally \u00b6 Create and activate a Python 3+ virtual environment as described above. The following instructions have been verified in a Python 3.9 virtual environment. Run them from the root folder of your iter8-analytics local repo. 1. pip install -r requirements.txt 2. pip install -e . 3. cd iter8_analytics 4. python fastapi_app.py Navigate to http://localhost:8080/docs on your browser. You can interact with the iter8-analytics service and read its API documentation here. The iter8-analytics APIs are intended to work with metric databases, and use Kubernetes secrets for obtaining the required authentication information for querying the metric DBs. Running unit tests for iter8-analytics locally \u00b6 1. pip install -r requirements.txt 2. pip install -r test-requirements.txt 3. pip install -e . 4. coverage run --source=iter8_analytics --omit=\"*/__init__.py\" -m pytest You can see the coverage report by opening htmlcov/index.html in your browser.","title":"Analytics"},{"location":"contributing/analytics/#extending-iter8s-analytics-functions","text":"Iter8's analytics functions are implemented in the iter8-analytics repo .","title":"Extending Iter8's Analytics Functions"},{"location":"contributing/analytics/#python-virtual-environment","text":"Use a Python 3+ virtual environment to locally develop iter8-analytics . You can create and activate a virtual environment as follows. git clone git@github.com:iter8-tools/iter8-analytics.git cd iter8-analytics python3 -m venv .venv source .venv/bin/activate","title":"Python virtual environment"},{"location":"contributing/analytics/#running-iter8-analytics-locally","text":"Create and activate a Python 3+ virtual environment as described above. The following instructions have been verified in a Python 3.9 virtual environment. Run them from the root folder of your iter8-analytics local repo. 1. pip install -r requirements.txt 2. pip install -e . 3. cd iter8_analytics 4. python fastapi_app.py Navigate to http://localhost:8080/docs on your browser. You can interact with the iter8-analytics service and read its API documentation here. The iter8-analytics APIs are intended to work with metric databases, and use Kubernetes secrets for obtaining the required authentication information for querying the metric DBs.","title":"Running iter8-analytics locally"},{"location":"contributing/analytics/#running-unit-tests-for-iter8-analytics-locally","text":"1. pip install -r requirements.txt 2. pip install -r test-requirements.txt 3. pip install -e . 4. coverage run --source=iter8_analytics --omit=\"*/__init__.py\" -m pytest You can see the coverage report by opening htmlcov/index.html in your browser.","title":"Running unit tests for iter8-analytics locally"},{"location":"contributing/newk8sstack/","text":"Add a K8s Stack / Service Mesh / Ingress \u00b6 Performing Iter8 experiments requires RBAC rules, which are contained in this Kustomize folder and are installed as part of the Iter8 installation. Enable Iter8 experiments over a new K8s stack by extending these RBAC rules. Step 1: Fork Iter8 \u00b6 Fork the Iter8 GitHub repo . Locally clone your forked repo. For the rest of this document, $ITER8 will refer to the root of your local Iter8 repo. Step 2: Edit kustomization.yaml \u00b6 cd $ITER8 /install/core/rbac/stacks Edit kustomization.yaml to add your K8s stack. At the time of writing, it contains the following stacks: resources : - iter8-knative - iter8-istio - iter8-kfserving - iter8-seldon # -iter8-<your stack> # add your stack here Step 3: Create subfolder \u00b6 mkdir iter8-<your stack> cp iter8-kfserving/kustomization.yaml iter8-<your stack>/kustomization.yaml Step 4: Create RBAC rules \u00b6 cd iter8-<your stack> Foo & Istio virtual service example Suppose Iter8 experiments on your stack involves manipulation of two types of resources: The foo resource belonging to the API group bar.my.org . The Istio virtual service resource. Note: Foo and bar are merely placeholders. It can be replaced by any standard K8s resource type like deployment or service , or a custom resource type, as required. Create RBAC rules that will enable Iter8 to manipulate foo resources and Istio virtual service resources during experiments. You can do so by creating roles.yaml and rolebindings.yaml files as follows. roles.yaml # This cluster role enables manipulation of foo resources apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : foo-for-<your stack> rules : - apiGroups : - bar.my.org resources : - foo verbs : - get - list - patch - update - create - delete - watch --- # This cluster role enables manipulation of Istio virtual services apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vs-for-<your stack> rules : - apiGroups : - networking.istio.io resources : - virtualservices verbs : - get - list - patch - update - create - delete - watch rolebindings.yaml # This cluster role binding enables Iter8 controller and task runner to manipulate # foo resources in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : foo-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : foo-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers --- # This role binding enables Iter8 controller and handler to manipulate # Istio virtual services in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vs-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vs-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers You can also refer to the Istio , KFServing , Knative , and Seldon examples. Step 5: Update RBAC rules \u00b6 Update the RBAC rules that are applied to the Kubernetes cluster as part of the Iter8 installation. Step 6: Submit PR \u00b6 Sign your commit and submit your pull request to the Iter8 repo.","title":"New K8s stack"},{"location":"contributing/newk8sstack/#add-a-k8s-stack-service-mesh-ingress","text":"Performing Iter8 experiments requires RBAC rules, which are contained in this Kustomize folder and are installed as part of the Iter8 installation. Enable Iter8 experiments over a new K8s stack by extending these RBAC rules.","title":"Add a K8s Stack / Service Mesh / Ingress"},{"location":"contributing/newk8sstack/#step-1-fork-iter8","text":"Fork the Iter8 GitHub repo . Locally clone your forked repo. For the rest of this document, $ITER8 will refer to the root of your local Iter8 repo.","title":"Step 1: Fork Iter8"},{"location":"contributing/newk8sstack/#step-2-edit-kustomizationyaml","text":"cd $ITER8 /install/core/rbac/stacks Edit kustomization.yaml to add your K8s stack. At the time of writing, it contains the following stacks: resources : - iter8-knative - iter8-istio - iter8-kfserving - iter8-seldon # -iter8-<your stack> # add your stack here","title":"Step 2: Edit kustomization.yaml"},{"location":"contributing/newk8sstack/#step-3-create-subfolder","text":"mkdir iter8-<your stack> cp iter8-kfserving/kustomization.yaml iter8-<your stack>/kustomization.yaml","title":"Step 3: Create subfolder"},{"location":"contributing/newk8sstack/#step-4-create-rbac-rules","text":"cd iter8-<your stack> Foo & Istio virtual service example Suppose Iter8 experiments on your stack involves manipulation of two types of resources: The foo resource belonging to the API group bar.my.org . The Istio virtual service resource. Note: Foo and bar are merely placeholders. It can be replaced by any standard K8s resource type like deployment or service , or a custom resource type, as required. Create RBAC rules that will enable Iter8 to manipulate foo resources and Istio virtual service resources during experiments. You can do so by creating roles.yaml and rolebindings.yaml files as follows. roles.yaml # This cluster role enables manipulation of foo resources apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : foo-for-<your stack> rules : - apiGroups : - bar.my.org resources : - foo verbs : - get - list - patch - update - create - delete - watch --- # This cluster role enables manipulation of Istio virtual services apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vs-for-<your stack> rules : - apiGroups : - networking.istio.io resources : - virtualservices verbs : - get - list - patch - update - create - delete - watch rolebindings.yaml # This cluster role binding enables Iter8 controller and task runner to manipulate # foo resources in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : foo-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : foo-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers --- # This role binding enables Iter8 controller and handler to manipulate # Istio virtual services in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vs-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vs-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers You can also refer to the Istio , KFServing , Knative , and Seldon examples.","title":"Step 4: Create RBAC rules"},{"location":"contributing/newk8sstack/#step-5-update-rbac-rules","text":"Update the RBAC rules that are applied to the Kubernetes cluster as part of the Iter8 installation.","title":"Step 5: Update RBAC rules"},{"location":"contributing/newk8sstack/#step-6-submit-pr","text":"Sign your commit and submit your pull request to the Iter8 repo.","title":"Step 6: Submit PR"},{"location":"contributing/overview/","text":"Overview \u00b6 Welcome! We are delighted that you want to contribute to Iter8! \ud83d\udc96 As you get started, you are in the best position to give us feedback on areas of our project that we need help with including: Problems found during setup of Iter8 Gaps in our quick start guide or other tutorials and documentation Bugs in our test and automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know! Ways to Contribute \u00b6 We welcome many different types of contributions including: Iter8 documentation/tutorials New features New K8s stack/service mesh/ingress Iter8 code samples for OpenShift Analytics Tasks Builds, CI Bug fixes Web design for https://iter8.tools Communications/social media/blog posts Reviewing pull requests Not everything happens through a GitHub pull request. Please come to our community meetings or contact us and let's discuss how we can work together. Come to Meetings! \u00b6 Everyone is welcome to join our community meetings. You never need an invite to attend. In fact, we want you to join us, even if you don\u2019t have anything you feel like you want to contribute. Just being there is enough! Our community meetings are on Wednesday at 1PM EST/EDT. You can find the video conference link here . You can find the agenda here as well as the meeting notes here . If you have a topic you would like to discuss, please put it on the agenda as well as a time estimate. Community meetings will be recorded and available to the public on our YouTube channel . Find an Issue \u00b6 Iter8 issues are managed centrally here . We have good first issues for new contributors and help wanted issues suitable for any contributor. Issued labeled good first issue have extra information to help you make your first contribution. Issues labeled help wanted are issues suitable for someone who isn't a core maintainer and is good to move onto after your first pull request. Sometimes there won\u2019t be any issues with these labels. That\u2019s ok! There is likely still something for you to work on. If you want to contribute but you don\u2019t know where to start or can't find a suitable issue, you can reach out to us over the Iter8 Slack workspace for help finding something to work on. Once you see an issue that you'd like to work on, please post a comment saying that you want to work on it. Something like \"I want to work on this\" is fine. Ask for Help \u00b6 The best ways to reach us with a question when contributing is to ask on: The original GitHub issue #development channel in the Iter8 Slack workspace Bring your questions to our community meetings Pull Request Lifecycle \u00b6 Your PR is associated with one (and infrequently, with more than one) GitHub issue . You can start the submission of your PR as soon as this issue has been created. Follow the standard GitHub fork and pull request process when creating and submitting your PR. The associated GitHub issue might need to go through design discussions and may not be ready for development. Your PR might require new tests; these new or existing tests may not yet be running successfully. At this stage, keep your PR as a draft , to signal that it is not yet ready for review. Once design discussions are complete and tests pass, convert the draft PR into a regular PR to signal that it is ready for review. Additionally, post a message in the #development Slack channel of the Iter8 Slack workspace with a link to your PR. This will expedite the review. You can expect an initial review within 1-2 days of submitting a PR, and follow up reviews (if any) to happen over 2-5 days. Use the #development Slack channel of Iter8 Slack workspace to ping/bump when the pull request is ready for further review or if it appears stalled. Iter8 releases happen frequently. Once your PR is merged, you can expect your contribution to show up live in a short amount of time at https://iter8.tools . Sign Your Commits \u00b6 Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. Read GitHub's documentation on signing your commits . You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running git commit --amend -s","title":"Overview"},{"location":"contributing/overview/#overview","text":"Welcome! We are delighted that you want to contribute to Iter8! \ud83d\udc96 As you get started, you are in the best position to give us feedback on areas of our project that we need help with including: Problems found during setup of Iter8 Gaps in our quick start guide or other tutorials and documentation Bugs in our test and automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know!","title":"Overview"},{"location":"contributing/overview/#ways-to-contribute","text":"We welcome many different types of contributions including: Iter8 documentation/tutorials New features New K8s stack/service mesh/ingress Iter8 code samples for OpenShift Analytics Tasks Builds, CI Bug fixes Web design for https://iter8.tools Communications/social media/blog posts Reviewing pull requests Not everything happens through a GitHub pull request. Please come to our community meetings or contact us and let's discuss how we can work together.","title":"Ways to Contribute"},{"location":"contributing/overview/#come-to-meetings","text":"Everyone is welcome to join our community meetings. You never need an invite to attend. In fact, we want you to join us, even if you don\u2019t have anything you feel like you want to contribute. Just being there is enough! Our community meetings are on Wednesday at 1PM EST/EDT. You can find the video conference link here . You can find the agenda here as well as the meeting notes here . If you have a topic you would like to discuss, please put it on the agenda as well as a time estimate. Community meetings will be recorded and available to the public on our YouTube channel .","title":"Come to Meetings!"},{"location":"contributing/overview/#find-an-issue","text":"Iter8 issues are managed centrally here . We have good first issues for new contributors and help wanted issues suitable for any contributor. Issued labeled good first issue have extra information to help you make your first contribution. Issues labeled help wanted are issues suitable for someone who isn't a core maintainer and is good to move onto after your first pull request. Sometimes there won\u2019t be any issues with these labels. That\u2019s ok! There is likely still something for you to work on. If you want to contribute but you don\u2019t know where to start or can't find a suitable issue, you can reach out to us over the Iter8 Slack workspace for help finding something to work on. Once you see an issue that you'd like to work on, please post a comment saying that you want to work on it. Something like \"I want to work on this\" is fine.","title":"Find an Issue"},{"location":"contributing/overview/#ask-for-help","text":"The best ways to reach us with a question when contributing is to ask on: The original GitHub issue #development channel in the Iter8 Slack workspace Bring your questions to our community meetings","title":"Ask for Help"},{"location":"contributing/overview/#pull-request-lifecycle","text":"Your PR is associated with one (and infrequently, with more than one) GitHub issue . You can start the submission of your PR as soon as this issue has been created. Follow the standard GitHub fork and pull request process when creating and submitting your PR. The associated GitHub issue might need to go through design discussions and may not be ready for development. Your PR might require new tests; these new or existing tests may not yet be running successfully. At this stage, keep your PR as a draft , to signal that it is not yet ready for review. Once design discussions are complete and tests pass, convert the draft PR into a regular PR to signal that it is ready for review. Additionally, post a message in the #development Slack channel of the Iter8 Slack workspace with a link to your PR. This will expedite the review. You can expect an initial review within 1-2 days of submitting a PR, and follow up reviews (if any) to happen over 2-5 days. Use the #development Slack channel of Iter8 Slack workspace to ping/bump when the pull request is ready for further review or if it appears stalled. Iter8 releases happen frequently. Once your PR is merged, you can expect your contribution to show up live in a short amount of time at https://iter8.tools .","title":"Pull Request Lifecycle"},{"location":"contributing/overview/#sign-your-commits","text":"Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. Read GitHub's documentation on signing your commits . You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running git commit --amend -s","title":"Sign Your Commits"},{"location":"contributing/tutorials/","text":"Contribute Tutorials \u00b6 Test your tutorial \u00b6 All iter8 tutorials include e2e tests, either as part of GitHub Actions workflows or as a standalone test script like this one if they require more resources than what is available in GitHub Actions workflows. When contributing a tutorial, please include relevant e2e tests. Locally serve Iter8 docs \u00b6 Pre-requisite: Python 3+. Use a Python 3 virtual environment to locally serve Iter8 docs. Run the following commands from the top-level directory of the Iter8 repo. cd mkdocs python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s Browse http://localhost:8000 to view your local Iter8 docs. Locally view live changes to Iter8 docs \u00b6 The overall structure of the documentation, as reflected in the nav tabs of https://iter8.tools , is located in the iter8/mkdocs/mkdocs.yml file. The markdown files for Iter8 docs are located under the iter8/mkdocs/docs folder. You will see live updates to http://localhost:8000 as you update the above files.","title":"Tutorials"},{"location":"contributing/tutorials/#contribute-tutorials","text":"","title":"Contribute Tutorials"},{"location":"contributing/tutorials/#test-your-tutorial","text":"All iter8 tutorials include e2e tests, either as part of GitHub Actions workflows or as a standalone test script like this one if they require more resources than what is available in GitHub Actions workflows. When contributing a tutorial, please include relevant e2e tests.","title":"Test your tutorial"},{"location":"contributing/tutorials/#locally-serve-iter8-docs","text":"Pre-requisite: Python 3+. Use a Python 3 virtual environment to locally serve Iter8 docs. Run the following commands from the top-level directory of the Iter8 repo. cd mkdocs python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s Browse http://localhost:8000 to view your local Iter8 docs.","title":"Locally serve Iter8 docs"},{"location":"contributing/tutorials/#locally-view-live-changes-to-iter8-docs","text":"The overall structure of the documentation, as reflected in the nav tabs of https://iter8.tools , is located in the iter8/mkdocs/mkdocs.yml file. The markdown files for Iter8 docs are located under the iter8/mkdocs/docs folder. You will see live updates to http://localhost:8000 as you update the above files.","title":"Locally view live changes to Iter8 docs"},{"location":"getting-started/help/","text":"Getting Help \u00b6 Read Iter8 docs . Join the Iter8 Slack workspace . File an issue or start a discussion on the Iter8 GitHub repo . Attend our weekly community meetings !","title":"Getting help"},{"location":"getting-started/help/#getting-help","text":"Read Iter8 docs . Join the Iter8 Slack workspace . File an issue or start a discussion on the Iter8 GitHub repo . Attend our weekly community meetings !","title":"Getting Help"},{"location":"getting-started/install/","text":"Installation \u00b6 Pre-requisite: Kustomize v3+ Get Kustomize v3+ by following these instructions . Install Iter8 in your Kubernetes cluster as follows. export TAG = master kustomize build https://github.com/iter8-tools/iter8/install/core/?ref = ${ TAG } | kubectl apply -f - kubectl wait crd -l creator = iter8 --for condition = established --timeout = 120s kustomize build https://github.com/iter8-tools/iter8/install/builtin-metrics/?ref = ${ TAG } | kubectl apply -f - kubectl wait --for = condition = Ready pods --all -n iter8-system Pinning the Iter8 version \u00b6 Iter8 release history is available here . To pin the version of Iter8 during the installation, select any Iter8 version >= v0.5.13 and change the TAG above. For example, to install version v0.5.14 of Iter8, use export TAG=v0.5.14 in the above commands. RBAC rules \u00b6 As part of Iter8 installation, the following RBAC rules are also installed in your cluster. Default RBAC Rules Resource Permissions Scope experiments.iter8.tools get, list, patch, update, watch Cluster-wide experiments.iter8.tools/status get, patch, update Cluster-wide metrics.iter8.tools get, list Cluster-wide jobs.batch create, delete, get, list, watch Cluster-wide leases.coordination.k8s.io get, list, watch, create, update, patch, delete iter8-system namespace events create iter8-system namespace services.serving.knative.dev get, list, patch, update Cluster-wide inferenceservices.serving.knative.dev get, list, patch, update Cluster-wide virtualservices.networking.istio.io get, list, patch, update, create, delete Cluster-wide destinationrules.networking.istio.io get, list, patch, update, create, delete Cluster-wide seldondeployments.machinelearning.seldon.io get, list, patch, update Cluster-wide","title":"Install"},{"location":"getting-started/install/#installation","text":"Pre-requisite: Kustomize v3+ Get Kustomize v3+ by following these instructions . Install Iter8 in your Kubernetes cluster as follows. export TAG = master kustomize build https://github.com/iter8-tools/iter8/install/core/?ref = ${ TAG } | kubectl apply -f - kubectl wait crd -l creator = iter8 --for condition = established --timeout = 120s kustomize build https://github.com/iter8-tools/iter8/install/builtin-metrics/?ref = ${ TAG } | kubectl apply -f - kubectl wait --for = condition = Ready pods --all -n iter8-system","title":"Installation"},{"location":"getting-started/install/#pinning-the-iter8-version","text":"Iter8 release history is available here . To pin the version of Iter8 during the installation, select any Iter8 version >= v0.5.13 and change the TAG above. For example, to install version v0.5.14 of Iter8, use export TAG=v0.5.14 in the above commands.","title":"Pinning the Iter8 version"},{"location":"getting-started/install/#rbac-rules","text":"As part of Iter8 installation, the following RBAC rules are also installed in your cluster. Default RBAC Rules Resource Permissions Scope experiments.iter8.tools get, list, patch, update, watch Cluster-wide experiments.iter8.tools/status get, patch, update Cluster-wide metrics.iter8.tools get, list Cluster-wide jobs.batch create, delete, get, list, watch Cluster-wide leases.coordination.k8s.io get, list, watch, create, update, patch, delete iter8-system namespace events create iter8-system namespace services.serving.knative.dev get, list, patch, update Cluster-wide inferenceservices.serving.knative.dev get, list, patch, update Cluster-wide virtualservices.networking.istio.io get, list, patch, update, create, delete Cluster-wide destinationrules.networking.istio.io get, list, patch, update, create, delete Cluster-wide seldondeployments.machinelearning.seldon.io get, list, patch, update Cluster-wide","title":"RBAC rules"},{"location":"getting-started/quick-start/","text":"Quick Start \u00b6 Scenario: A/B testing A/B testing enables you to compare two versions of an app/ML model, and select a winner based on a (business) reward metric and objectives (SLOs). In this tutorial, you will: Perform A/B testing. Specify user-engagement as the reward metric, and latency and error-rate based objectives. Iter8 will find a winner by comparing the two versions in terms of the reward, and by validating versions in terms of the objectives. Use New Relic as the provider for user-engagement metric, and Prometheus as the provider for latency and error-rate metrics. Combine A/B testing with progressive deployment . Iter8 will progressively shift the traffic towards the winner and promote it at the end as depicted below. Before you begin, you will need... The kubectl CLI . Kustomize 3+ . Go 1.13+ . (Seldon Only) Helm 3+ This tutorial is available for the following K8s stacks. Istio KFServing Knative Seldon Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup. 1. Create Kubernetes cluster \u00b6 Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288 2. Clone Iter8 repo \u00b6 git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) 3. Install K8s stack and Iter8 \u00b6 Choose the K8s stack over which you are performing the A/B testing experiment. Istio Setup Istio, Iter8, a mock New Relic service, and Prometheus add-on within your cluster. $ITER8 /samples/istio/quickstart/platformsetup.sh KFServing Setup KFServing, Iter8, a mock New Relic service, and Prometheus add-on within your cluster. $ITER8 /samples/kfserving/quickstart/platformsetup.sh Knative Setup Knative, Iter8, a mock New Relic service, and Prometheus add-on within your cluster. Knative can work with multiple networking layers. So can Iter8's Knative extension. Choose the networking layer for Knative. Contour $ITER8 /samples/knative/quickstart/platformsetup.sh contour Kourier $ITER8 /samples/knative/quickstart/platformsetup.sh kourier Gloo This step requires Python. This will install glooctl binary under $HOME/.gloo folder. $ITER8 /samples/knative/quickstart/platformsetup.sh gloo Istio $ITER8 /samples/knative/quickstart/platformsetup.sh istio Seldon Setup Seldon Core, Seldon Analytics and Iter8 within your cluster. $ITER8 /samples/seldon/quickstart/platformsetup.sh 4. Create app/ML model versions \u00b6 Istio Deploy the bookinfo microservice application including two versions of the productpage microservice. kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Look inside productpage-v2.yaml (v1 is similar) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v2 labels : app : productpage version : v2 spec : replicas : 1 selector : matchLabels : app : productpage version : v2 template : metadata : annotations : sidecar.istio.io/inject : \"true\" labels : app : productpage version : v2 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v2\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"green\" - name : reward_min value : \"10\" - name : reward_max value : \"20\" - name : port value : \"9080\" KFServing Deploy two KFServing inference services corresponding to two versions of a TensorFlow classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-baseline spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers\" Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-candidate spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers-2\" Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 100 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 0 Knative Deploy two versions of a Knative app. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0 Seldon Deploy two Seldon Deployments corresponding to two versions of an Iris classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/routing-rule.yaml kubectl wait --for condition = ready --timeout = 600s pods --all -n ns-baseline kubectl wait --for condition = ready --timeout = 600s pods --all -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-baseline spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/sklearn/iris implementation : SKLEARN_SERVER Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-candidate spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/xgboost/iris implementation : XGBOOST_SERVER Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - istio-system/seldon-gateway hosts : - iris.example.com http : - route : - destination : host : iris-default.ns-baseline.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v1 weight : 100 - destination : host : iris-default.ns-candidate.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v2 weight : 0 5. Generate requests \u00b6 Istio Generate requests to your app using Fortio as follows. # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never KFServing Generate requests to your model as follows. Port forward Istio ingress in terminal one INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Send requests in terminal two curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 0 .2 done Knative Generate requests using Fortio as follows. # URL_VALUE is the URL where your Knative application serves requests URL_VALUE = $( kubectl get ksvc sample-app -o json | jq .status.address.url ) sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/knative/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-t\" , \"6000s\" , \"-qps\" , \"16\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never Seldon Generate requests using Fortio as follows. URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/seldon/quickstart/fortio.yaml | sed \"s/6000s/600s/g\" | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 apiVersion : batch/v1 kind : Job metadata : name : fortio-requests spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"5\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: iris.example.com' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"data\": {\"ndarray\":[[6.8,2.8,4.8,1.4]]}}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/api/v1.0/predictions volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv1-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"0.7\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-baseline/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv2-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"1\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-candidate/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 6. Define metrics \u00b6 Iter8 introduces a Kubernetes CRD called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. Define the Iter8 metrics used in this experiment as follows. Istio kubectl apply -f $ITER8 /samples/istio/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 apiVersion : v1 kind : Namespace metadata : labels : creator : iter8 stack : istio name : iter8-istio --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-istio spec : params : - name : nrql value : | SELECT average(duration) FROM Sessions WHERE version='$name' SINCE $elapsedTime sec ago description : Average duration of a session type : Gauge provider : newrelic jqExpression : \".results[0] | .[] | tonumber\" urlTemplate : http://metrics-mock.iter8-system.svc.cluster.local:8080/newrelic --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count namespace : iter8-istio spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate namespace : iter8-istio spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : le500ms-latency-percentile namespace : iter8-istio spec : description : Less than 500 ms latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_bucket{le='500',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_request_duration_milliseconds_bucket{le='+Inf',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-istio/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency namespace : iter8-istio spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_sum{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : iter8-istio spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query KFServing kubectl apply -f $ITER8 /samples/kfserving/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 apiVersion : v1 kind : Namespace metadata : name : iter8-kfserving --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-kfserving spec : params : - name : nrql value : | SELECT average(duration) FROM Sessions WHERE version='$version' SINCE $elapsedTime sec ago description : Average duration of a session type : Gauge provider : newrelic jqExpression : \".results[0] | .[] | tonumber\" urlTemplate : http://metrics-mock.iter8-system.svc.cluster.local:8080/newrelic --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-kfserving spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(revision_app_request_latencies_bucket{namespace_name='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-kfserving spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-kfserving spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-kfserving spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_sum{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-kfserving spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Knative kubectl apply -f $ITER8 /samples/knative/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 apiVersion : v1 kind : Namespace metadata : name : iter8-knative --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-knative spec : params : - name : nrql value : | SELECT average(duration) FROM Sessions WHERE version='$name' SINCE $elapsedTime sec ago description : Average duration of a session type : Gauge headerTemplates : - name : X-Query-Key value : t0p-secret-api-key provider : newrelic jqExpression : \".results[0] | .[] | tonumber\" urlTemplate : http://metrics-mock.iter8-system.svc.cluster.local:8080/newrelic --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-knative spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(revision_app_request_latencies_bucket{revision_name='$name'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-knative/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-knative spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',revision_name='$name'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-knative spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',revision_name='$name'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{revision_name='$name'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-knative/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-knative spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_sum{revision_name='$name'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{revision_name='$name'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-knative/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-knative spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$name'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Seldon kubectl apply -f $ITER8 /samples/seldon/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 apiVersion : v1 kind : Namespace metadata : name : iter8-seldon --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-seldon spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(seldon_api_executor_client_requests_seconds_bucket{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-seldon spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-seldon spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_server_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-seldon spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_client_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-seldon spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-seldon spec : description : Number of feedback requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{service='feedback',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query Metrics in your environment You can use metrics from any RESTful provider in Iter8 experiments. In this tutorial, the metrics related to latency and error-rate objectives are collected by the Prometheus instance created in Step 3. The urlTemplate field in these metrics point to this Prometheus instance. If you wish to use these latency and error-rate metrics with your own application, change the urlTemplate values to match the URL of your Prometheus instance. In this tutorial, the user-engagement metric is synthetically generated by a mock New Relic service/Prometheus service. For your application, replace this metric with any business metric you wish to optimize. 7. Launch experiment \u00b6 Iter8 defines a Kubernetes resource called Experiment that automates A/B, A/B/n, Canary, and Conformance experiments. During an experiment, Iter8 can compare multiple versions, find, and safely promote the winning version (winner) based on business metrics and SLOs. Launch the Iter8 experiment that orchestrates A/B testing for the app/ML model in this tutorial. Istio kubectl apply -f $ITER8 /samples/istio/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight KFServing kubectl apply -f $ITER8 /samples/kfserving/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-kfserving/request-count rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-kfserving/mean-latency upperLimit : 2000 - metric : iter8-kfserving/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-kfserving/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 25 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml Knative kubectl apply -f $ITER8 /samples/knative/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-knative/request-count rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate Seldon kubectl apply -f $ITER8 /samples/seldon/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : iris strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-seldon/request-count rewards : # Business rewards - metric : iter8-seldon/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-seldon/mean-latency upperLimit : 2000 - metric : iter8-seldon/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-seldon/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about model versions used in this experiment baseline : name : iris-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : sid value : iris - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v1.yaml candidates : - name : iris-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : sid value : iris - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v2.yaml The process automated by Iter8 during this experiment is depicted below. 8. Observe experiment \u00b6 Observe the experiment in realtime. a) Observe metrics \u00b6 Install iter8ctl . You can change the directory where iter8ctl binary is installed by changing GOBIN below. GO111MODULE = on GOBIN = /usr/local/bin go get github.com/iter8-tools/iter8ctl@v0.1.3 Periodically describe the experiment. while clear ; do kubectl get experiment quickstart-exp -o yaml | iter8ctl describe -f - sleep 8 done Look inside metrics summary The iter8ctl output will be similar to the following. ****** Overview ****** Experiment name: quickstart-exp Experiment namespace: default Target: default/sample-app Testing pattern: A/B Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Running Number of completed iterations: 8 ****** Winner Assessment ****** App versions in this experiment: [ sample-app-v1 sample-app-v2 ] Winning version: sample-app-v2 Version recommended for promotion: sample-app-v2 ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------------+---------------+---------------+ | OBJECTIVE | SAMPLE-APP-V1 | SAMPLE-APP-V2 | +--------------------------------------------+---------------+---------------+ | iter8-knative/mean-latency < = | true | true | | 50 .000 | | | +--------------------------------------------+---------------+---------------+ | iter8-knative/95th-percentile-tail-latency | true | true | | < = 100 .000 | | | +--------------------------------------------+---------------+---------------+ | iter8-knative/error-rate < = | true | true | | 0 .010 | | | +--------------------------------------------+---------------+---------------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------------+---------------+---------------+ | METRIC | SAMPLE-APP-V1 | SAMPLE-APP-V2 | +--------------------------------------------+---------------+---------------+ | iter8-knative/request-count | 1213 .625 | 361 .962 | +--------------------------------------------+---------------+---------------+ | iter8-knative/user-engagement | 10 .023 | 14 .737 | +--------------------------------------------+---------------+---------------+ | iter8-knative/mean-latency | 1 .133 | 1 .175 | | ( milliseconds ) | | | +--------------------------------------------+---------------+---------------+ | iter8-knative/95th-percentile-tail-latency | 4 .768 | 4 .824 | | ( milliseconds ) | | | +--------------------------------------------+---------------+---------------+ | iter8-knative/error-rate | 0 .000 | 0 .000 | +--------------------------------------------+---------------+---------------+ As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by both versions and the candidate improves over the baseline version in terms of the reward metric. The candidate is identified as the winner and is recommended for promotion. b) Observe traffic \u00b6 Istio kubectl -n bookinfo-iter8 get vs bookinfo -o json --watch | jq .spec.http [ 0 ] .route Look inside traffic summary The kubectl output will be similar to the following. [ { \"destination\" : { \"host\" : \"productpage\" , \"port\" : { \"number\" : 9080 } , \"subset\" : \"productpage-v1\" } , \"weight\" : 35 } , { \"destination\" : { \"host\" : \"productpage\" , \"port\" : { \"number\" : 9080 } , \"subset\" : \"productpage-v2\" } , \"weight\" : 65 } ] KFServing kubectl get vs routing-rule -o json --watch | jq .spec.http [ 0 ] .route Look inside traffic summary [ { \"destination\" : { \"host\" : \"flowers-predictor-default.ns-baseline.svc.cluster.local\" }, \"headers\" : { \"request\" : { \"set\" : { \"Host\" : \"flowers-predictor-default.ns-baseline\" } }, \"response\" : { \"set\" : { \"version\" : \"flowers-v1\" } } }, \"weight\" : 5 }, { \"destination\" : { \"host\" : \"flowers-predictor-default.ns-candidate.svc.cluster.local\" }, \"headers\" : { \"request\" : { \"set\" : { \"Host\" : \"flowers-predictor-default.ns-candidate\" } }, \"response\" : { \"set\" : { \"version\" : \"flowers-v2\" } } }, \"weight\" : 95 } ] Knative kubectl get ksvc sample-app -o json --watch | jq .status.traffic Look inside traffic summary The kubectl output will be similar to the following. [ { \"latestRevision\" : false, \"percent\" : 45 , \"revisionName\" : \"sample-app-v1\" , \"tag\" : \"current\" , \"url\" : \"http://current-sample-app.default.example.com\" } , { \"latestRevision\" : true, \"percent\" : 55 , \"revisionName\" : \"sample-app-v2\" , \"tag\" : \"candidate\" , \"url\" : \"http://candidate-sample-app.default.example.com\" } ] Seldon kubectl get vs routing-rule -o json --watch | jq .spec.http [ 0 ] .route Look inside traffic summary [ { \"destination\" : { \"host\" : \"iris-default.ns-baseline.svc.cluster.local\" , \"port\" : { \"number\" : 8000 } }, \"headers\" : { \"response\" : { \"set\" : { \"version\" : \"iris-v1\" } } }, \"weight\" : 25 }, { \"destination\" : { \"host\" : \"iris-default.ns-candidate.svc.cluster.local\" , \"port\" : { \"number\" : 8000 } }, \"headers\" : { \"response\" : { \"set\" : { \"version\" : \"iris-v2\" } } }, \"weight\" : 75 } As the experiment progresses, you should see traffic progressively shift from the baseline version to the candidate version. c) Observe progress \u00b6 kubectl get experiment quickstart-exp --watch Look inside progress summary The kubectl output will be similar to the following. NAME TYPE TARGET STAGE COMPLETED ITERATIONS MESSAGE quickstart-exp Canary default/sample-app Running 1 IterationUpdate: Completed Iteration 1 quickstart-exp Canary default/sample-app Running 2 IterationUpdate: Completed Iteration 2 quickstart-exp Canary default/sample-app Running 3 IterationUpdate: Completed Iteration 3 quickstart-exp Canary default/sample-app Running 4 IterationUpdate: Completed Iteration 4 quickstart-exp Canary default/sample-app Running 5 IterationUpdate: Completed Iteration 5 quickstart-exp Canary default/sample-app Running 6 IterationUpdate: Completed Iteration 6 quickstart-exp Canary default/sample-app Running 7 IterationUpdate: Completed Iteration 7 quickstart-exp Canary default/sample-app Running 8 IterationUpdate: Completed Iteration 8 quickstart-exp Canary default/sample-app Running 9 IterationUpdate: Completed Iteration 9 When the experiment completes, you will see the experiment stage change from Running to Completed . Understanding what happened You created two versions of your app/ML model. You generated requests for your app/ML model versions. At the start of the experiment, 100% of the requests are sent to the baseline and 0% to the candidate. You created an Iter8 experiment with A/B testing pattern and progressive deployment pattern. In each iteration, Iter8 observed the latency and error-rate metrics collected by Prometheus, and the user-engagement metric from New Relic/Prometheus; Iter8 verified that the candidate satisfied all objectives, verified that the candidate improved over the baseline in terms of user-engagement, identified candidate as the winner, progressively shifted traffic from the baseline to the candidate, and promoted the candidate. 9. Cleanup \u00b6 Istio kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/experiment.yaml kubectl delete namespace bookinfo-iter8 KFServing kubectl delete -f $ITER8 /samples/kfserving/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml Knative kubectl delete -f $ITER8 /samples/knative/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml Seldon kubectl delete -f $ITER8 /samples/seldon/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/candidate.yaml","title":"Quick start"},{"location":"getting-started/quick-start/#quick-start","text":"Scenario: A/B testing A/B testing enables you to compare two versions of an app/ML model, and select a winner based on a (business) reward metric and objectives (SLOs). In this tutorial, you will: Perform A/B testing. Specify user-engagement as the reward metric, and latency and error-rate based objectives. Iter8 will find a winner by comparing the two versions in terms of the reward, and by validating versions in terms of the objectives. Use New Relic as the provider for user-engagement metric, and Prometheus as the provider for latency and error-rate metrics. Combine A/B testing with progressive deployment . Iter8 will progressively shift the traffic towards the winner and promote it at the end as depicted below. Before you begin, you will need... The kubectl CLI . Kustomize 3+ . Go 1.13+ . (Seldon Only) Helm 3+ This tutorial is available for the following K8s stacks. Istio KFServing Knative Seldon Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup.","title":"Quick Start"},{"location":"getting-started/quick-start/#1-create-kubernetes-cluster","text":"Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288","title":"1. Create Kubernetes cluster"},{"location":"getting-started/quick-start/#2-clone-iter8-repo","text":"git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd )","title":"2. Clone Iter8 repo"},{"location":"getting-started/quick-start/#3-install-k8s-stack-and-iter8","text":"Choose the K8s stack over which you are performing the A/B testing experiment. Istio Setup Istio, Iter8, a mock New Relic service, and Prometheus add-on within your cluster. $ITER8 /samples/istio/quickstart/platformsetup.sh KFServing Setup KFServing, Iter8, a mock New Relic service, and Prometheus add-on within your cluster. $ITER8 /samples/kfserving/quickstart/platformsetup.sh Knative Setup Knative, Iter8, a mock New Relic service, and Prometheus add-on within your cluster. Knative can work with multiple networking layers. So can Iter8's Knative extension. Choose the networking layer for Knative. Contour $ITER8 /samples/knative/quickstart/platformsetup.sh contour Kourier $ITER8 /samples/knative/quickstart/platformsetup.sh kourier Gloo This step requires Python. This will install glooctl binary under $HOME/.gloo folder. $ITER8 /samples/knative/quickstart/platformsetup.sh gloo Istio $ITER8 /samples/knative/quickstart/platformsetup.sh istio Seldon Setup Seldon Core, Seldon Analytics and Iter8 within your cluster. $ITER8 /samples/seldon/quickstart/platformsetup.sh","title":"3. Install K8s stack and Iter8"},{"location":"getting-started/quick-start/#4-create-appml-model-versions","text":"Istio Deploy the bookinfo microservice application including two versions of the productpage microservice. kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Look inside productpage-v2.yaml (v1 is similar) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v2 labels : app : productpage version : v2 spec : replicas : 1 selector : matchLabels : app : productpage version : v2 template : metadata : annotations : sidecar.istio.io/inject : \"true\" labels : app : productpage version : v2 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v2\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"green\" - name : reward_min value : \"10\" - name : reward_max value : \"20\" - name : port value : \"9080\" KFServing Deploy two KFServing inference services corresponding to two versions of a TensorFlow classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-baseline spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers\" Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-candidate spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers-2\" Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 100 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 0 Knative Deploy two versions of a Knative app. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0 Seldon Deploy two Seldon Deployments corresponding to two versions of an Iris classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/routing-rule.yaml kubectl wait --for condition = ready --timeout = 600s pods --all -n ns-baseline kubectl wait --for condition = ready --timeout = 600s pods --all -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-baseline spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/sklearn/iris implementation : SKLEARN_SERVER Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-candidate spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/xgboost/iris implementation : XGBOOST_SERVER Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - istio-system/seldon-gateway hosts : - iris.example.com http : - route : - destination : host : iris-default.ns-baseline.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v1 weight : 100 - destination : host : iris-default.ns-candidate.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v2 weight : 0","title":"4. Create app/ML model versions"},{"location":"getting-started/quick-start/#5-generate-requests","text":"Istio Generate requests to your app using Fortio as follows. # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never KFServing Generate requests to your model as follows. Port forward Istio ingress in terminal one INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Send requests in terminal two curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 0 .2 done Knative Generate requests using Fortio as follows. # URL_VALUE is the URL where your Knative application serves requests URL_VALUE = $( kubectl get ksvc sample-app -o json | jq .status.address.url ) sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/knative/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-t\" , \"6000s\" , \"-qps\" , \"16\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never Seldon Generate requests using Fortio as follows. URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/seldon/quickstart/fortio.yaml | sed \"s/6000s/600s/g\" | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 apiVersion : batch/v1 kind : Job metadata : name : fortio-requests spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"5\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: iris.example.com' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"data\": {\"ndarray\":[[6.8,2.8,4.8,1.4]]}}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/api/v1.0/predictions volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv1-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"0.7\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-baseline/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv2-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"1\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-candidate/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"5. Generate requests"},{"location":"getting-started/quick-start/#6-define-metrics","text":"Iter8 introduces a Kubernetes CRD called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. Define the Iter8 metrics used in this experiment as follows. Istio kubectl apply -f $ITER8 /samples/istio/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 apiVersion : v1 kind : Namespace metadata : labels : creator : iter8 stack : istio name : iter8-istio --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-istio spec : params : - name : nrql value : | SELECT average(duration) FROM Sessions WHERE version='$name' SINCE $elapsedTime sec ago description : Average duration of a session type : Gauge provider : newrelic jqExpression : \".results[0] | .[] | tonumber\" urlTemplate : http://metrics-mock.iter8-system.svc.cluster.local:8080/newrelic --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count namespace : iter8-istio spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate namespace : iter8-istio spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : le500ms-latency-percentile namespace : iter8-istio spec : description : Less than 500 ms latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_bucket{le='500',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_request_duration_milliseconds_bucket{le='+Inf',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-istio/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency namespace : iter8-istio spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_sum{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : iter8-istio spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query KFServing kubectl apply -f $ITER8 /samples/kfserving/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 apiVersion : v1 kind : Namespace metadata : name : iter8-kfserving --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-kfserving spec : params : - name : nrql value : | SELECT average(duration) FROM Sessions WHERE version='$version' SINCE $elapsedTime sec ago description : Average duration of a session type : Gauge provider : newrelic jqExpression : \".results[0] | .[] | tonumber\" urlTemplate : http://metrics-mock.iter8-system.svc.cluster.local:8080/newrelic --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-kfserving spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(revision_app_request_latencies_bucket{namespace_name='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-kfserving spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-kfserving spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-kfserving spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_sum{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-kfserving spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Knative kubectl apply -f $ITER8 /samples/knative/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 apiVersion : v1 kind : Namespace metadata : name : iter8-knative --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-knative spec : params : - name : nrql value : | SELECT average(duration) FROM Sessions WHERE version='$name' SINCE $elapsedTime sec ago description : Average duration of a session type : Gauge headerTemplates : - name : X-Query-Key value : t0p-secret-api-key provider : newrelic jqExpression : \".results[0] | .[] | tonumber\" urlTemplate : http://metrics-mock.iter8-system.svc.cluster.local:8080/newrelic --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-knative spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(revision_app_request_latencies_bucket{revision_name='$name'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-knative/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-knative spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',revision_name='$name'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-knative spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',revision_name='$name'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{revision_name='$name'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-knative/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-knative spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_sum{revision_name='$name'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{revision_name='$name'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-knative/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-knative spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$name'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Seldon kubectl apply -f $ITER8 /samples/seldon/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 apiVersion : v1 kind : Namespace metadata : name : iter8-seldon --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-seldon spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(seldon_api_executor_client_requests_seconds_bucket{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-seldon spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-seldon spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_server_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-seldon spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_client_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-seldon spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-seldon spec : description : Number of feedback requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{service='feedback',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query Metrics in your environment You can use metrics from any RESTful provider in Iter8 experiments. In this tutorial, the metrics related to latency and error-rate objectives are collected by the Prometheus instance created in Step 3. The urlTemplate field in these metrics point to this Prometheus instance. If you wish to use these latency and error-rate metrics with your own application, change the urlTemplate values to match the URL of your Prometheus instance. In this tutorial, the user-engagement metric is synthetically generated by a mock New Relic service/Prometheus service. For your application, replace this metric with any business metric you wish to optimize.","title":"6. Define metrics"},{"location":"getting-started/quick-start/#7-launch-experiment","text":"Iter8 defines a Kubernetes resource called Experiment that automates A/B, A/B/n, Canary, and Conformance experiments. During an experiment, Iter8 can compare multiple versions, find, and safely promote the winning version (winner) based on business metrics and SLOs. Launch the Iter8 experiment that orchestrates A/B testing for the app/ML model in this tutorial. Istio kubectl apply -f $ITER8 /samples/istio/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight KFServing kubectl apply -f $ITER8 /samples/kfserving/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-kfserving/request-count rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-kfserving/mean-latency upperLimit : 2000 - metric : iter8-kfserving/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-kfserving/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 25 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml Knative kubectl apply -f $ITER8 /samples/knative/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-knative/request-count rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate Seldon kubectl apply -f $ITER8 /samples/seldon/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : iris strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-seldon/request-count rewards : # Business rewards - metric : iter8-seldon/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-seldon/mean-latency upperLimit : 2000 - metric : iter8-seldon/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-seldon/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about model versions used in this experiment baseline : name : iris-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : sid value : iris - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v1.yaml candidates : - name : iris-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : sid value : iris - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v2.yaml The process automated by Iter8 during this experiment is depicted below.","title":"7. Launch experiment"},{"location":"getting-started/quick-start/#8-observe-experiment","text":"Observe the experiment in realtime.","title":"8. Observe experiment"},{"location":"getting-started/quick-start/#a-observe-metrics","text":"Install iter8ctl . You can change the directory where iter8ctl binary is installed by changing GOBIN below. GO111MODULE = on GOBIN = /usr/local/bin go get github.com/iter8-tools/iter8ctl@v0.1.3 Periodically describe the experiment. while clear ; do kubectl get experiment quickstart-exp -o yaml | iter8ctl describe -f - sleep 8 done Look inside metrics summary The iter8ctl output will be similar to the following. ****** Overview ****** Experiment name: quickstart-exp Experiment namespace: default Target: default/sample-app Testing pattern: A/B Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Running Number of completed iterations: 8 ****** Winner Assessment ****** App versions in this experiment: [ sample-app-v1 sample-app-v2 ] Winning version: sample-app-v2 Version recommended for promotion: sample-app-v2 ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------------+---------------+---------------+ | OBJECTIVE | SAMPLE-APP-V1 | SAMPLE-APP-V2 | +--------------------------------------------+---------------+---------------+ | iter8-knative/mean-latency < = | true | true | | 50 .000 | | | +--------------------------------------------+---------------+---------------+ | iter8-knative/95th-percentile-tail-latency | true | true | | < = 100 .000 | | | +--------------------------------------------+---------------+---------------+ | iter8-knative/error-rate < = | true | true | | 0 .010 | | | +--------------------------------------------+---------------+---------------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------------+---------------+---------------+ | METRIC | SAMPLE-APP-V1 | SAMPLE-APP-V2 | +--------------------------------------------+---------------+---------------+ | iter8-knative/request-count | 1213 .625 | 361 .962 | +--------------------------------------------+---------------+---------------+ | iter8-knative/user-engagement | 10 .023 | 14 .737 | +--------------------------------------------+---------------+---------------+ | iter8-knative/mean-latency | 1 .133 | 1 .175 | | ( milliseconds ) | | | +--------------------------------------------+---------------+---------------+ | iter8-knative/95th-percentile-tail-latency | 4 .768 | 4 .824 | | ( milliseconds ) | | | +--------------------------------------------+---------------+---------------+ | iter8-knative/error-rate | 0 .000 | 0 .000 | +--------------------------------------------+---------------+---------------+ As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by both versions and the candidate improves over the baseline version in terms of the reward metric. The candidate is identified as the winner and is recommended for promotion.","title":"a) Observe metrics"},{"location":"getting-started/quick-start/#b-observe-traffic","text":"Istio kubectl -n bookinfo-iter8 get vs bookinfo -o json --watch | jq .spec.http [ 0 ] .route Look inside traffic summary The kubectl output will be similar to the following. [ { \"destination\" : { \"host\" : \"productpage\" , \"port\" : { \"number\" : 9080 } , \"subset\" : \"productpage-v1\" } , \"weight\" : 35 } , { \"destination\" : { \"host\" : \"productpage\" , \"port\" : { \"number\" : 9080 } , \"subset\" : \"productpage-v2\" } , \"weight\" : 65 } ] KFServing kubectl get vs routing-rule -o json --watch | jq .spec.http [ 0 ] .route Look inside traffic summary [ { \"destination\" : { \"host\" : \"flowers-predictor-default.ns-baseline.svc.cluster.local\" }, \"headers\" : { \"request\" : { \"set\" : { \"Host\" : \"flowers-predictor-default.ns-baseline\" } }, \"response\" : { \"set\" : { \"version\" : \"flowers-v1\" } } }, \"weight\" : 5 }, { \"destination\" : { \"host\" : \"flowers-predictor-default.ns-candidate.svc.cluster.local\" }, \"headers\" : { \"request\" : { \"set\" : { \"Host\" : \"flowers-predictor-default.ns-candidate\" } }, \"response\" : { \"set\" : { \"version\" : \"flowers-v2\" } } }, \"weight\" : 95 } ] Knative kubectl get ksvc sample-app -o json --watch | jq .status.traffic Look inside traffic summary The kubectl output will be similar to the following. [ { \"latestRevision\" : false, \"percent\" : 45 , \"revisionName\" : \"sample-app-v1\" , \"tag\" : \"current\" , \"url\" : \"http://current-sample-app.default.example.com\" } , { \"latestRevision\" : true, \"percent\" : 55 , \"revisionName\" : \"sample-app-v2\" , \"tag\" : \"candidate\" , \"url\" : \"http://candidate-sample-app.default.example.com\" } ] Seldon kubectl get vs routing-rule -o json --watch | jq .spec.http [ 0 ] .route Look inside traffic summary [ { \"destination\" : { \"host\" : \"iris-default.ns-baseline.svc.cluster.local\" , \"port\" : { \"number\" : 8000 } }, \"headers\" : { \"response\" : { \"set\" : { \"version\" : \"iris-v1\" } } }, \"weight\" : 25 }, { \"destination\" : { \"host\" : \"iris-default.ns-candidate.svc.cluster.local\" , \"port\" : { \"number\" : 8000 } }, \"headers\" : { \"response\" : { \"set\" : { \"version\" : \"iris-v2\" } } }, \"weight\" : 75 } As the experiment progresses, you should see traffic progressively shift from the baseline version to the candidate version.","title":"b) Observe traffic"},{"location":"getting-started/quick-start/#c-observe-progress","text":"kubectl get experiment quickstart-exp --watch Look inside progress summary The kubectl output will be similar to the following. NAME TYPE TARGET STAGE COMPLETED ITERATIONS MESSAGE quickstart-exp Canary default/sample-app Running 1 IterationUpdate: Completed Iteration 1 quickstart-exp Canary default/sample-app Running 2 IterationUpdate: Completed Iteration 2 quickstart-exp Canary default/sample-app Running 3 IterationUpdate: Completed Iteration 3 quickstart-exp Canary default/sample-app Running 4 IterationUpdate: Completed Iteration 4 quickstart-exp Canary default/sample-app Running 5 IterationUpdate: Completed Iteration 5 quickstart-exp Canary default/sample-app Running 6 IterationUpdate: Completed Iteration 6 quickstart-exp Canary default/sample-app Running 7 IterationUpdate: Completed Iteration 7 quickstart-exp Canary default/sample-app Running 8 IterationUpdate: Completed Iteration 8 quickstart-exp Canary default/sample-app Running 9 IterationUpdate: Completed Iteration 9 When the experiment completes, you will see the experiment stage change from Running to Completed . Understanding what happened You created two versions of your app/ML model. You generated requests for your app/ML model versions. At the start of the experiment, 100% of the requests are sent to the baseline and 0% to the candidate. You created an Iter8 experiment with A/B testing pattern and progressive deployment pattern. In each iteration, Iter8 observed the latency and error-rate metrics collected by Prometheus, and the user-engagement metric from New Relic/Prometheus; Iter8 verified that the candidate satisfied all objectives, verified that the candidate improved over the baseline in terms of user-engagement, identified candidate as the winner, progressively shifted traffic from the baseline to the candidate, and promoted the candidate.","title":"c) Observe progress"},{"location":"getting-started/quick-start/#9-cleanup","text":"Istio kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/experiment.yaml kubectl delete namespace bookinfo-iter8 KFServing kubectl delete -f $ITER8 /samples/kfserving/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml Knative kubectl delete -f $ITER8 /samples/knative/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml Seldon kubectl delete -f $ITER8 /samples/seldon/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/candidate.yaml","title":"9. Cleanup"},{"location":"metrics/builtin/","text":"Builtin Metrics \u00b6 Builtin latency/error metrics Iter8 ships with a set of nine builtin metrics that measure your app/ML model's performance in terms of latency and errors. You can collect and use these metrics in experiments without the need to configure any external databases. This feature enables you to get started with Iter8 experiments, especially, SLO validation experiments, quickly. List of builtin metrics \u00b6 The following are the set of builtin Iter8 metrics. Namespace Name Type Description iter8-system request-count Counter Number of requests iter8-system error-count Gauge Number of responses with HTTP status code 4xx or 5xx iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx iter8-system mean-latency Gauge Mean response latency iter8-system latency-50 th -percentile Gauge 50 th percentile (median) response latency iter8-system latency-75 th -percentile Gauge 75 th percentile response latency iter8-system latency-90 th -percentile Gauge 90 th percentile response latency iter8-system latency-95 th -percentile Gauge 95 th percentile response latency iter8-system latency-99 th -percentile Gauge 99 th percentile response latency Collecting builtin metrics \u00b6 Use the metrics/collect task in an experiment to collect builtin metrics for your app/ML model versions. Example \u00b6 For an example of an experiment that uses builtin metrics, look inside the Knative experiment in this tutorial .","title":"Builtin metrics"},{"location":"metrics/builtin/#builtin-metrics","text":"Builtin latency/error metrics Iter8 ships with a set of nine builtin metrics that measure your app/ML model's performance in terms of latency and errors. You can collect and use these metrics in experiments without the need to configure any external databases. This feature enables you to get started with Iter8 experiments, especially, SLO validation experiments, quickly.","title":"Builtin Metrics"},{"location":"metrics/builtin/#list-of-builtin-metrics","text":"The following are the set of builtin Iter8 metrics. Namespace Name Type Description iter8-system request-count Counter Number of requests iter8-system error-count Gauge Number of responses with HTTP status code 4xx or 5xx iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx iter8-system mean-latency Gauge Mean response latency iter8-system latency-50 th -percentile Gauge 50 th percentile (median) response latency iter8-system latency-75 th -percentile Gauge 75 th percentile response latency iter8-system latency-90 th -percentile Gauge 90 th percentile response latency iter8-system latency-95 th -percentile Gauge 95 th percentile response latency iter8-system latency-99 th -percentile Gauge 99 th percentile response latency","title":"List of builtin metrics"},{"location":"metrics/builtin/#collecting-builtin-metrics","text":"Use the metrics/collect task in an experiment to collect builtin metrics for your app/ML model versions.","title":"Collecting builtin metrics"},{"location":"metrics/builtin/#example","text":"For an example of an experiment that uses builtin metrics, look inside the Knative experiment in this tutorial .","title":"Example"},{"location":"metrics/custom/","text":"Defining Custom Metrics \u00b6 Custom Iter8 metrics enable you to use data from any database for evaluating app/ML model versions within Iter8 experiments. This document describes how you can define custom Iter8 metrics and (optionally) supply authentication information that may be required by the metrics provider. Metric providers differ in the following aspects. HTTP request authentication method: no authentication, basic auth, API keys, or bearer token HTTP request method: GET or POST Format of HTTP parameters and/or JSON body used while querying them Format of the JSON response returned by the provider The logic used by Iter8 to extract the metric value from the JSON response The examples in this document focus on Prometheus, NewRelic, Sysdig, and Elastic. However, the principles illustrated here will enable you to use metrics from any provider in experiments. Metrics with/without auth \u00b6 Note: Metrics are defined by you, the Iter8 end-user . Prometheus Prometheus does not support any authentication mechanism out-of-the-box . However, Prometheus can be setup in conjunction with a reverse proxy, which in turn can support HTTP request authentication, as described here . No Authentication The following is an example of an Iter8 metric with Prometheus as the provider. This example assumes that Prometheus can be queried by Iter8 without any authentication. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://myprometheusservice.com/api/v1 Basic auth Suppose Prometheus is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can enable Iter8 to query this Prometheus instance as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic promcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter authType : Basic secret : myns/promcredentials jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : https://my.secure.prometheus.service.com/api/v1 Brief explanation of the request-count metric Prometheus enables metric queries using HTTP GET requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of request-count , and defaulted to GET . Iter8 will query Prometheus during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named query as required by Prometheus . The value of this parameter is derived by substituting the placeholders in the value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Prometheus. The urlTemplate field provides the URL of the prometheus service. New Relic New Relic uses API Keys to authenticate requests as documented here . The API key may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. API key embedded in metric The following is an example of an Iter8 metric with Prometheus as the provider. In this example, t0p-secret-api-key is the New Relic API key. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter headerTemplates : - name : X-Query-Key value : t0p-secret-api-key jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id API key embedded in secret Suppose your New Relic API key is t0p-secret-api-key ; you wish to store this API key in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the API key. kubectl create secret generic nrcredentials -n myns --from-literal = mykey = t0p-secret-api-key The above secret contains a data field named mykey whose value is the API key. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics = myns Define metric: When defining the metric, ensure that the authType field is set to APIKey and the appropriate secret is referenced. In the headerTemplates field, include X-Query-Key as the name of a header field (as required by New Relic ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${mykey} at query time, by looking up the referenced secret named nrcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter authType : APIKey secret : myns/nrcredentials headerTemplates : - name : X-Query-Key value : ${mykey} jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id Brief explanation of the name-count metric New Relic enables metric queries using both HTTP GET or POST requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of name-count , and defaulted to GET . Iter8 will query New Relic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named nrql as required by New Relic . The value of this parameter is derived by substituting the placeholders in its value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by New Relic. The urlTemplate field provides the URL of the New Relic service. Sysdig Sysdig data API accepts HTTP POST requests and uses a bearer token for authentication as documented here . The bearer token may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. Bearer token embedded in metric The following is an example of an Iter8 metric with Sysdig as the provider. In this example, 87654321-1234-1234-1234-123456789012 is the Sysdig bearer token (also referred to as access key by Sysdig). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer 87654321-1234-1234-1234-123456789012 jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Bearer token embedded in secret Suppose your Sysdig token is 87654321-1234-1234-1234-123456789012 ; you wish to store this token in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the token. kubectl create secret generic sdcredentials -n myns --from-literal = token = 87654321 -1234-1234-1234-123456789012 The above secret contains a data field named token whose value is the Sysdig token. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Bearer and the appropriate secret is referenced. In the headerTemplates field, include Authorize header field (as required by Sysdig ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${token} at query time, by looking up the referenced secret named sdcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST authType : Bearer secret : myns/sdcredentials type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer ${token} jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Brief explanation of the cpu-utilization metric Sysdig enables metric queries using both POST requests; hence, the method field of the Iter8 metric is set to POST. Iter8 will query Sysdig during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Sysdig . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Sysdig. The urlTemplate field provides the URL of the Sysdig service. Elastic Elasticsearch REST API accepts HTTP GET or POST requests and uses basic authentication as documented here . Suppose Elasticsearch is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can then enable Iter8 to query the Elasticsearch service as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic elasticcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : average-sales spec : description : An elastic example provider : elastic body : >- { \"aggs\": { \"range\": { \"date_range\": { \"field\": \"date\", \"ranges\": [ { \"from\": \"now-${elapsedTime}s/s\" } ] } }, \"items_to_sell\": { \"filter\": { \"term\": { \"version\": \"${revision}\" } }, \"aggs\": { \"avg_sales\": { \"avg\": { \"field\": \"sale_price\" } } } } } } method : POST authType : Basic secret : myns/elasticcredentials type : Gauge headerTemplates : - name : Content-Type value : application/json jqExpression : \".aggregations.items_to_sell.avg_sales.value | tonumber\" urlTemplate : https://secure.elastic.com/my/sales Brief explanation of the average sales metric Elastic enables metric queries using GET or POST requests. In the elastic example, The method field of the Iter8 metric is set to POST. Iter8 will query Elastic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Elastic . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Elastic. The urlTemplate field provides the URL of the Elastic service. Placeholder substitution \u00b6 Note: This step is automated by Iter8 . Iter8 will substitute placeholders in the metric query based on the time elapsed since the start of the experiment, and information associated with each version in the experiment. Suppose the metrics defined above are referenced within an experiment as follows. Further, suppose this experiment has started, Iter8 is about to do an iteration of this experiment, and the time elapsed since the start of the experiment is 600 seconds. Look inside sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : sample-exp spec : target : default/sample-app strategy : testingPattern : Canary criteria : # This experiment assumes that metrics have been created in the `myns` namespace requestCount : myns/request-count objectives : - metric : myns/name-count lowerLimit : 50 - metric : myns/cpu-utilization upperLimit : 90 - metric : myns/average-sales lowerLimit : \"250.0\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : userfilter value : 'usergroup!~\"wakanda\"' candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : userfilter value : 'usergroup=~\"wakanda\"' For the sample experiment above, Iter8 will use two HTTP(S) queries to fetch metric values, one for the baseline version, and another for the candidate version. Prometheus Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named query whose value equals: sum(increase(revision_app_request_latencies_count{service_name='current',usergroup!~\"wakanda\"}[600s])) or on() vector(0) New Relic Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named nrql whose value equals: SELECT count(appName) FROM PageView WHERE revisionName='sample-app-v1' SINCE 600 seconds ago Sysdig Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"last\" : 600 , \"sampling\" : 600 , \"filter\" : \"kubernetes.app.revision.name = 'sample-app-v1'\" , \"metrics\" : [ { \"id\" : \"cpu.cores.used\" , \"aggregations\" : { \"time\" : \"avg\" , \"group\" : \"sum\" } } ], \"dataSourceType\" : \"container\" , \"paging\" : { \"from\" : 0 , \"to\" : 99 } } Elastic Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"aggs\" : { \"range\" : { \"date_range\" : { \"field\" : \"date\" , \"ranges\" : [ { \"from\" : \"now-600s/s\" } ] } }, \"items_to_sell\" : { \"filter\" : { \"term\" : { \"version\" : \"sample-app-v1\" } }, \"aggs\" : { \"avg_sales\" : { \"avg\" : { \"field\" : \"sale_price\" } } } } } } The placeholder $elapsedTime has been substituted with 600, which is the time elapsed since the start of the experiment. The other placeholders have been substituted based on the versionInfo field of the baseline version in the experiment. Iter8 builds and sends an HTTP request in a similar manner for the candidate version as well. JSON response \u00b6 Note: This step is handled by the metrics provider . The metrics provider is expected to respond to Iter8's HTTP request with a JSON object. The format of this JSON object is defined by the provider. Prometheus The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } } New Relic The format of the New Relic JSON response is discussed here . A sample New Relic response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"results\" : [ { \"count\" : 80275388 } ], \"metadata\" : { \"eventTypes\" : [ \"PageView\" ], \"eventType\" : \"PageView\" , \"openEnded\" : true , \"beginTime\" : \"2014-08-03T19:00:00Z\" , \"endTime\" : \"2017-01-18T23:18:41Z\" , \"beginTimeMillis=\" : 1407092400000 , \"endTimeMillis\" : 1484781521198 , \"rawSince\" : \"'2014-08-04 00:00:00+0500'\" , \"rawUntil\" : \"`now`\" , \"rawCompareWith\" : \"\" , \"clippedTimeWindows\" : { \"Browser\" : { \"beginTimeMillis\" : 1483571921198 , \"endTimeMillis\" : 1484781521198 , \"retentionMillis\" : 1209600000 } }, \"messages\" : [], \"contents\" : [ { \"function\" : \"count\" , \"attribute\" : \"appName\" , \"simple\" : true } ] } } Sysdig The format of the Sysdig JSON response is discussed here . A sample Sysdig response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\" : [ { \"t\" : 1582756200 , \"d\" : [ 6.481 ] } ], \"start\" : 1582755600 , \"end\" : 1582756200 } Elastic The format of the Elastic JSON response is discussed here . A sample Elastic response is as follows. 1 2 3 4 5 6 7 8 { \"aggregations\" : { \"items_to_sell\" : { \"doc_count\" : 3 , \"avg_sales\" : { \"value\" : 128.33333333333334 } } } } Processing the JSON response \u00b6 Note: This step is automated by Iter8 . Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. New Relic Consider the jqExpression defined in the sample New Relic metric . Let us apply it to the sample JSON response from New Relic . echo '{ \"results\": [ { \"count\": 80275388 } ], \"metadata\": { \"eventTypes\": [ \"PageView\" ], \"eventType\": \"PageView\", \"openEnded\": true, \"beginTime\": \"2014-08-03T19:00:00Z\", \"endTime\": \"2017-01-18T23:18:41Z\", \"beginTimeMillis=\": 1407092400000, \"endTimeMillis\": 1484781521198, \"rawSince\": \"' 2014 -08-04 00 :00:00+0500 '\", \"rawUntil\": \"`now`\", \"rawCompareWith\": \"\", \"clippedTimeWindows\": { \"Browser\": { \"beginTimeMillis\": 1483571921198, \"endTimeMillis\": 1484781521198, \"retentionMillis\": 1209600000 } }, \"messages\": [], \"contents\": [ { \"function\": \"count\", \"attribute\": \"appName\", \"simple\": true } ] } }' | jq \".results[0].count | tonumber\" Executing the above command results yields 80275388 , a number, as required by Iter8. Sysdig Consider the jqExpression defined in the sample Sysdig metric . Let us apply it to the sample JSON response from Sysdig . echo '{ \"data\": [ { \"t\": 1582756200, \"d\": [ 6.481 ] } ], \"start\": 1582755600, \"end\": 1582756200 }' | jq \".data[0].d[0] | tonumber\" Executing the above command results yields 6.481 , a number, as required by Iter8. Elastic Consider the jqExpression defined in the sample Elastic metric . Let us apply it to the sample JSON response from Elastic . echo '{ \"aggregations\": { \"items_to_sell\": { \"doc_count\": 3, \"avg_sales\": { \"value\": 128.33333333333334 } } } }' | jq \".aggregations.items_to_sell.avg_sales.value | tonumber\" Executing the above command results yields 128.33333333333334 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression . Error handling \u00b6 Note: This step is automated by Iter8 . Errors may occur during Iter8's metric queries due to a number of reasons (for example, due to an invalid jqExpression supplied within the metric). If Iter8 encounters errors during its attempt to retrieve metric values, Iter8 will mark the respective metric as unavailable. Iter8 can be used with any provider that can receive an HTTP request and respond with a JSON object containing the metrics information. Documentation requests and contributions (PRs) are welcome for providers not listed here. \u21a9 In a conformance experiment, n = 1 . In canary and A/B experiments, n = 2 . In A/B/n experiments, n > 2 . \u21a9 \u21a9 \u21a9 \u21a9","title":"Custom metrics"},{"location":"metrics/custom/#defining-custom-metrics","text":"Custom Iter8 metrics enable you to use data from any database for evaluating app/ML model versions within Iter8 experiments. This document describes how you can define custom Iter8 metrics and (optionally) supply authentication information that may be required by the metrics provider. Metric providers differ in the following aspects. HTTP request authentication method: no authentication, basic auth, API keys, or bearer token HTTP request method: GET or POST Format of HTTP parameters and/or JSON body used while querying them Format of the JSON response returned by the provider The logic used by Iter8 to extract the metric value from the JSON response The examples in this document focus on Prometheus, NewRelic, Sysdig, and Elastic. However, the principles illustrated here will enable you to use metrics from any provider in experiments.","title":"Defining Custom Metrics"},{"location":"metrics/custom/#metrics-withwithout-auth","text":"Note: Metrics are defined by you, the Iter8 end-user . Prometheus Prometheus does not support any authentication mechanism out-of-the-box . However, Prometheus can be setup in conjunction with a reverse proxy, which in turn can support HTTP request authentication, as described here . No Authentication The following is an example of an Iter8 metric with Prometheus as the provider. This example assumes that Prometheus can be queried by Iter8 without any authentication. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://myprometheusservice.com/api/v1 Basic auth Suppose Prometheus is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can enable Iter8 to query this Prometheus instance as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic promcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter authType : Basic secret : myns/promcredentials jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : https://my.secure.prometheus.service.com/api/v1 Brief explanation of the request-count metric Prometheus enables metric queries using HTTP GET requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of request-count , and defaulted to GET . Iter8 will query Prometheus during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named query as required by Prometheus . The value of this parameter is derived by substituting the placeholders in the value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Prometheus. The urlTemplate field provides the URL of the prometheus service. New Relic New Relic uses API Keys to authenticate requests as documented here . The API key may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. API key embedded in metric The following is an example of an Iter8 metric with Prometheus as the provider. In this example, t0p-secret-api-key is the New Relic API key. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter headerTemplates : - name : X-Query-Key value : t0p-secret-api-key jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id API key embedded in secret Suppose your New Relic API key is t0p-secret-api-key ; you wish to store this API key in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the API key. kubectl create secret generic nrcredentials -n myns --from-literal = mykey = t0p-secret-api-key The above secret contains a data field named mykey whose value is the API key. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics = myns Define metric: When defining the metric, ensure that the authType field is set to APIKey and the appropriate secret is referenced. In the headerTemplates field, include X-Query-Key as the name of a header field (as required by New Relic ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${mykey} at query time, by looking up the referenced secret named nrcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter authType : APIKey secret : myns/nrcredentials headerTemplates : - name : X-Query-Key value : ${mykey} jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id Brief explanation of the name-count metric New Relic enables metric queries using both HTTP GET or POST requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of name-count , and defaulted to GET . Iter8 will query New Relic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named nrql as required by New Relic . The value of this parameter is derived by substituting the placeholders in its value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by New Relic. The urlTemplate field provides the URL of the New Relic service. Sysdig Sysdig data API accepts HTTP POST requests and uses a bearer token for authentication as documented here . The bearer token may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. Bearer token embedded in metric The following is an example of an Iter8 metric with Sysdig as the provider. In this example, 87654321-1234-1234-1234-123456789012 is the Sysdig bearer token (also referred to as access key by Sysdig). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer 87654321-1234-1234-1234-123456789012 jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Bearer token embedded in secret Suppose your Sysdig token is 87654321-1234-1234-1234-123456789012 ; you wish to store this token in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the token. kubectl create secret generic sdcredentials -n myns --from-literal = token = 87654321 -1234-1234-1234-123456789012 The above secret contains a data field named token whose value is the Sysdig token. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Bearer and the appropriate secret is referenced. In the headerTemplates field, include Authorize header field (as required by Sysdig ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${token} at query time, by looking up the referenced secret named sdcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST authType : Bearer secret : myns/sdcredentials type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer ${token} jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Brief explanation of the cpu-utilization metric Sysdig enables metric queries using both POST requests; hence, the method field of the Iter8 metric is set to POST. Iter8 will query Sysdig during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Sysdig . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Sysdig. The urlTemplate field provides the URL of the Sysdig service. Elastic Elasticsearch REST API accepts HTTP GET or POST requests and uses basic authentication as documented here . Suppose Elasticsearch is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can then enable Iter8 to query the Elasticsearch service as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic elasticcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : average-sales spec : description : An elastic example provider : elastic body : >- { \"aggs\": { \"range\": { \"date_range\": { \"field\": \"date\", \"ranges\": [ { \"from\": \"now-${elapsedTime}s/s\" } ] } }, \"items_to_sell\": { \"filter\": { \"term\": { \"version\": \"${revision}\" } }, \"aggs\": { \"avg_sales\": { \"avg\": { \"field\": \"sale_price\" } } } } } } method : POST authType : Basic secret : myns/elasticcredentials type : Gauge headerTemplates : - name : Content-Type value : application/json jqExpression : \".aggregations.items_to_sell.avg_sales.value | tonumber\" urlTemplate : https://secure.elastic.com/my/sales Brief explanation of the average sales metric Elastic enables metric queries using GET or POST requests. In the elastic example, The method field of the Iter8 metric is set to POST. Iter8 will query Elastic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Elastic . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Elastic. The urlTemplate field provides the URL of the Elastic service.","title":"Metrics with/without auth"},{"location":"metrics/custom/#placeholder-substitution","text":"Note: This step is automated by Iter8 . Iter8 will substitute placeholders in the metric query based on the time elapsed since the start of the experiment, and information associated with each version in the experiment. Suppose the metrics defined above are referenced within an experiment as follows. Further, suppose this experiment has started, Iter8 is about to do an iteration of this experiment, and the time elapsed since the start of the experiment is 600 seconds. Look inside sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : sample-exp spec : target : default/sample-app strategy : testingPattern : Canary criteria : # This experiment assumes that metrics have been created in the `myns` namespace requestCount : myns/request-count objectives : - metric : myns/name-count lowerLimit : 50 - metric : myns/cpu-utilization upperLimit : 90 - metric : myns/average-sales lowerLimit : \"250.0\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : userfilter value : 'usergroup!~\"wakanda\"' candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : userfilter value : 'usergroup=~\"wakanda\"' For the sample experiment above, Iter8 will use two HTTP(S) queries to fetch metric values, one for the baseline version, and another for the candidate version. Prometheus Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named query whose value equals: sum(increase(revision_app_request_latencies_count{service_name='current',usergroup!~\"wakanda\"}[600s])) or on() vector(0) New Relic Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named nrql whose value equals: SELECT count(appName) FROM PageView WHERE revisionName='sample-app-v1' SINCE 600 seconds ago Sysdig Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"last\" : 600 , \"sampling\" : 600 , \"filter\" : \"kubernetes.app.revision.name = 'sample-app-v1'\" , \"metrics\" : [ { \"id\" : \"cpu.cores.used\" , \"aggregations\" : { \"time\" : \"avg\" , \"group\" : \"sum\" } } ], \"dataSourceType\" : \"container\" , \"paging\" : { \"from\" : 0 , \"to\" : 99 } } Elastic Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"aggs\" : { \"range\" : { \"date_range\" : { \"field\" : \"date\" , \"ranges\" : [ { \"from\" : \"now-600s/s\" } ] } }, \"items_to_sell\" : { \"filter\" : { \"term\" : { \"version\" : \"sample-app-v1\" } }, \"aggs\" : { \"avg_sales\" : { \"avg\" : { \"field\" : \"sale_price\" } } } } } } The placeholder $elapsedTime has been substituted with 600, which is the time elapsed since the start of the experiment. The other placeholders have been substituted based on the versionInfo field of the baseline version in the experiment. Iter8 builds and sends an HTTP request in a similar manner for the candidate version as well.","title":"Placeholder substitution"},{"location":"metrics/custom/#json-response","text":"Note: This step is handled by the metrics provider . The metrics provider is expected to respond to Iter8's HTTP request with a JSON object. The format of this JSON object is defined by the provider. Prometheus The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } } New Relic The format of the New Relic JSON response is discussed here . A sample New Relic response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"results\" : [ { \"count\" : 80275388 } ], \"metadata\" : { \"eventTypes\" : [ \"PageView\" ], \"eventType\" : \"PageView\" , \"openEnded\" : true , \"beginTime\" : \"2014-08-03T19:00:00Z\" , \"endTime\" : \"2017-01-18T23:18:41Z\" , \"beginTimeMillis=\" : 1407092400000 , \"endTimeMillis\" : 1484781521198 , \"rawSince\" : \"'2014-08-04 00:00:00+0500'\" , \"rawUntil\" : \"`now`\" , \"rawCompareWith\" : \"\" , \"clippedTimeWindows\" : { \"Browser\" : { \"beginTimeMillis\" : 1483571921198 , \"endTimeMillis\" : 1484781521198 , \"retentionMillis\" : 1209600000 } }, \"messages\" : [], \"contents\" : [ { \"function\" : \"count\" , \"attribute\" : \"appName\" , \"simple\" : true } ] } } Sysdig The format of the Sysdig JSON response is discussed here . A sample Sysdig response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\" : [ { \"t\" : 1582756200 , \"d\" : [ 6.481 ] } ], \"start\" : 1582755600 , \"end\" : 1582756200 } Elastic The format of the Elastic JSON response is discussed here . A sample Elastic response is as follows. 1 2 3 4 5 6 7 8 { \"aggregations\" : { \"items_to_sell\" : { \"doc_count\" : 3 , \"avg_sales\" : { \"value\" : 128.33333333333334 } } } }","title":"JSON response"},{"location":"metrics/custom/#processing-the-json-response","text":"Note: This step is automated by Iter8 . Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. New Relic Consider the jqExpression defined in the sample New Relic metric . Let us apply it to the sample JSON response from New Relic . echo '{ \"results\": [ { \"count\": 80275388 } ], \"metadata\": { \"eventTypes\": [ \"PageView\" ], \"eventType\": \"PageView\", \"openEnded\": true, \"beginTime\": \"2014-08-03T19:00:00Z\", \"endTime\": \"2017-01-18T23:18:41Z\", \"beginTimeMillis=\": 1407092400000, \"endTimeMillis\": 1484781521198, \"rawSince\": \"' 2014 -08-04 00 :00:00+0500 '\", \"rawUntil\": \"`now`\", \"rawCompareWith\": \"\", \"clippedTimeWindows\": { \"Browser\": { \"beginTimeMillis\": 1483571921198, \"endTimeMillis\": 1484781521198, \"retentionMillis\": 1209600000 } }, \"messages\": [], \"contents\": [ { \"function\": \"count\", \"attribute\": \"appName\", \"simple\": true } ] } }' | jq \".results[0].count | tonumber\" Executing the above command results yields 80275388 , a number, as required by Iter8. Sysdig Consider the jqExpression defined in the sample Sysdig metric . Let us apply it to the sample JSON response from Sysdig . echo '{ \"data\": [ { \"t\": 1582756200, \"d\": [ 6.481 ] } ], \"start\": 1582755600, \"end\": 1582756200 }' | jq \".data[0].d[0] | tonumber\" Executing the above command results yields 6.481 , a number, as required by Iter8. Elastic Consider the jqExpression defined in the sample Elastic metric . Let us apply it to the sample JSON response from Elastic . echo '{ \"aggregations\": { \"items_to_sell\": { \"doc_count\": 3, \"avg_sales\": { \"value\": 128.33333333333334 } } } }' | jq \".aggregations.items_to_sell.avg_sales.value | tonumber\" Executing the above command results yields 128.33333333333334 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression .","title":"Processing the JSON response"},{"location":"metrics/custom/#error-handling","text":"Note: This step is automated by Iter8 . Errors may occur during Iter8's metric queries due to a number of reasons (for example, due to an invalid jqExpression supplied within the metric). If Iter8 encounters errors during its attempt to retrieve metric values, Iter8 will mark the respective metric as unavailable. Iter8 can be used with any provider that can receive an HTTP request and respond with a JSON object containing the metrics information. Documentation requests and contributions (PRs) are welcome for providers not listed here. \u21a9 In a conformance experiment, n = 1 . In canary and A/B experiments, n = 2 . In A/B/n experiments, n > 2 . \u21a9 \u21a9 \u21a9 \u21a9","title":"Error handling"},{"location":"metrics/using-metrics/","text":"Using Metrics in Experiments \u00b6 Iter8 metric resources Iter8 defines a custom Kubernetes resource (CRD) called Metric that makes it easy to define and use metrics in experiments. Iter8 installation includes a set of pre-defined builtin metrics that pertain to app/ML model latency/errors. You can also define custom metrics that enable you to utilize data from Prometheus, New Relic, Sysdig, Elastic or any other database of your choice. List metrics \u00b6 Find the set Iter8 metrics available in your cluster using kubectl get . kubectl get metrics.iter8.tools --all-namespaces NAMESPACE NAME TYPE DESCRIPTION iter8-kfserving user-engagement Gauge Average duration of a session iter8-system error-count Counter Number of responses with HTTP status code 4xx or 5xx ( Iter8 builtin metric ) iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx ( Iter8 builtin metric ) iter8-system latency-50th-percentile Gauge 50th percentile ( median ) latency ( Iter8 builtin metric ) iter8-system latency-75th-percentile Gauge 75th percentile latency ( Iter8 builtin metric ) iter8-system latency-90th-percentile Gauge 90th percentile latency ( Iter8 builtin metric ) iter8-system latency-95th-percentile Gauge 95th percentile latency ( Iter8 builtin metric ) iter8-system latency-99th-percentile Gauge 99th percentile latency ( Iter8 builtin metric ) iter8-system mean-latency Gauge Mean latency ( Iter8 builtin metric ) iter8-system request-count Counter Number of requests ( Iter8 builtin metric ) Referencing metrics within experiments \u00b6 Use metrics in experiments by referencing them in the criteria section of the experiment manifest. Reference metrics using the namespace/name or name format . Sample experiment illustrating the use of metrics kind : Experiment ... spec : ... criteria : requestCount : iter8-knative/request-count # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" Observing metric values \u00b6 During an experiment, Iter8 reports the metric values observed for each version. Use iter8ctl to observe these metric values in realtime. See here for an example.","title":"Using metrics"},{"location":"metrics/using-metrics/#using-metrics-in-experiments","text":"Iter8 metric resources Iter8 defines a custom Kubernetes resource (CRD) called Metric that makes it easy to define and use metrics in experiments. Iter8 installation includes a set of pre-defined builtin metrics that pertain to app/ML model latency/errors. You can also define custom metrics that enable you to utilize data from Prometheus, New Relic, Sysdig, Elastic or any other database of your choice.","title":"Using Metrics in Experiments"},{"location":"metrics/using-metrics/#list-metrics","text":"Find the set Iter8 metrics available in your cluster using kubectl get . kubectl get metrics.iter8.tools --all-namespaces NAMESPACE NAME TYPE DESCRIPTION iter8-kfserving user-engagement Gauge Average duration of a session iter8-system error-count Counter Number of responses with HTTP status code 4xx or 5xx ( Iter8 builtin metric ) iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx ( Iter8 builtin metric ) iter8-system latency-50th-percentile Gauge 50th percentile ( median ) latency ( Iter8 builtin metric ) iter8-system latency-75th-percentile Gauge 75th percentile latency ( Iter8 builtin metric ) iter8-system latency-90th-percentile Gauge 90th percentile latency ( Iter8 builtin metric ) iter8-system latency-95th-percentile Gauge 95th percentile latency ( Iter8 builtin metric ) iter8-system latency-99th-percentile Gauge 99th percentile latency ( Iter8 builtin metric ) iter8-system mean-latency Gauge Mean latency ( Iter8 builtin metric ) iter8-system request-count Counter Number of requests ( Iter8 builtin metric )","title":"List metrics"},{"location":"metrics/using-metrics/#referencing-metrics-within-experiments","text":"Use metrics in experiments by referencing them in the criteria section of the experiment manifest. Reference metrics using the namespace/name or name format . Sample experiment illustrating the use of metrics kind : Experiment ... spec : ... criteria : requestCount : iter8-knative/request-count # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\"","title":"Referencing metrics within experiments"},{"location":"metrics/using-metrics/#observing-metric-values","text":"During an experiment, Iter8 reports the metric values observed for each version. Use iter8ctl to observe these metric values in realtime. See here for an example.","title":"Observing metric values"},{"location":"reference/experiment/","text":"Experiment Resource \u00b6 Experiment resource Iter8's Experiment resource type enables application developers and service operators to automate A/B, A/B/n, Canary and Conformance experiments for Kubernetes apps/ML models. The controls provided by the experiment resource type encompass testing, deployment, traffic engineering, and version promotion functions . Sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-knative/request-count rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate Version This document describes version v2alpha2 of Iter8's experiment API. Metadata \u00b6 Standard Kubernetes meta.v1/ObjectMeta resource. Spec \u00b6 Field name Field type Description Required target string Identifies the app under experimentation and determines which experiments can run concurrently. Experiments that have the same target value will not be scheduled concurrently but will be run sequentially in the order of their creation timestamps. Experiments whose target values differ from each other can be scheduled by Iter8 concurrently. Yes strategy Strategy The experimentation strategy which specifies how app versions are tested, how traffic is shifted during experiment, and what tasks are executed at the start and end of the experiment. Yes criteria Criteria Criteria used for evaluating versions. This section includes (business) rewards, service-level objectives (SLOs) and indicators (SLIs). No duration Duration Duration of the experiment. No versionInfo VersionInfo Versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates. No Status \u00b6 Field name Field type Description Required conditions [] ExperimentCondition A set of conditions that express progress of an experiment. No initTime metav1.Time The time the experiment is created. No startTime metav1.Time The time when the first iteration of experiment begins No lastUpdateTime metav1.Time The time when the status was most recently updated. No stage string Indicator of the progress of an experiment. The stage is Waiting before an experiment executes its start action, Initializing while running the start action, Running while the experiment has begun its first iteration and is progressing, Finishing while any finish action is running and Completed when the experiment terminates. No completedIterations int32 Number of completed iterations of the experiment. This is undefined until the experiment reaches the Running stage. No currentWeightDistribution [] WeightData The latest observed split of traffic between versions. Expressed as percentage. Iter8 ensures that this field is current until the final iteration of the experiment. Iter8 will cease to update this field once a finish action is invoked. No analysis Analysis Result of latest query to the Iter8 analytics service. No versionRecommendedForPromotion string The version recommended for promotion. This field is initially populated by Iter8 as the baseline version and continuously updated during the course of the experiment to match the winner. The value of this field is typically used by finish actions to promote a version at the end of an experiment. No metrics [] MetricInfo A list of metrics referenced in the criteria section of this experiment. No message string Human readable message. No Experiment field types \u00b6 Strategy \u00b6 Field name Field type Description Required testingPattern string Determines the logic used to evaluate the app versions and determine the winner of the experiment. Iter8 supports two testing patterns, namely, Canary and Conformance . Yes deploymentPattern string Determines if and how traffic is shifted during an experiment. This field is relevant only for experiments using the Canary testing pattern. Iter8 supports two deployment patterns, namely, Progressive and FixedSplit . No actions map[ActionType][] TaskSpec An action is a sequence of tasks that can be executed by Iter8. ActionType is a string enum with three valid values: start , loop , and finish . The start action, if specified, is executed at the start of the experiment. The loop action, if specified, is executed during every loop of the experiment, after all the iterations within the loop have completed. The finish action, if specified, is executed at the end of the experiment after all the loops have completed. The actions field is used to specify all three types of actions. No TaskSpec \u00b6 Specification of a task that will be executed as part of experiment actions. Tasks are documented here . Field name Field type Description Required task string Name of the task. Task names express both the library and the task within the library in the format 'library/task' . Yes with map[string] apiextensionsv1.JSON Inputs to the task. No Criteria \u00b6 Field name Field type Description Required requestCount string Reference to the metric used to count the number of requests sent to app versions. No rewards Reward [] A list of metrics along with their preferred directions. Currently, this list needs to be of size one. This field can only be used in experiments with A/B and A/B/n testing patterns. No objectives Objective [] A list of metrics along with acceptable upper limits, lower limits, or both upper and lower limits for them. Iter8 will verify if app versions satisfy these objectives. No indicators string[] A list of metric references. Iter8 will collect and report the values of these metrics in addition to those referenced in the objectives section. No Note: References to metric resource objects within experiment criteria should be in the namespace/name format or in the name format. If the name format is used (i.e., if only the name of the metric is specified), then Iter8 searches for the metric in the namespace of the experiment resource. If Iter8 cannot find the metric, then the reference is considered invalid and the experiment will terminate in a failure. Objective \u00b6 Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes upperLimit Quantity Upper limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be below the limit. No lowerLimit Quantity Lower limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be above the limit. No Reward \u00b6 Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes preferredDirection string Indicates if higher values or lower values of this metric are preferable. High and Low are the two permissible values for this string. Yes Duration \u00b6 The duration of the experiment. Field name Field type Description Required maxLoops int32 Maximum number of loops in the experiment. In case of a failure, the experiment may be terminated earlier. Default value = 1. No iterationsPerLoop int32 Number of iterations per experiment loop . In case of a failure, the experiment may be terminated earlier. Default value = 15. No intervalSeconds int32 Duration of a single iteration of the experiment in seconds. Default value = 20 seconds. No Note : Suppose an experiment has maxLoops = x , iterationsPerLoop = y , and intervalSeconds = z . Assuming the experiment does not terminate early due to failures, it would take a minimum of x*y*z seconds to complete. The actual duration may be more due to additional time incurred in acquiring the target , and executing the start , loop and finish actions . VersionInfo \u00b6 spec.versionInfo describes the app versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates . Field name Field type Description Required baseline VersionDetail Details of the current or baseline version. Yes candidates [] VersionDetail Details of the candidate version or versions, if any. No Number of versions Conformance experiments involve only a single version (baseline). Hence, in these experiments, the candidates field must be omitted. A/B and Canary experiments involve two versions, a baseline and a candidate. Hence, the candidates field must be a list of length one in these experiments. A/B/n experiments involve three or more versions. Hence, in these experiments, the candidates field must be of length two or more. VersionDetail \u00b6 Field name Field type Description Required name string Name of the version. Yes variables [] NamedValue Variables are name-value pairs associated with a version. Metrics and tasks within experiment specs can contain strings with placeholders. Iter8 uses variables to substitute placeholders in these strings. No weightObjRef corev1.ObjectReference Reference to a Kubernetes resource and a field-path within the resource. Iter8 uses weightObjRef to get or set weight (traffic percentage) for the version. No MetricInfo \u00b6 Field name Field type Description Required name string Identifies an Iter8 metric using the namespace/name or name format . Yes metric [] Metric Iter8 metric object referenced by name. No ExperimentCondition \u00b6 Conditions express aspects of the progress of an experiment. The Completed condition indicates whether or not an experiment has completed. The Failed condition indicates whether or not an experiment completed successfully or in failure. The TargetAcquired condition indicates that an experiment has acquired the target and is now scheduled to run. At any point in time, for any given target, Iter8 ensures that at most one experiment has the conditions TargetAcquired set to True and Completed set to False . Field name Field type Description Required type string Type of condition. Valid types are TargetAcquired , Completed and Failed . Yes status corev1.ConditionStatus status of condition, one of True , False , or Unknown . Yes lastTransitionTime metav1.Time The last time any field in the condition was changed. No reason string A reason for the change in value. No message string Human readable decription. No Analysis \u00b6 Field name Field type Description Required aggregatedBuiltinHists AggregatedBuiltinHists This field is used to store intermediate results from the metrics/collect task that enables builtin metrics . Reserved for Iter8 internal use. No aggregatedMetrics AggregatedMetricsAnalysis Most recently observed metric values for all metrics referenced in the experiment criteria. No winnerAssessment WinnerAssessmentAnalysis Information about the winner of the experiment. No versionAssessments VersionAssessmentAnalysis For each version, a summary analysis identifying whether or not the version is satisfying the experiment criteria. No weights WeightsAnalysis Recommended weight distribution to be applied before the next iteration of the experiment. No AggregatedBuiltinHists \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 builtin metrics collect task is the only valid value for this field. Reserved for Iter8 internal use. Yes timestamp metav1.Time The time when this field was last updated. Reserved for Iter8 internal use. Yes message string Human readable message. Reserved for Iter8 internal use. No data apiextensionsv1.JSON Aggregated histogram data for storing intermediate results for builtin metics collection. Reserved for Iter8 internal use. No VersionAssessmentAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string][]bool map of version name to a list of boolean values, one for each objective specified in the experiment criteria, indicating whether not the objective is satisified. No AggregatedMetricsAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string] AggregatedMetricsData Map from metric name to most recent data (from all versions) for the metric. Yes AggregatedMetricsData \u00b6 Field name Field type Description Required max Quantity The maximum value observed for this metric accross all versions. Yes min Quantity The minimum value observed for this metric accross all versions. Yes data map[string] AggregatedMetricsVersionData A map from version name to the most recent aggregated metrics data for that version. No AggregatedMetricsVersionData \u00b6 Field name Field type Description Required max Quantity The maximum value observed for this metric for this version over all observations. No min Quantity The minimum value observed for this metric for this version over all observations. No value Quantity The value. No sampleSize int32 The number of requests observed by this version. No WinnerAssessmentAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data WinnerAssessmentData Details on whether or not a winner has been identified and which version if so. No WinnerAssessmentData \u00b6 Field name Field type Description Required winnerFound bool Whether or not a winner has been identified. Yes winner string The name of the identified winner, if one has been found. No WeightAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data [] WeightData List of version name/value pairs representing a recommended weight for each version No WeightData \u00b6 Field name Field type Description Required name string Version name Yes value int32 Percentage of traffic being sent to the version. Yes Common field types \u00b6 NamedValue \u00b6 Field name Field type Description Required name string Name of a variable. Yes value string Value of a variable. Yes A/B/n experiments involve more than one candidate. Their description is coming soon. \u21a9","title":"Experiment resource"},{"location":"reference/experiment/#experiment-resource","text":"Experiment resource Iter8's Experiment resource type enables application developers and service operators to automate A/B, A/B/n, Canary and Conformance experiments for Kubernetes apps/ML models. The controls provided by the experiment resource type encompass testing, deployment, traffic engineering, and version promotion functions . Sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-knative/request-count rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate Version This document describes version v2alpha2 of Iter8's experiment API.","title":"Experiment Resource"},{"location":"reference/experiment/#metadata","text":"Standard Kubernetes meta.v1/ObjectMeta resource.","title":"Metadata"},{"location":"reference/experiment/#spec","text":"Field name Field type Description Required target string Identifies the app under experimentation and determines which experiments can run concurrently. Experiments that have the same target value will not be scheduled concurrently but will be run sequentially in the order of their creation timestamps. Experiments whose target values differ from each other can be scheduled by Iter8 concurrently. Yes strategy Strategy The experimentation strategy which specifies how app versions are tested, how traffic is shifted during experiment, and what tasks are executed at the start and end of the experiment. Yes criteria Criteria Criteria used for evaluating versions. This section includes (business) rewards, service-level objectives (SLOs) and indicators (SLIs). No duration Duration Duration of the experiment. No versionInfo VersionInfo Versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates. No","title":"Spec"},{"location":"reference/experiment/#status","text":"Field name Field type Description Required conditions [] ExperimentCondition A set of conditions that express progress of an experiment. No initTime metav1.Time The time the experiment is created. No startTime metav1.Time The time when the first iteration of experiment begins No lastUpdateTime metav1.Time The time when the status was most recently updated. No stage string Indicator of the progress of an experiment. The stage is Waiting before an experiment executes its start action, Initializing while running the start action, Running while the experiment has begun its first iteration and is progressing, Finishing while any finish action is running and Completed when the experiment terminates. No completedIterations int32 Number of completed iterations of the experiment. This is undefined until the experiment reaches the Running stage. No currentWeightDistribution [] WeightData The latest observed split of traffic between versions. Expressed as percentage. Iter8 ensures that this field is current until the final iteration of the experiment. Iter8 will cease to update this field once a finish action is invoked. No analysis Analysis Result of latest query to the Iter8 analytics service. No versionRecommendedForPromotion string The version recommended for promotion. This field is initially populated by Iter8 as the baseline version and continuously updated during the course of the experiment to match the winner. The value of this field is typically used by finish actions to promote a version at the end of an experiment. No metrics [] MetricInfo A list of metrics referenced in the criteria section of this experiment. No message string Human readable message. No","title":"Status"},{"location":"reference/experiment/#experiment-field-types","text":"","title":"Experiment field types"},{"location":"reference/experiment/#strategy","text":"Field name Field type Description Required testingPattern string Determines the logic used to evaluate the app versions and determine the winner of the experiment. Iter8 supports two testing patterns, namely, Canary and Conformance . Yes deploymentPattern string Determines if and how traffic is shifted during an experiment. This field is relevant only for experiments using the Canary testing pattern. Iter8 supports two deployment patterns, namely, Progressive and FixedSplit . No actions map[ActionType][] TaskSpec An action is a sequence of tasks that can be executed by Iter8. ActionType is a string enum with three valid values: start , loop , and finish . The start action, if specified, is executed at the start of the experiment. The loop action, if specified, is executed during every loop of the experiment, after all the iterations within the loop have completed. The finish action, if specified, is executed at the end of the experiment after all the loops have completed. The actions field is used to specify all three types of actions. No","title":"Strategy"},{"location":"reference/experiment/#taskspec","text":"Specification of a task that will be executed as part of experiment actions. Tasks are documented here . Field name Field type Description Required task string Name of the task. Task names express both the library and the task within the library in the format 'library/task' . Yes with map[string] apiextensionsv1.JSON Inputs to the task. No","title":"TaskSpec"},{"location":"reference/experiment/#criteria","text":"Field name Field type Description Required requestCount string Reference to the metric used to count the number of requests sent to app versions. No rewards Reward [] A list of metrics along with their preferred directions. Currently, this list needs to be of size one. This field can only be used in experiments with A/B and A/B/n testing patterns. No objectives Objective [] A list of metrics along with acceptable upper limits, lower limits, or both upper and lower limits for them. Iter8 will verify if app versions satisfy these objectives. No indicators string[] A list of metric references. Iter8 will collect and report the values of these metrics in addition to those referenced in the objectives section. No Note: References to metric resource objects within experiment criteria should be in the namespace/name format or in the name format. If the name format is used (i.e., if only the name of the metric is specified), then Iter8 searches for the metric in the namespace of the experiment resource. If Iter8 cannot find the metric, then the reference is considered invalid and the experiment will terminate in a failure.","title":"Criteria"},{"location":"reference/experiment/#objective","text":"Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes upperLimit Quantity Upper limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be below the limit. No lowerLimit Quantity Lower limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be above the limit. No","title":"Objective"},{"location":"reference/experiment/#reward","text":"Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes preferredDirection string Indicates if higher values or lower values of this metric are preferable. High and Low are the two permissible values for this string. Yes","title":"Reward"},{"location":"reference/experiment/#duration","text":"The duration of the experiment. Field name Field type Description Required maxLoops int32 Maximum number of loops in the experiment. In case of a failure, the experiment may be terminated earlier. Default value = 1. No iterationsPerLoop int32 Number of iterations per experiment loop . In case of a failure, the experiment may be terminated earlier. Default value = 15. No intervalSeconds int32 Duration of a single iteration of the experiment in seconds. Default value = 20 seconds. No Note : Suppose an experiment has maxLoops = x , iterationsPerLoop = y , and intervalSeconds = z . Assuming the experiment does not terminate early due to failures, it would take a minimum of x*y*z seconds to complete. The actual duration may be more due to additional time incurred in acquiring the target , and executing the start , loop and finish actions .","title":"Duration"},{"location":"reference/experiment/#versioninfo","text":"spec.versionInfo describes the app versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates . Field name Field type Description Required baseline VersionDetail Details of the current or baseline version. Yes candidates [] VersionDetail Details of the candidate version or versions, if any. No Number of versions Conformance experiments involve only a single version (baseline). Hence, in these experiments, the candidates field must be omitted. A/B and Canary experiments involve two versions, a baseline and a candidate. Hence, the candidates field must be a list of length one in these experiments. A/B/n experiments involve three or more versions. Hence, in these experiments, the candidates field must be of length two or more.","title":"VersionInfo"},{"location":"reference/experiment/#versiondetail","text":"Field name Field type Description Required name string Name of the version. Yes variables [] NamedValue Variables are name-value pairs associated with a version. Metrics and tasks within experiment specs can contain strings with placeholders. Iter8 uses variables to substitute placeholders in these strings. No weightObjRef corev1.ObjectReference Reference to a Kubernetes resource and a field-path within the resource. Iter8 uses weightObjRef to get or set weight (traffic percentage) for the version. No","title":"VersionDetail"},{"location":"reference/experiment/#metricinfo","text":"Field name Field type Description Required name string Identifies an Iter8 metric using the namespace/name or name format . Yes metric [] Metric Iter8 metric object referenced by name. No","title":"MetricInfo"},{"location":"reference/experiment/#experimentcondition","text":"Conditions express aspects of the progress of an experiment. The Completed condition indicates whether or not an experiment has completed. The Failed condition indicates whether or not an experiment completed successfully or in failure. The TargetAcquired condition indicates that an experiment has acquired the target and is now scheduled to run. At any point in time, for any given target, Iter8 ensures that at most one experiment has the conditions TargetAcquired set to True and Completed set to False . Field name Field type Description Required type string Type of condition. Valid types are TargetAcquired , Completed and Failed . Yes status corev1.ConditionStatus status of condition, one of True , False , or Unknown . Yes lastTransitionTime metav1.Time The last time any field in the condition was changed. No reason string A reason for the change in value. No message string Human readable decription. No","title":"ExperimentCondition"},{"location":"reference/experiment/#analysis","text":"Field name Field type Description Required aggregatedBuiltinHists AggregatedBuiltinHists This field is used to store intermediate results from the metrics/collect task that enables builtin metrics . Reserved for Iter8 internal use. No aggregatedMetrics AggregatedMetricsAnalysis Most recently observed metric values for all metrics referenced in the experiment criteria. No winnerAssessment WinnerAssessmentAnalysis Information about the winner of the experiment. No versionAssessments VersionAssessmentAnalysis For each version, a summary analysis identifying whether or not the version is satisfying the experiment criteria. No weights WeightsAnalysis Recommended weight distribution to be applied before the next iteration of the experiment. No","title":"Analysis"},{"location":"reference/experiment/#aggregatedbuiltinhists","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 builtin metrics collect task is the only valid value for this field. Reserved for Iter8 internal use. Yes timestamp metav1.Time The time when this field was last updated. Reserved for Iter8 internal use. Yes message string Human readable message. Reserved for Iter8 internal use. No data apiextensionsv1.JSON Aggregated histogram data for storing intermediate results for builtin metics collection. Reserved for Iter8 internal use. No","title":"AggregatedBuiltinHists"},{"location":"reference/experiment/#versionassessmentanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string][]bool map of version name to a list of boolean values, one for each objective specified in the experiment criteria, indicating whether not the objective is satisified. No","title":"VersionAssessmentAnalysis"},{"location":"reference/experiment/#aggregatedmetricsanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string] AggregatedMetricsData Map from metric name to most recent data (from all versions) for the metric. Yes","title":"AggregatedMetricsAnalysis"},{"location":"reference/experiment/#aggregatedmetricsdata","text":"Field name Field type Description Required max Quantity The maximum value observed for this metric accross all versions. Yes min Quantity The minimum value observed for this metric accross all versions. Yes data map[string] AggregatedMetricsVersionData A map from version name to the most recent aggregated metrics data for that version. No","title":"AggregatedMetricsData"},{"location":"reference/experiment/#aggregatedmetricsversiondata","text":"Field name Field type Description Required max Quantity The maximum value observed for this metric for this version over all observations. No min Quantity The minimum value observed for this metric for this version over all observations. No value Quantity The value. No sampleSize int32 The number of requests observed by this version. No","title":"AggregatedMetricsVersionData"},{"location":"reference/experiment/#winnerassessmentanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data WinnerAssessmentData Details on whether or not a winner has been identified and which version if so. No","title":"WinnerAssessmentAnalysis"},{"location":"reference/experiment/#winnerassessmentdata","text":"Field name Field type Description Required winnerFound bool Whether or not a winner has been identified. Yes winner string The name of the identified winner, if one has been found. No","title":"WinnerAssessmentData"},{"location":"reference/experiment/#weightanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data [] WeightData List of version name/value pairs representing a recommended weight for each version No","title":"WeightAnalysis"},{"location":"reference/experiment/#weightdata","text":"Field name Field type Description Required name string Version name Yes value int32 Percentage of traffic being sent to the version. Yes","title":"WeightData"},{"location":"reference/experiment/#common-field-types","text":"","title":"Common field types"},{"location":"reference/experiment/#namedvalue","text":"Field name Field type Description Required name string Name of a variable. Yes value string Value of a variable. Yes A/B/n experiments involve more than one candidate. Their description is coming soon. \u21a9","title":"NamedValue"},{"location":"reference/metrics/","text":"Metric Resource \u00b6 Metric resource Iter8 defines the Metric resource type, which encapsulates the REST query that is used to retrieve a metric value from the metrics provider. Metric resources are referenced in experiments. Version This document describes version v2alpha2 of Iter8's metric API. Metrics usage is documented here and creation of metrics is documented here . Sample metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[$elapsedTime])) or on() vector(0) description : Number of requests type : counter provider : prometheus jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Metadata \u00b6 Standard Kubernetes meta.v1/ObjectMeta resource. Spec \u00b6 Field name Field type Description Required description string Human readable description. This field is meant for informational purposes. No units string Units of measurement. This field is meant for informational purposes. No provider string Type of the metrics provider (example, prometheus , newrelic , sysdig , elastic , ...). The keyword iter8 is reserved for Iter8 builtin metrics. No params [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP query parameters used by Iter8 when querying the metrics provider. Each name represents a parameter name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. No body string String used to construct the JSON body of the HTTP request. Body may be templated, in which Iter8 will attempt to substitute placeholders in the template at query time using version information. No type string Metric type. Valid values are Counter and Gauge . Default value = Gauge . A Counter metric is one whose value never decreases over time. A Gauge metric is one whose value may increase or decrease over time. No method string HTTP method (verb) used in the HTTP request. Valid values are GET and POST . Default value = GET . No authType string Identifies the type of authentication used in the HTTP request. Valid values are Basic , Bearer and APIKey which correspond to HTTP authentication with these respective methods. No sampleSize string Reference to a metric that represents the number of data points over which the value of this metric is computed. This field applies only to Gauge metrics. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace of the referring metric. No secret string Reference to a secret that contains information used for authenticating with the metrics provider. In particular, Iter8 uses data in this secret to substitute placeholders in the HTTP headers and URL while querying the provider. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace where Iter8 is installed (which is iter8-system by default). No headerTemplates [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP request headers used by Iter8 when querying the metrics provider. Each name represents a header field name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. Placeholder substitution is attempted only if authType and secret fields are present. No jqExpression string The jq expression used by Iter8 to extract the metric value from the JSON response returned by the provider. Yes urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. Yes","title":"Metric resource"},{"location":"reference/metrics/#metric-resource","text":"Metric resource Iter8 defines the Metric resource type, which encapsulates the REST query that is used to retrieve a metric value from the metrics provider. Metric resources are referenced in experiments. Version This document describes version v2alpha2 of Iter8's metric API. Metrics usage is documented here and creation of metrics is documented here . Sample metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[$elapsedTime])) or on() vector(0) description : Number of requests type : counter provider : prometheus jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query","title":"Metric Resource"},{"location":"reference/metrics/#metadata","text":"Standard Kubernetes meta.v1/ObjectMeta resource.","title":"Metadata"},{"location":"reference/metrics/#spec","text":"Field name Field type Description Required description string Human readable description. This field is meant for informational purposes. No units string Units of measurement. This field is meant for informational purposes. No provider string Type of the metrics provider (example, prometheus , newrelic , sysdig , elastic , ...). The keyword iter8 is reserved for Iter8 builtin metrics. No params [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP query parameters used by Iter8 when querying the metrics provider. Each name represents a parameter name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. No body string String used to construct the JSON body of the HTTP request. Body may be templated, in which Iter8 will attempt to substitute placeholders in the template at query time using version information. No type string Metric type. Valid values are Counter and Gauge . Default value = Gauge . A Counter metric is one whose value never decreases over time. A Gauge metric is one whose value may increase or decrease over time. No method string HTTP method (verb) used in the HTTP request. Valid values are GET and POST . Default value = GET . No authType string Identifies the type of authentication used in the HTTP request. Valid values are Basic , Bearer and APIKey which correspond to HTTP authentication with these respective methods. No sampleSize string Reference to a metric that represents the number of data points over which the value of this metric is computed. This field applies only to Gauge metrics. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace of the referring metric. No secret string Reference to a secret that contains information used for authenticating with the metrics provider. In particular, Iter8 uses data in this secret to substitute placeholders in the HTTP headers and URL while querying the provider. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace where Iter8 is installed (which is iter8-system by default). No headerTemplates [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP request headers used by Iter8 when querying the metrics provider. Each name represents a header field name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. Placeholder substitution is attempted only if authType and secret fields are present. No jqExpression string The jq expression used by Iter8 to extract the metric value from the JSON response returned by the provider. Yes urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. Yes","title":"Spec"},{"location":"reference/tasks/","text":"Tasks \u00b6 Tasks are an extension mechanism for enhancing the behavior of Iter8 experiments and can be specified within the spec.strategy.actions field of the experiment. Tasks are grouped into libraries. The following task libraries are available. common library Task for executing a shell command. metrics library Task for collecting builtin metrics. The above task can also be used to generate requests for app/ML model versions without collecting builtin metrics. notification library Task for sending a Slack notification.","title":"Tasks"},{"location":"reference/tasks/#tasks","text":"Tasks are an extension mechanism for enhancing the behavior of Iter8 experiments and can be specified within the spec.strategy.actions field of the experiment. Tasks are grouped into libraries. The following task libraries are available. common library Task for executing a shell command. metrics library Task for collecting builtin metrics. The above task can also be used to generate requests for app/ML model versions without collecting builtin metrics. notification library Task for sending a Slack notification.","title":"Tasks"},{"location":"reference/tasks/common/","text":"Common Tasks \u00b6 common/exec \u00b6 Overview \u00b6 The common/exec task executes a shell command with arguments. Arguments are specified using placeholders that are dynamically substituted at runtime. The common/exec task can be used as part of a finish action to promote the winning version at the end of an experiment. Example \u00b6 The following (partially-specified) experiment executes kubectl apply using a YAML manifest at the end of the experiment. The URL for the manifest contains a placeholder .promote , which is dynamically substituted at the end of the experiment. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml ... versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 variables : - name : promote value : baseline candidates : - name : sample-app-v2 variables : - name : promote value : candidate Inputs \u00b6 Field name Field type Description Required cmd string The command that should be executed Yes args []string A list of command line arguments that should be passed to cmd . No disableInterpolation bool Flag indicating whether or not to disable placeholder subsitution. For details, see below . Default is false . No Result \u00b6 The command with the supplied arguments will be executed. In the example above , a YAML file corresponding to the baseline or candidate version will be applied to the cluster. If this task exits with a non-zero error code, the experiment to which it belongs will fail. Dynamic placeholder substitution \u00b6 Inputs to tasks can contain placeholders, or template variables, which will be dynamically substituted when the task is executed by Iter8. In the example above , one input is: kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/ {{ .promote }} .yaml In this case, the placeholder is {{ .promote }} . Placeholder substitution in task inputs works as follows. Iter8 will find the version recommended for promotion which is determined by Iter8 during the course of the experiment, and stored in the status.versionRecommendedForPromotion field of the experiment resource. The version recommended for promotion is the winner, if a winner has been found in the experiment. Otherwise, it is the baseline version supplied in the spec.versionInfo field of the experiment. If the placeholder is {{ .name }} , Iter8 will substitute it with the name of the version recommended for promotion. If it is any other variable, Iter8 will substitute it with the value of the corresponding variable for the version recommended for promotion. Variable values are specified in the variables field of the version detail when the experiment is created. Disabling placeholder substitution \u00b6 By default, the common/exec task will attempt to find the version recommended for promotion, and use the values defined for it (in the spec.versionInfo portion of the experiment). However, this behavior will lead to task failure when the version recommended for promotion is not available. This is usually the case when a start task is executed. To use the common/exec task as part of an experiment start action, set disableInterpolation to true.","title":"Common Tasks"},{"location":"reference/tasks/common/#common-tasks","text":"","title":"Common Tasks"},{"location":"reference/tasks/common/#commonexec","text":"","title":"common/exec"},{"location":"reference/tasks/common/#overview","text":"The common/exec task executes a shell command with arguments. Arguments are specified using placeholders that are dynamically substituted at runtime. The common/exec task can be used as part of a finish action to promote the winning version at the end of an experiment.","title":"Overview"},{"location":"reference/tasks/common/#example","text":"The following (partially-specified) experiment executes kubectl apply using a YAML manifest at the end of the experiment. The URL for the manifest contains a placeholder .promote , which is dynamically substituted at the end of the experiment. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml ... versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 variables : - name : promote value : baseline candidates : - name : sample-app-v2 variables : - name : promote value : candidate","title":"Example"},{"location":"reference/tasks/common/#inputs","text":"Field name Field type Description Required cmd string The command that should be executed Yes args []string A list of command line arguments that should be passed to cmd . No disableInterpolation bool Flag indicating whether or not to disable placeholder subsitution. For details, see below . Default is false . No","title":"Inputs"},{"location":"reference/tasks/common/#result","text":"The command with the supplied arguments will be executed. In the example above , a YAML file corresponding to the baseline or candidate version will be applied to the cluster. If this task exits with a non-zero error code, the experiment to which it belongs will fail.","title":"Result"},{"location":"reference/tasks/common/#dynamic-placeholder-substitution","text":"Inputs to tasks can contain placeholders, or template variables, which will be dynamically substituted when the task is executed by Iter8. In the example above , one input is: kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/ {{ .promote }} .yaml In this case, the placeholder is {{ .promote }} . Placeholder substitution in task inputs works as follows. Iter8 will find the version recommended for promotion which is determined by Iter8 during the course of the experiment, and stored in the status.versionRecommendedForPromotion field of the experiment resource. The version recommended for promotion is the winner, if a winner has been found in the experiment. Otherwise, it is the baseline version supplied in the spec.versionInfo field of the experiment. If the placeholder is {{ .name }} , Iter8 will substitute it with the name of the version recommended for promotion. If it is any other variable, Iter8 will substitute it with the value of the corresponding variable for the version recommended for promotion. Variable values are specified in the variables field of the version detail when the experiment is created.","title":"Dynamic placeholder substitution"},{"location":"reference/tasks/common/#disabling-placeholder-substitution","text":"By default, the common/exec task will attempt to find the version recommended for promotion, and use the values defined for it (in the spec.versionInfo portion of the experiment). However, this behavior will lead to task failure when the version recommended for promotion is not available. This is usually the case when a start task is executed. To use the common/exec task as part of an experiment start action, set disableInterpolation to true.","title":"Disabling placeholder substitution"},{"location":"reference/tasks/metrics/","text":"Metrics Tasks \u00b6 metrics/collect \u00b6 Overview \u00b6 The metrics/collect takes enables collection of builtin metrics . It generates a stream of HTTP requests to one or more app/ML model versions, and collects latency/error metrics. Example \u00b6 The following start action contains a metrics/collect task which is executed at the start of the experiment. The task sends a certain number of HTTP requests to each version specified in the task, and collects builtin latency/error metrics for them. start : - task : metrics/collect with : versions : # Version names must be unique. # Each version name in the task must match the name of some version # in the versionInfo field of the experiment spec. - name : iter8-app # URL is where this version receives HTTP requests url : http://iter8-app.default.svc:8000 - name : iter8-app-candidate url : http://iter8-app-candidate.default.svc:8000 Inputs \u00b6 Field name Field type Description Required time string Duration of the metrics/collect task run. Specified in the Go duration string format . Default value is 5s . No payloadURL string URL of JSON-encoded data. If this field is specified, the metrics collector will send HTTP POST requests to versions, and the POST requests will contain this JSON data as payload. No versions [] Version A non-empty list of versions. Yes loadOnly bool If set to true, this task will send requests without collecting metrics. Default value is false . No Version \u00b6 Field name Field type Description Required name string Name of the version. Version names must be unique and must match one of the version names in the VersionInfo field of the experiment. Yes qps float How many queries per second will be sent to this version. Default is 8.0. No headers map[string]string HTTP headers to be used in requests sent to this version. No url string HTTP URL of this version. Yes Result \u00b6 This task will run for the specified duration ( time ), send requests to each version ( versions ) at the specified rate ( qps ), and will collect built-in metrics for each version. Builtin metric values are stored in the metrics field of the experiment status in the same manner as custom metric values. The task may result in an error, for instance, if one or more required fields are missing or if URLs are mis-specified. In this case, the experiment to which it belongs will fail. Start vs loop actions \u00b6 If this task is embedded in start actions, it will run once at the beginning of the experiment. If this task is embedded in loop actions, it will run in each loop of the experiment. The results from each run will be aggregated. Load generation without metrics collection \u00b6 You can use this task to send HTTP GET and POST requests to app/ML model versions without collecting metrics by setting the loadOnly input to true .","title":"Metrics Tasks"},{"location":"reference/tasks/metrics/#metrics-tasks","text":"","title":"Metrics Tasks"},{"location":"reference/tasks/metrics/#metricscollect","text":"","title":"metrics/collect"},{"location":"reference/tasks/metrics/#overview","text":"The metrics/collect takes enables collection of builtin metrics . It generates a stream of HTTP requests to one or more app/ML model versions, and collects latency/error metrics.","title":"Overview"},{"location":"reference/tasks/metrics/#example","text":"The following start action contains a metrics/collect task which is executed at the start of the experiment. The task sends a certain number of HTTP requests to each version specified in the task, and collects builtin latency/error metrics for them. start : - task : metrics/collect with : versions : # Version names must be unique. # Each version name in the task must match the name of some version # in the versionInfo field of the experiment spec. - name : iter8-app # URL is where this version receives HTTP requests url : http://iter8-app.default.svc:8000 - name : iter8-app-candidate url : http://iter8-app-candidate.default.svc:8000","title":"Example"},{"location":"reference/tasks/metrics/#inputs","text":"Field name Field type Description Required time string Duration of the metrics/collect task run. Specified in the Go duration string format . Default value is 5s . No payloadURL string URL of JSON-encoded data. If this field is specified, the metrics collector will send HTTP POST requests to versions, and the POST requests will contain this JSON data as payload. No versions [] Version A non-empty list of versions. Yes loadOnly bool If set to true, this task will send requests without collecting metrics. Default value is false . No","title":"Inputs"},{"location":"reference/tasks/metrics/#version","text":"Field name Field type Description Required name string Name of the version. Version names must be unique and must match one of the version names in the VersionInfo field of the experiment. Yes qps float How many queries per second will be sent to this version. Default is 8.0. No headers map[string]string HTTP headers to be used in requests sent to this version. No url string HTTP URL of this version. Yes","title":"Version"},{"location":"reference/tasks/metrics/#result","text":"This task will run for the specified duration ( time ), send requests to each version ( versions ) at the specified rate ( qps ), and will collect built-in metrics for each version. Builtin metric values are stored in the metrics field of the experiment status in the same manner as custom metric values. The task may result in an error, for instance, if one or more required fields are missing or if URLs are mis-specified. In this case, the experiment to which it belongs will fail.","title":"Result"},{"location":"reference/tasks/metrics/#start-vs-loop-actions","text":"If this task is embedded in start actions, it will run once at the beginning of the experiment. If this task is embedded in loop actions, it will run in each loop of the experiment. The results from each run will be aggregated.","title":"Start vs loop actions"},{"location":"reference/tasks/metrics/#load-generation-without-metrics-collection","text":"You can use this task to send HTTP GET and POST requests to app/ML model versions without collecting metrics by setting the loadOnly input to true .","title":"Load generation without metrics collection"},{"location":"reference/tasks/notification/","text":"Notification Tasks \u00b6 notification/slack \u00b6 Overview \u00b6 The notification/slack task posts a Slack message about current state of the experiment. Example \u00b6 The following task notifies a Slack channel with id C0138103183 and using the token contained in the secret slack-token in the ns namespace. task : notification/slack with : channel : C0138103183 secret : ns/slack-token Inputs \u00b6 Field name Field type Description Required channel string Name of the Slack channel to which messages should be posted. Yes secret string Identifies a secret containing a token to be used for authentication. Expressed as namespace/name . If namespace is not specified, the namespace of the experiment is used. Yes Result \u00b6 A Slack message describing the experiment will be posted to the specified channel. Below is a sample Slack notification from this task. Requirements \u00b6 Slack API token \u00b6 An API token allowing posting messages to the desired Slack channel is needed. To obtain a suitable token, see Sending messages using Incoming Webhooks . Once you have the token, store it in a Kubernetes secret. For example, to create the secret slack-secret in the default namespace: kubectl create secret generic slack-secret --from-literal = token = <slack token> Permission to read secret with Slack token \u00b6 The Iter8 task runner needs permission to read the identified secret. For example the following RBAC changes will allow the task runner read the secret from the default namespace: kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/tasks/rbac/read-secrets.yaml Inspect role and rolebinding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # This role enables reading of secrets apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : iter8-secret-reader rules : - apiGroups : - \"\" resources : - secrets verbs : [ \"get\" , \"list\" ] --- # This role binding enables Iter8 handler to read secrets in the default namespace. # To change the namespace apply to the target namespace apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : iter8-secret-reader-handler roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : iter8-secret-reader subjects : - kind : ServiceAccount name : iter8-handlers namespace : iter8-system Slack channel ID \u00b6 A Slack channel is identified by an id. To find the id, open the Slack channel in a web browser. The channel id is the portion of the URL of the form: CXXXXXXXX","title":"Notification Tasks"},{"location":"reference/tasks/notification/#notification-tasks","text":"","title":"Notification Tasks"},{"location":"reference/tasks/notification/#notificationslack","text":"","title":"notification/slack"},{"location":"reference/tasks/notification/#overview","text":"The notification/slack task posts a Slack message about current state of the experiment.","title":"Overview"},{"location":"reference/tasks/notification/#example","text":"The following task notifies a Slack channel with id C0138103183 and using the token contained in the secret slack-token in the ns namespace. task : notification/slack with : channel : C0138103183 secret : ns/slack-token","title":"Example"},{"location":"reference/tasks/notification/#inputs","text":"Field name Field type Description Required channel string Name of the Slack channel to which messages should be posted. Yes secret string Identifies a secret containing a token to be used for authentication. Expressed as namespace/name . If namespace is not specified, the namespace of the experiment is used. Yes","title":"Inputs"},{"location":"reference/tasks/notification/#result","text":"A Slack message describing the experiment will be posted to the specified channel. Below is a sample Slack notification from this task.","title":"Result"},{"location":"reference/tasks/notification/#requirements","text":"","title":"Requirements"},{"location":"reference/tasks/notification/#slack-api-token","text":"An API token allowing posting messages to the desired Slack channel is needed. To obtain a suitable token, see Sending messages using Incoming Webhooks . Once you have the token, store it in a Kubernetes secret. For example, to create the secret slack-secret in the default namespace: kubectl create secret generic slack-secret --from-literal = token = <slack token>","title":"Slack API token"},{"location":"reference/tasks/notification/#permission-to-read-secret-with-slack-token","text":"The Iter8 task runner needs permission to read the identified secret. For example the following RBAC changes will allow the task runner read the secret from the default namespace: kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/tasks/rbac/read-secrets.yaml Inspect role and rolebinding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # This role enables reading of secrets apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : iter8-secret-reader rules : - apiGroups : - \"\" resources : - secrets verbs : [ \"get\" , \"list\" ] --- # This role binding enables Iter8 handler to read secrets in the default namespace. # To change the namespace apply to the target namespace apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : iter8-secret-reader-handler roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : iter8-secret-reader subjects : - kind : ServiceAccount name : iter8-handlers namespace : iter8-system","title":"Permission to read secret with Slack token"},{"location":"reference/tasks/notification/#slack-channel-id","text":"A Slack channel is identified by an id. To find the id, open the Slack channel in a web browser. The channel id is the portion of the URL of the form: CXXXXXXXX","title":"Slack channel ID"},{"location":"tutorials/deployment-patterns/fixed-split/","text":"Fixed Split Deployment \u00b6 Scenario: FixedSplit deployment FixedSplit deployment , as the name indicates, is meant for scenarios where you do not want Iter8 to shift traffic between versions during the experiment. In this tutorial, you will: Modify the quick start tutorial to use FixedSplit instead of Progressive deployment. The modified A/B testing experiment with FixedSplit deployment pattern is depicted below. Before you begin... This tutorial is available for the following K8s stacks. Istio KFServing Knative Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup. Steps 1 to 3 \u00b6 Please follow steps 1 through 3 of the quick start tutorial . 4. Create versions and initialize traffic split \u00b6 Istio kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/fixed-split/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : bookinfo spec : gateways : - mesh - bookinfo-gateway hosts : - productpage - \"bookinfo.example.com\" http : - match : - uri : exact : /productpage - uri : prefix : /static - uri : exact : /login - uri : exact : /logout - uri : prefix : /api/v1/products route : - destination : host : productpage port : number : 9080 subset : productpage-v1 weight : 60 - destination : host : productpage port : number : 9080 subset : productpage-v2 weight : 40 KFServing kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 60 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 40 Knative kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Knative service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 60 - tag : candidate latestRevision : true percent : 40 5. Generate requests \u00b6 Please follow Step 5 of the quick start tutorial . 6. Define metrics \u00b6 Please follow Step 6 of the quick start tutorial . 7. Launch experiment \u00b6 Istio kubectl apply -f $ITER8 /samples/istio/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will not shift traffic during iterations deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 300 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml KFServing kubectl apply -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-kfserving/request-count rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-kfserving/mean-latency upperLimit : 1500 - metric : iter8-kfserving/95th-percentile-tail-latency upperLimit : 2000 - metric : iter8-kfserving/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 25 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml Knative kubectl apply -f $ITER8 /samples/knative/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-knative/request-count rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 variables : - name : promote value : baseline candidates : - name : sample-app-v2 variables : - name : promote value : candidate The process automated by Iter8 during this experiment is depicted below. 8. Observe experiment \u00b6 Please follow Step 8 of the quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with fixedsplit-exp in your commands. You can also observe traffic by suitably modifying the commands for observing traffic. Understanding what happened You created two versions of your app/ML model. You generated requests for your app/ML model versions. At the start of the experiment, 60% of the requests are sent to the baseline and 40% to the candidate. You created an Iter8 experiment with A/B testing pattern and FixedSplit deployment pattern. In each iteration, Iter8 observed the latency and error-rate metrics collected by Prometheus, and the user-engagement metric from New Relic; Iter8 verified that the candidate satisfied all objectives, verified that the candidate improved over the baseline in terms of user-engagement, identified candidate as the winner, and finally promoted the candidate. 9. Cleanup \u00b6 Istio kubectl delete -f $ITER8 /samples/istio/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete ns bookinfo-iter8 KFServing kubectl delete -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml Knative kubectl delete -f $ITER8 /samples/knative/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/fortio.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml","title":"FixedSplit deployment"},{"location":"tutorials/deployment-patterns/fixed-split/#fixed-split-deployment","text":"Scenario: FixedSplit deployment FixedSplit deployment , as the name indicates, is meant for scenarios where you do not want Iter8 to shift traffic between versions during the experiment. In this tutorial, you will: Modify the quick start tutorial to use FixedSplit instead of Progressive deployment. The modified A/B testing experiment with FixedSplit deployment pattern is depicted below. Before you begin... This tutorial is available for the following K8s stacks. Istio KFServing Knative Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup.","title":"Fixed Split Deployment"},{"location":"tutorials/deployment-patterns/fixed-split/#steps-1-to-3","text":"Please follow steps 1 through 3 of the quick start tutorial .","title":"Steps 1 to 3"},{"location":"tutorials/deployment-patterns/fixed-split/#4-create-versions-and-initialize-traffic-split","text":"Istio kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/fixed-split/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : bookinfo spec : gateways : - mesh - bookinfo-gateway hosts : - productpage - \"bookinfo.example.com\" http : - match : - uri : exact : /productpage - uri : prefix : /static - uri : exact : /login - uri : exact : /logout - uri : prefix : /api/v1/products route : - destination : host : productpage port : number : 9080 subset : productpage-v1 weight : 60 - destination : host : productpage port : number : 9080 subset : productpage-v2 weight : 40 KFServing kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 60 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 40 Knative kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Knative service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 60 - tag : candidate latestRevision : true percent : 40","title":"4. Create versions and initialize traffic split"},{"location":"tutorials/deployment-patterns/fixed-split/#5-generate-requests","text":"Please follow Step 5 of the quick start tutorial .","title":"5. Generate requests"},{"location":"tutorials/deployment-patterns/fixed-split/#6-define-metrics","text":"Please follow Step 6 of the quick start tutorial .","title":"6. Define metrics"},{"location":"tutorials/deployment-patterns/fixed-split/#7-launch-experiment","text":"Istio kubectl apply -f $ITER8 /samples/istio/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will not shift traffic during iterations deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 300 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml KFServing kubectl apply -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-kfserving/request-count rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-kfserving/mean-latency upperLimit : 1500 - metric : iter8-kfserving/95th-percentile-tail-latency upperLimit : 2000 - metric : iter8-kfserving/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 25 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml Knative kubectl apply -f $ITER8 /samples/knative/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-knative/request-count rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 variables : - name : promote value : baseline candidates : - name : sample-app-v2 variables : - name : promote value : candidate The process automated by Iter8 during this experiment is depicted below.","title":"7. Launch experiment"},{"location":"tutorials/deployment-patterns/fixed-split/#8-observe-experiment","text":"Please follow Step 8 of the quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with fixedsplit-exp in your commands. You can also observe traffic by suitably modifying the commands for observing traffic. Understanding what happened You created two versions of your app/ML model. You generated requests for your app/ML model versions. At the start of the experiment, 60% of the requests are sent to the baseline and 40% to the candidate. You created an Iter8 experiment with A/B testing pattern and FixedSplit deployment pattern. In each iteration, Iter8 observed the latency and error-rate metrics collected by Prometheus, and the user-engagement metric from New Relic; Iter8 verified that the candidate satisfied all objectives, verified that the candidate improved over the baseline in terms of user-engagement, identified candidate as the winner, and finally promoted the candidate.","title":"8. Observe experiment"},{"location":"tutorials/deployment-patterns/fixed-split/#9-cleanup","text":"Istio kubectl delete -f $ITER8 /samples/istio/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete ns bookinfo-iter8 KFServing kubectl delete -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml Knative kubectl delete -f $ITER8 /samples/knative/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/fortio.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml","title":"9. Cleanup"},{"location":"tutorials/deployment-patterns/progressive/","text":"Progressive Deployment \u00b6 Scenario: Progressive deployment Progressive deployment enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment. Progressive deployment is the default deployment pattern for Iter8 experiments. Progressive deployment during an A/B testing experiment is depicted below. Tutorials with progressive deployment \u00b6 The A/B testing (quick start) and canary testing tutorials demonstrate progressive deployment. Specifying weightObjRef \u00b6 Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. Below are a few examples that demonstrate how to specify weightObjRef in experiments. Istio The A/B testing experiment for Istio app uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : productpage-v1 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight KFServing The A/B testing experiment for KFServing model uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight Knative The A/B testing experiment for Knative app uses a Knative service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Knative service and the traffic fields within the Knative service corresponding to the versions. versionInfo : baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent Traffic controls \u00b6 You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Progressive deployment"},{"location":"tutorials/deployment-patterns/progressive/#progressive-deployment","text":"Scenario: Progressive deployment Progressive deployment enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment. Progressive deployment is the default deployment pattern for Iter8 experiments. Progressive deployment during an A/B testing experiment is depicted below.","title":"Progressive Deployment"},{"location":"tutorials/deployment-patterns/progressive/#tutorials-with-progressive-deployment","text":"The A/B testing (quick start) and canary testing tutorials demonstrate progressive deployment.","title":"Tutorials with progressive deployment"},{"location":"tutorials/deployment-patterns/progressive/#specifying-weightobjref","text":"Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. Below are a few examples that demonstrate how to specify weightObjRef in experiments. Istio The A/B testing experiment for Istio app uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : productpage-v1 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight KFServing The A/B testing experiment for KFServing model uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight Knative The A/B testing experiment for Knative app uses a Knative service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Knative service and the traffic fields within the Knative service corresponding to the versions. versionInfo : baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent","title":"Specifying weightObjRef"},{"location":"tutorials/deployment-patterns/progressive/#traffic-controls","text":"You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Traffic controls"},{"location":"tutorials/testing-patterns/ab/","text":"A/B Testing \u00b6 A/B testing pattern is documented as part of quick start .","title":"A/B (quick start)"},{"location":"tutorials/testing-patterns/ab/#ab-testing","text":"A/B testing pattern is documented as part of quick start .","title":"A/B Testing"},{"location":"tutorials/testing-patterns/canary/","text":"Canary Testing \u00b6 Scenario: Canary testing Canary testing enables you to safely rollout a new version of your app/ML model after validating service-level objectives (SLOs). In this tutorial, you will: Perform canary testing. Specify latency and error-rate based service-level objectives (SLOs). If the candidate version satisfies SLOs, Iter8 will declare it as the winner. Use Prometheus as the provider for latency and error-rate metrics. Combine canary testing with progressive deployment . Iter8 will progressively shift the traffic towards the winner and promote it at the end as depicted below. Before you begin... This tutorial is available for the following K8s stacks. Istio Knative Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup. Steps 1 to 6 \u00b6 Please follow steps 1 through 6 of the quick start tutorial . 7. Launch experiment \u00b6 Launch the Iter8 experiment that orchestrates canary testing for the app in this tutorial. Istio kubectl apply -f $ITER8 /samples/istio/canary/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : canary-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Canary test testingPattern : Canary # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : objectives : # metrics used to validate versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used in metric queries value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used in metric queries value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight Knative kubectl apply -f $ITER8 /samples/knative/canary/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : canary-exp spec : target : default/sample-app strategy : testingPattern : Canary deploymentPattern : Progressive actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-knative/request-count objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate The process automated by Iter8 during this experiment is depicted below. 8. Observe experiment \u00b6 Follow step 8 of quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with canary-exp in your commands. You can also observe traffic by suitably modifying the commands for observing traffic. Understanding what happened You created two versions of your app/ML model. You generated requests for your app/ML model versions. At the start of the experiment, 100% of the requests are sent to the baseline and 0% to the candidate. You created an Iter8 experiment with canary testing pattern and progressive deployment pattern. In each iteration, Iter8 observed the latency and error-rate metrics collected by Prometheus; Iter8 verified that the candidate satisfied all the SLOs, identified candidate as the winner, progressively shifted traffic from the baseline to the candidate, and promoted the candidate. 9. Cleanup \u00b6 Istio kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/canary/experiment.yaml kubectl delete namespace bookinfo-iter8 Knative kubectl delete -f $ITER8 /samples/knative/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/knative/canary/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"Canary"},{"location":"tutorials/testing-patterns/canary/#canary-testing","text":"Scenario: Canary testing Canary testing enables you to safely rollout a new version of your app/ML model after validating service-level objectives (SLOs). In this tutorial, you will: Perform canary testing. Specify latency and error-rate based service-level objectives (SLOs). If the candidate version satisfies SLOs, Iter8 will declare it as the winner. Use Prometheus as the provider for latency and error-rate metrics. Combine canary testing with progressive deployment . Iter8 will progressively shift the traffic towards the winner and promote it at the end as depicted below. Before you begin... This tutorial is available for the following K8s stacks. Istio Knative Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup.","title":"Canary Testing"},{"location":"tutorials/testing-patterns/canary/#steps-1-to-6","text":"Please follow steps 1 through 6 of the quick start tutorial .","title":"Steps 1 to 6"},{"location":"tutorials/testing-patterns/canary/#7-launch-experiment","text":"Launch the Iter8 experiment that orchestrates canary testing for the app in this tutorial. Istio kubectl apply -f $ITER8 /samples/istio/canary/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : canary-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Canary test testingPattern : Canary # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : objectives : # metrics used to validate versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used in metric queries value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used in metric queries value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight Knative kubectl apply -f $ITER8 /samples/knative/canary/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : canary-exp spec : target : default/sample-app strategy : testingPattern : Canary deploymentPattern : Progressive actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-knative/request-count objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate The process automated by Iter8 during this experiment is depicted below.","title":"7. Launch experiment"},{"location":"tutorials/testing-patterns/canary/#8-observe-experiment","text":"Follow step 8 of quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with canary-exp in your commands. You can also observe traffic by suitably modifying the commands for observing traffic. Understanding what happened You created two versions of your app/ML model. You generated requests for your app/ML model versions. At the start of the experiment, 100% of the requests are sent to the baseline and 0% to the candidate. You created an Iter8 experiment with canary testing pattern and progressive deployment pattern. In each iteration, Iter8 observed the latency and error-rate metrics collected by Prometheus; Iter8 verified that the candidate satisfied all the SLOs, identified candidate as the winner, progressively shifted traffic from the baseline to the candidate, and promoted the candidate.","title":"8. Observe experiment"},{"location":"tutorials/testing-patterns/canary/#9-cleanup","text":"Istio kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/canary/experiment.yaml kubectl delete namespace bookinfo-iter8 Knative kubectl delete -f $ITER8 /samples/knative/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/knative/canary/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"9. Cleanup"},{"location":"tutorials/testing-patterns/conformance/","text":"Conformance Testing \u00b6 Scenario: Conformance testing Conformance testing enables you to validate a version of your app/ML model using service-level objectives (SLOs). In this tutorial, you will: Perform conformance testing. Specify latency and error-rate based service-level objectives (SLOs). If your version satisfies SLOs, Iter8 will declare it as the winner. Before you begin... This tutorial is available for the following K8s stacks. Istio Knative Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup. Steps 1, 2, and 3 \u00b6 Please follow steps 1, 2, and 3 of the quick start tutorial . 4. Create app/ML model version \u00b6 Istio Deploy bookinfo app: kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/conformance/bookinfo-app.yaml Look inside productpage-v1 defined in bookinfo-app.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v1 labels : app : productpage version : v1 spec : replicas : 1 selector : matchLabels : app : productpage version : v1 template : metadata : annotations : sidecar.istio.io/inject : \"true\" prometheus.io/scrape : \"true\" prometheus.io/path : /metrics prometheus.io/port : \"9080\" labels : app : productpage version : v1 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v1\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"red\" - name : reward_min value : \"0\" - name : reward_max value : \"5\" - name : port value : \"9080\" Knative Deploy a Knative app. kubectl apply -f $ITER8 /samples/knative/conformance/baseline.yaml Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" 5. Generate requests \u00b6 Istio Generate requests using Fortio as follows. kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never Knative Generation of requests is handled automatically by the Iter8 experiment. 6. Define metrics \u00b6 Istio Please follow step 6 of the quick start tutorial . Knative Metrics collection is handled automatically by the Iter8 experiment. 7. Launch experiment \u00b6 Launch the Iter8 experiment that orchestrates conformance testing for the app/ML model in this tutorial. Istio kubectl apply -f $ITER8 /samples/istio/conformance/experiment.yaml Look inside experiment.yaml apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : conformance-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Conformance test testingPattern : Conformance criteria : objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 The process automated by Iter8 during this experiment is depicted below. Knative kubectl apply -f $ITER8 /samples/knative/conformance/experiment.yaml Look inside experiment.yaml apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : conformance-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a conformance test testingPattern : Conformance actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app.default.svc.cluster.local criteria : objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/error-count upperLimit : 0 duration : maxLoops : 10 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 The process automated by Iter8 during this experiment is depicted below. 8. Observe experiment \u00b6 Follow step 8 of quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with conformance-exp in your commands. Understanding what happened You created a single version of an app/ML model. You generated requests for your app/ML model versions. You created an Iter8 experiment with conformance testing pattern. In each iteration, Iter8 observed the latency and error-rate metrics for your application; Iter8 verified that the version (referred to as baseline in a conformance experiment) satisfied all the SLOs, and identified baseline as the winner. 9. Cleanup \u00b6 Istio kubectl delete -f $ITER8 /samples/istio/conformance/fortio.yaml kubectl delete -f $ITER8 /samples/istio/conformance/experiment.yaml kubectl delete ns bookinfo-iter8 Knative kubectl delete -f $ITER8 /samples/knative/conformance/experiment.yaml kubectl delete -f $ITER8 /samples/knative/conformance/baseline.yaml","title":"Conformance"},{"location":"tutorials/testing-patterns/conformance/#conformance-testing","text":"Scenario: Conformance testing Conformance testing enables you to validate a version of your app/ML model using service-level objectives (SLOs). In this tutorial, you will: Perform conformance testing. Specify latency and error-rate based service-level objectives (SLOs). If your version satisfies SLOs, Iter8 will declare it as the winner. Before you begin... This tutorial is available for the following K8s stacks. Istio Knative Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup.","title":"Conformance Testing"},{"location":"tutorials/testing-patterns/conformance/#steps-1-2-and-3","text":"Please follow steps 1, 2, and 3 of the quick start tutorial .","title":"Steps 1, 2, and 3"},{"location":"tutorials/testing-patterns/conformance/#4-create-appml-model-version","text":"Istio Deploy bookinfo app: kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/conformance/bookinfo-app.yaml Look inside productpage-v1 defined in bookinfo-app.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v1 labels : app : productpage version : v1 spec : replicas : 1 selector : matchLabels : app : productpage version : v1 template : metadata : annotations : sidecar.istio.io/inject : \"true\" prometheus.io/scrape : \"true\" prometheus.io/path : /metrics prometheus.io/port : \"9080\" labels : app : productpage version : v1 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v1\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"red\" - name : reward_min value : \"0\" - name : reward_max value : \"5\" - name : port value : \"9080\" Knative Deploy a Knative app. kubectl apply -f $ITER8 /samples/knative/conformance/baseline.yaml Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\"","title":"4. Create app/ML model version"},{"location":"tutorials/testing-patterns/conformance/#5-generate-requests","text":"Istio Generate requests using Fortio as follows. kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never Knative Generation of requests is handled automatically by the Iter8 experiment.","title":"5. Generate requests"},{"location":"tutorials/testing-patterns/conformance/#6-define-metrics","text":"Istio Please follow step 6 of the quick start tutorial . Knative Metrics collection is handled automatically by the Iter8 experiment.","title":"6. Define metrics"},{"location":"tutorials/testing-patterns/conformance/#7-launch-experiment","text":"Launch the Iter8 experiment that orchestrates conformance testing for the app/ML model in this tutorial. Istio kubectl apply -f $ITER8 /samples/istio/conformance/experiment.yaml Look inside experiment.yaml apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : conformance-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Conformance test testingPattern : Conformance criteria : objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 The process automated by Iter8 during this experiment is depicted below. Knative kubectl apply -f $ITER8 /samples/knative/conformance/experiment.yaml Look inside experiment.yaml apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : conformance-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a conformance test testingPattern : Conformance actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app.default.svc.cluster.local criteria : objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/error-count upperLimit : 0 duration : maxLoops : 10 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 The process automated by Iter8 during this experiment is depicted below.","title":"7. Launch experiment"},{"location":"tutorials/testing-patterns/conformance/#8-observe-experiment","text":"Follow step 8 of quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with conformance-exp in your commands. Understanding what happened You created a single version of an app/ML model. You generated requests for your app/ML model versions. You created an Iter8 experiment with conformance testing pattern. In each iteration, Iter8 observed the latency and error-rate metrics for your application; Iter8 verified that the version (referred to as baseline in a conformance experiment) satisfied all the SLOs, and identified baseline as the winner.","title":"8. Observe experiment"},{"location":"tutorials/testing-patterns/conformance/#9-cleanup","text":"Istio kubectl delete -f $ITER8 /samples/istio/conformance/fortio.yaml kubectl delete -f $ITER8 /samples/istio/conformance/experiment.yaml kubectl delete ns bookinfo-iter8 Knative kubectl delete -f $ITER8 /samples/knative/conformance/experiment.yaml kubectl delete -f $ITER8 /samples/knative/conformance/baseline.yaml","title":"9. Cleanup"},{"location":"tutorials/traffic-engineering/mirroring/","text":"Traffic Mirroring \u00b6 Scenario: SLO validation for a dark launched version with mirrored traffic Traffic mirroring or shadowing enables experimenting with a dark launched version with zero-impact on end-users. Mirrored traffic is a replica of the real user requests that is routed to the dark version. Metrics are collected and evaluated for the dark version, but responses from the dark version are ignored. In this tutorial, you will use mirror traffic to a dark launched version as depicted below. Before you begin... This tutorial is available for the following K8s stacks. Knative Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup. Steps 1 to 3 \u00b6 Please follow steps 1 through 3 of the quick start tutorial . 4. Create app with live and dark versions \u00b6 kubectl apply -f $ITER8 /samples/knative/mirroring/service.yaml Look inside service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - revisionName : sample-app-v1 percent : 100 - latestRevision : true percent : 0 5. Create routing rule \u00b6 kubectl apply -f $ITER8 /samples/knative/mirroring/routing-rules.yaml Look inside routing-rules.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : example-mirroring spec : gateways : - mesh - knative-serving/knative-ingress-gateway hosts : - example.com http : - rewrite : authority : example.com route : - destination : host : knative-local-gateway.istio-system.svc.cluster.local mirror : host : knative-local-gateway.istio-system.svc.cluster.local mirrorPercent : 40 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : example-routing spec : gateways : - knative-serving/knative-local-gateway hosts : - \"*\" http : - match : - authority : prefix : example.com-shadow route : - destination : host : sample-app-v2.default.svc.cluster.local port : number : 80 headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v2 - match : - authority : prefix : example.com route : - destination : host : sample-app-v1.default.svc.cluster.local port : number : 80 headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1 6. Generate requests \u00b6 TEMP_DIR = $( mktemp -d ) cd $TEMP_DIR curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .8.2 sh - istio-1.8.2/bin/istioctl kube-inject -f $ITER8 /samples/knative/mirroring/curl.yaml | kubectl create -f - cd $ITER8 Look inside curl.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : batch/v1 kind : Job metadata : name : curl spec : template : spec : activeDeadlineSeconds : 6000 containers : - name : curl image : tutum/curl command : - /bin/sh - -c - | sleep 10.0 while true; do curl -sS example.com sleep 0.5 done restartPolicy : Never 7. Create Iter8 experiment \u00b6 kubectl wait --for = condition = Ready ksvc/sample-app kubectl apply -f $ITER8 /samples/knative/mirroring/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : mirroring spec : target : default/sample-app strategy : testingPattern : Conformance actions : start : - task : knative/init-experiment criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about version used in this experiment baseline : name : current variables : - name : revision value : sample-app-v2 8. Observe experiment \u00b6 Please follow Step 8 of the quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with mirroring in your commands. You can also observe traffic by suitably modifying the commands for observing traffic. Understanding what happened You configured a Knative service with two versions of your app. In the service.yaml manifest, you specified that the live version, sample-app-v1 , should receive 100% of the production traffic and the dark version, sample-app-v2 , should receive 0% of the production traffic. You used example.com as the HTTP host in this tutorial. Note: In your production cluster, use domain(s) that you own in the setup of the virtual services. You set up Istio virtual services which mapped the Knative revisions to the custom domain. The virtual services specified the following routing rules: all HTTP requests with their Host header or :authority pseudo-header set to example.com would be sent to sample-app-v1 . 40% of these requests would be mirrored and sent to sample-app-v2 and responses from sample-app-v2 would be ignored. You generated traffic for example.com using a curl -based job. You injected Istio sidecar injected into it to simulate traffic generation from within the cluster. The sidecar was needed in order to correctly route traffic. Note: You used Istio version 1.8.2 to inject the sidecar. This version of Istio corresponds to the one installed in Step 3 of the quick start tutorial . If you have a different version of Istio installed in your cluster, change the Istio version during sidecar injection appropriately. You created an Iter8 Conformance experiment to evaluate the dark version. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics for the dark version collected by Prometheus, and verified that the dark version satisfied all the objectives specified in experiment.yaml . 9. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/mirroring/curl.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/experiment.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/routing-rules.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/service.yaml","title":"Traffic mirroring/shadowing"},{"location":"tutorials/traffic-engineering/mirroring/#traffic-mirroring","text":"Scenario: SLO validation for a dark launched version with mirrored traffic Traffic mirroring or shadowing enables experimenting with a dark launched version with zero-impact on end-users. Mirrored traffic is a replica of the real user requests that is routed to the dark version. Metrics are collected and evaluated for the dark version, but responses from the dark version are ignored. In this tutorial, you will use mirror traffic to a dark launched version as depicted below. Before you begin... This tutorial is available for the following K8s stacks. Knative Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup.","title":"Traffic Mirroring"},{"location":"tutorials/traffic-engineering/mirroring/#steps-1-to-3","text":"Please follow steps 1 through 3 of the quick start tutorial .","title":"Steps 1 to 3"},{"location":"tutorials/traffic-engineering/mirroring/#4-create-app-with-live-and-dark-versions","text":"kubectl apply -f $ITER8 /samples/knative/mirroring/service.yaml Look inside service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - revisionName : sample-app-v1 percent : 100 - latestRevision : true percent : 0","title":"4. Create app with live and dark versions"},{"location":"tutorials/traffic-engineering/mirroring/#5-create-routing-rule","text":"kubectl apply -f $ITER8 /samples/knative/mirroring/routing-rules.yaml Look inside routing-rules.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : example-mirroring spec : gateways : - mesh - knative-serving/knative-ingress-gateway hosts : - example.com http : - rewrite : authority : example.com route : - destination : host : knative-local-gateway.istio-system.svc.cluster.local mirror : host : knative-local-gateway.istio-system.svc.cluster.local mirrorPercent : 40 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : example-routing spec : gateways : - knative-serving/knative-local-gateway hosts : - \"*\" http : - match : - authority : prefix : example.com-shadow route : - destination : host : sample-app-v2.default.svc.cluster.local port : number : 80 headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v2 - match : - authority : prefix : example.com route : - destination : host : sample-app-v1.default.svc.cluster.local port : number : 80 headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1","title":"5. Create routing rule"},{"location":"tutorials/traffic-engineering/mirroring/#6-generate-requests","text":"TEMP_DIR = $( mktemp -d ) cd $TEMP_DIR curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .8.2 sh - istio-1.8.2/bin/istioctl kube-inject -f $ITER8 /samples/knative/mirroring/curl.yaml | kubectl create -f - cd $ITER8 Look inside curl.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : batch/v1 kind : Job metadata : name : curl spec : template : spec : activeDeadlineSeconds : 6000 containers : - name : curl image : tutum/curl command : - /bin/sh - -c - | sleep 10.0 while true; do curl -sS example.com sleep 0.5 done restartPolicy : Never","title":"6. Generate requests"},{"location":"tutorials/traffic-engineering/mirroring/#7-create-iter8-experiment","text":"kubectl wait --for = condition = Ready ksvc/sample-app kubectl apply -f $ITER8 /samples/knative/mirroring/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : mirroring spec : target : default/sample-app strategy : testingPattern : Conformance actions : start : - task : knative/init-experiment criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about version used in this experiment baseline : name : current variables : - name : revision value : sample-app-v2","title":"7. Create Iter8 experiment"},{"location":"tutorials/traffic-engineering/mirroring/#8-observe-experiment","text":"Please follow Step 8 of the quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with mirroring in your commands. You can also observe traffic by suitably modifying the commands for observing traffic. Understanding what happened You configured a Knative service with two versions of your app. In the service.yaml manifest, you specified that the live version, sample-app-v1 , should receive 100% of the production traffic and the dark version, sample-app-v2 , should receive 0% of the production traffic. You used example.com as the HTTP host in this tutorial. Note: In your production cluster, use domain(s) that you own in the setup of the virtual services. You set up Istio virtual services which mapped the Knative revisions to the custom domain. The virtual services specified the following routing rules: all HTTP requests with their Host header or :authority pseudo-header set to example.com would be sent to sample-app-v1 . 40% of these requests would be mirrored and sent to sample-app-v2 and responses from sample-app-v2 would be ignored. You generated traffic for example.com using a curl -based job. You injected Istio sidecar injected into it to simulate traffic generation from within the cluster. The sidecar was needed in order to correctly route traffic. Note: You used Istio version 1.8.2 to inject the sidecar. This version of Istio corresponds to the one installed in Step 3 of the quick start tutorial . If you have a different version of Istio installed in your cluster, change the Istio version during sidecar injection appropriately. You created an Iter8 Conformance experiment to evaluate the dark version. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics for the dark version collected by Prometheus, and verified that the dark version satisfied all the objectives specified in experiment.yaml .","title":"8. Observe experiment"},{"location":"tutorials/traffic-engineering/mirroring/#9-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/mirroring/curl.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/experiment.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/routing-rules.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/service.yaml","title":"9. Cleanup"},{"location":"tutorials/traffic-engineering/session-affinity/","text":"Session Affinity \u00b6 Scenario: A/B testing with session affinity (sticky sessions) Session affinity ensures that the version to which a particular user's request is routed remains consistent throughout the duration of the experiment. In this tutorial, you will use a FixedSplit deployment in conjunction with session affinity. The experiment will involve two user groups, 1 and 2. Reqeusts from user group 1 will have a userhash header value prefixed with 111 and will be routed to the baseline version. Requests from user group 2 will have a userhash header value prefixed with 101 and will be routed to the candidate version. The session affinity experiment is depicted below. Before you begin... This tutorial is available for the following K8s stacks. KFServing Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup. Steps 1 to 3 \u00b6 Please follow steps 1 through 3 of the quick start tutorial . 4. Create versions and initialize routing rule \u00b6 KFServing kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Virtual service with routing rule 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - match : - headers : userhash : # user hash is a 10-digit random binary string prefix : \"101\" # in expectation, 1/8th of user hashes will match this prefix route : # matching users will always go to v2 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 - route : # non-matching users will always go to v1 - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 5. Generate requests \u00b6 KFServing Generate requests to your model as follows. Port forward (terminal one) INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Baseline requests (terminal two) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1111100000\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 0 .29 done Candidate requests (terminal three) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1010101010\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 2 .0 done 6. Define metrics \u00b6 Please follow Step 6 of the quick start tutorial . 7. Launch experiment \u00b6 KFServing kubectl apply -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : session-affinity-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-kfserving/request-count rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-kfserving/mean-latency upperLimit : 2000 - metric : iter8-kfserving/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-kfserving/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml The process automated by Iter8 during this experiment is depicted below. 8. Observe experiment \u00b6 Please follow Step 8 of the quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with session-affinity-exp in your commands. You can also observe traffic by suitably modifying the commands for observing traffic. Understanding what happened You created two versions of your app/ML model. You generated requests for your app/ML model versions. Throughout the experiment, users from Group 1 (i.e., users whose requests had a userhash header value prefixed with 111 ) were routed to the baseline version; and users from Group 2 (i.e., users whose requests had a userhash header value prefixed with 101 ) were routed to the candidate version. You created an Iter8 experiment with A/B testing pattern and FixedSplit deployment pattern. In each iteration, Iter8 observed the latency and error-rate metrics collected by Prometheus, and the user-engagement metric from New Relic; Iter8 verified that the candidate satisfied all objectives, verified that the candidate improved over the baseline in terms of user-engagement, identified candidate as the winner, and finally promoted the candidate. 9. Cleanup \u00b6 KFServing kubectl delete -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml","title":"Session affinity"},{"location":"tutorials/traffic-engineering/session-affinity/#session-affinity","text":"Scenario: A/B testing with session affinity (sticky sessions) Session affinity ensures that the version to which a particular user's request is routed remains consistent throughout the duration of the experiment. In this tutorial, you will use a FixedSplit deployment in conjunction with session affinity. The experiment will involve two user groups, 1 and 2. Reqeusts from user group 1 will have a userhash header value prefixed with 111 and will be routed to the baseline version. Requests from user group 2 will have a userhash header value prefixed with 101 and will be routed to the candidate version. The session affinity experiment is depicted below. Before you begin... This tutorial is available for the following K8s stacks. KFServing Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup.","title":"Session Affinity"},{"location":"tutorials/traffic-engineering/session-affinity/#steps-1-to-3","text":"Please follow steps 1 through 3 of the quick start tutorial .","title":"Steps 1 to 3"},{"location":"tutorials/traffic-engineering/session-affinity/#4-create-versions-and-initialize-routing-rule","text":"KFServing kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Virtual service with routing rule 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - match : - headers : userhash : # user hash is a 10-digit random binary string prefix : \"101\" # in expectation, 1/8th of user hashes will match this prefix route : # matching users will always go to v2 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 - route : # non-matching users will always go to v1 - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1","title":"4. Create versions and initialize routing rule"},{"location":"tutorials/traffic-engineering/session-affinity/#5-generate-requests","text":"KFServing Generate requests to your model as follows. Port forward (terminal one) INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Baseline requests (terminal two) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1111100000\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 0 .29 done Candidate requests (terminal three) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1010101010\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 2 .0 done","title":"5. Generate requests"},{"location":"tutorials/traffic-engineering/session-affinity/#6-define-metrics","text":"Please follow Step 6 of the quick start tutorial .","title":"6. Define metrics"},{"location":"tutorials/traffic-engineering/session-affinity/#7-launch-experiment","text":"KFServing kubectl apply -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : session-affinity-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-kfserving/request-count rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-kfserving/mean-latency upperLimit : 2000 - metric : iter8-kfserving/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-kfserving/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml The process automated by Iter8 during this experiment is depicted below.","title":"7. Launch experiment"},{"location":"tutorials/traffic-engineering/session-affinity/#8-observe-experiment","text":"Please follow Step 8 of the quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with session-affinity-exp in your commands. You can also observe traffic by suitably modifying the commands for observing traffic. Understanding what happened You created two versions of your app/ML model. You generated requests for your app/ML model versions. Throughout the experiment, users from Group 1 (i.e., users whose requests had a userhash header value prefixed with 111 ) were routed to the baseline version; and users from Group 2 (i.e., users whose requests had a userhash header value prefixed with 101 ) were routed to the candidate version. You created an Iter8 experiment with A/B testing pattern and FixedSplit deployment pattern. In each iteration, Iter8 observed the latency and error-rate metrics collected by Prometheus, and the user-engagement metric from New Relic; Iter8 verified that the candidate satisfied all objectives, verified that the candidate improved over the baseline in terms of user-engagement, identified candidate as the winner, and finally promoted the candidate.","title":"8. Observe experiment"},{"location":"tutorials/traffic-engineering/session-affinity/#9-cleanup","text":"KFServing kubectl delete -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml","title":"9. Cleanup"},{"location":"tutorials/traffic-engineering/user-segmentation/","text":"User Segmentation \u00b6 Scenario: SLO validation with user segmentation User segmentation is the ability to carve out a specific segment of users for an experiment, leaving the rest of the users unaffected by the experiment. In this tutorial, we will segment users into two groups: those from Wakanda, and others. Users from Wakanda will participate in the experiment: specifically, requests originating in Wakanda may be routed to baseline or candidate; requests that are originating from Wakanda will not participate in the experiment and are routed only to the baseline The experiment is depicted below. Before you begin... This tutorial is available for the following K8s stacks. Knative Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup. Steps 1 to 3 \u00b6 Please follow steps 1 through 3 of the quick start tutorial . 4. Create versions \u00b6 kubectl apply -f $ITER8 /samples/knative/user-segmentation/services.yaml Look inside services.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v1 namespace : default spec : template : metadata : name : sample-app-v1-blue annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v2 namespace : default spec : template : metadata : name : sample-app-v2-green annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" 5. Create routing rule \u00b6 kubectl apply -f $ITER8 /samples/knative/user-segmentation/routing-rule.yaml Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-for-wakanda spec : gateways : - mesh - knative-serving/knative-ingress-gateway - knative-serving/knative-local-gateway hosts : - example.com http : - match : - headers : country : exact : wakanda route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default weight : 100 - destination : host : sample-app-v2.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v2-green Host : sample-app-v2.default weight : 0 - route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default 6. Generate traffic \u00b6 TEMP_DIR = $( mktemp -d ) cd $TEMP_DIR curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .8.2 sh - istio-1.8.2/bin/istioctl kube-inject -f $ITER8 /samples/knative/user-segmentation/curl.yaml | kubectl create -f - cd $ITER8 Look inside curl.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : batch/v1 kind : Job metadata : name : curl spec : template : spec : activeDeadlineSeconds : 6000 containers : - name : curl-from-gondor image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS example.com -H \"country: gondor\" sleep 1.0 done - name : curl-from-wakanda image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS example.com -H \"country: wakanda\" sleep 0.25 done restartPolicy : Never 7. Create Iter8 experiment \u00b6 kubectl wait --for = condition = Ready ksvc/sample-app-v1 kubectl wait --for = condition = Ready ksvc/sample-app-v2 kubectl apply -f $ITER8 /samples/knative/user-segmentation/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : request-routing spec : # this experiment uses the fully-qualified name of the Istio virtual service as the target target : default/routing-for-wakanda strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1-blue weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : candidate variables : - name : revision value : sample-app-v2-green weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[1].weight 8. Observe experiment \u00b6 Please follow Step 8 of the quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with request-routing in your commands. You can also observe traffic by suitably modifying the commands for observing traffic. Understanding what happened You configured two Knative services corresponding to two versions of your app in services.yaml . You used example.com as the HTTP host in this tutorial. Note: In your production cluster, use domain(s) that you own in the setup of the virtual service. You set up an Istio virtual service which mapped the Knative services to this custom domain. The virtual service specified the following routing rules: all HTTP requests to example.com with their Host header or :authority pseudo-header not set to wakanda would be routed to the baseline ; those with wakanda Host header or :authority pseudo-header may be routed to baseline and candidate . The percentage of wakandan requests sent to candidate is 0% at the beginning of the experiment. You generated traffic for example.com using a curl -job with two curl -containers to simulate user requests. You injected Istio sidecar injected into it to simulate traffic generation from within the cluster. The sidecar was needed in order to correctly route traffic. One of the curl -containers sets the country header field to wakanda , and the other to gondor . Note: You used Istio version 1.8.2 to inject the sidecar. This version of Istio corresponds to the one installed in Step 3 of the quick start tutorial . If you have a different version of Istio installed in your cluster, change the Istio version during sidecar injection appropriately. You created an Iter8 Canary experiment with Progressive deployment pattern to evaluate the candidate . In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, and verified that the candidate version satisfied all the objectives specified in the experiment. It progressively increased the proportion of traffic with country: wakanda header that is routed to the candidate . 9. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/user-segmentation/experiment.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/curl.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/routing-rule.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/services.yaml","title":"User segmentation"},{"location":"tutorials/traffic-engineering/user-segmentation/#user-segmentation","text":"Scenario: SLO validation with user segmentation User segmentation is the ability to carve out a specific segment of users for an experiment, leaving the rest of the users unaffected by the experiment. In this tutorial, we will segment users into two groups: those from Wakanda, and others. Users from Wakanda will participate in the experiment: specifically, requests originating in Wakanda may be routed to baseline or candidate; requests that are originating from Wakanda will not participate in the experiment and are routed only to the baseline The experiment is depicted below. Before you begin... This tutorial is available for the following K8s stacks. Knative Please choose the same K8s stack consistently throughout this tutorial. If you wish to switch K8s stacks between tutorials, start from a clean K8s cluster, so that your cluster is correctly setup.","title":"User Segmentation"},{"location":"tutorials/traffic-engineering/user-segmentation/#steps-1-to-3","text":"Please follow steps 1 through 3 of the quick start tutorial .","title":"Steps 1 to 3"},{"location":"tutorials/traffic-engineering/user-segmentation/#4-create-versions","text":"kubectl apply -f $ITER8 /samples/knative/user-segmentation/services.yaml Look inside services.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v1 namespace : default spec : template : metadata : name : sample-app-v1-blue annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v2 namespace : default spec : template : metadata : name : sample-app-v2-green annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\"","title":"4. Create versions"},{"location":"tutorials/traffic-engineering/user-segmentation/#5-create-routing-rule","text":"kubectl apply -f $ITER8 /samples/knative/user-segmentation/routing-rule.yaml Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-for-wakanda spec : gateways : - mesh - knative-serving/knative-ingress-gateway - knative-serving/knative-local-gateway hosts : - example.com http : - match : - headers : country : exact : wakanda route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default weight : 100 - destination : host : sample-app-v2.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v2-green Host : sample-app-v2.default weight : 0 - route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default","title":"5. Create routing rule"},{"location":"tutorials/traffic-engineering/user-segmentation/#6-generate-traffic","text":"TEMP_DIR = $( mktemp -d ) cd $TEMP_DIR curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .8.2 sh - istio-1.8.2/bin/istioctl kube-inject -f $ITER8 /samples/knative/user-segmentation/curl.yaml | kubectl create -f - cd $ITER8 Look inside curl.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : batch/v1 kind : Job metadata : name : curl spec : template : spec : activeDeadlineSeconds : 6000 containers : - name : curl-from-gondor image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS example.com -H \"country: gondor\" sleep 1.0 done - name : curl-from-wakanda image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS example.com -H \"country: wakanda\" sleep 0.25 done restartPolicy : Never","title":"6. Generate traffic"},{"location":"tutorials/traffic-engineering/user-segmentation/#7-create-iter8-experiment","text":"kubectl wait --for = condition = Ready ksvc/sample-app-v1 kubectl wait --for = condition = Ready ksvc/sample-app-v2 kubectl apply -f $ITER8 /samples/knative/user-segmentation/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : request-routing spec : # this experiment uses the fully-qualified name of the Istio virtual service as the target target : default/routing-for-wakanda strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1-blue weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : candidate variables : - name : revision value : sample-app-v2-green weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[1].weight","title":"7. Create Iter8 experiment"},{"location":"tutorials/traffic-engineering/user-segmentation/#8-observe-experiment","text":"Please follow Step 8 of the quick start tutorial to observe the experiment in realtime. Note that the experiment in this tutorial uses a different name from the quick start one. Replace the experiment name quickstart-exp with request-routing in your commands. You can also observe traffic by suitably modifying the commands for observing traffic. Understanding what happened You configured two Knative services corresponding to two versions of your app in services.yaml . You used example.com as the HTTP host in this tutorial. Note: In your production cluster, use domain(s) that you own in the setup of the virtual service. You set up an Istio virtual service which mapped the Knative services to this custom domain. The virtual service specified the following routing rules: all HTTP requests to example.com with their Host header or :authority pseudo-header not set to wakanda would be routed to the baseline ; those with wakanda Host header or :authority pseudo-header may be routed to baseline and candidate . The percentage of wakandan requests sent to candidate is 0% at the beginning of the experiment. You generated traffic for example.com using a curl -job with two curl -containers to simulate user requests. You injected Istio sidecar injected into it to simulate traffic generation from within the cluster. The sidecar was needed in order to correctly route traffic. One of the curl -containers sets the country header field to wakanda , and the other to gondor . Note: You used Istio version 1.8.2 to inject the sidecar. This version of Istio corresponds to the one installed in Step 3 of the quick start tutorial . If you have a different version of Istio installed in your cluster, change the Istio version during sidecar injection appropriately. You created an Iter8 Canary experiment with Progressive deployment pattern to evaluate the candidate . In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, and verified that the candidate version satisfied all the objectives specified in the experiment. It progressively increased the proportion of traffic with country: wakanda header that is routed to the candidate .","title":"8. Observe experiment"},{"location":"tutorials/traffic-engineering/user-segmentation/#9-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/user-segmentation/experiment.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/curl.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/routing-rule.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/services.yaml","title":"9. Cleanup"}]}