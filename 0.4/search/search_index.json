{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Iter8 \u00b6","title":"Home"},{"location":"#iter8","text":"","title":"Iter8"},{"location":"contributing/","text":"Contributing \u00b6 We are delighted that you are considering a contribution to Iter8! Please discuss the change you wish to make using issues , discussions , or the Iter8 slack workspace before submitting your PR. Locally serving Iter8 docs \u00b6 Pre-requisite: Python 3+. Use a Python 3 virtual environment to locally serve Iter8 docs as follows. python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s Browse http://localhost:8000 to view your local Iter8 docs. Locally viewing live changes to Iter8 docs \u00b6 The overall structure of the documentation, as reflected in the nav tabs of https://iter8.tools , is located in the iter8/mkdocs/mkdocs.yml file. The markdown files for Iter8 docs are located under the iter8/mkdocs/docs folder. You will see live updates to http://localhost:8000 as you update markdown files in the iter8/mkdocs folder. Contributing an Iter8 tutorial \u00b6 All iter8 tutorials include e2e tests, either as part of GitHub Actions workflows or as a standalone test script like this one if they require more resources than what is available in GitHub Actions workflows. When contributing a tutorial, please include relevant e2e tests. Extending Iter8 in other ways \u00b6 Documentation for contributing other Iter8 extensions such as new handler tasks, analytics capabilities, and observability features is coming soon.","title":"Contributing"},{"location":"contributing/#contributing","text":"We are delighted that you are considering a contribution to Iter8! Please discuss the change you wish to make using issues , discussions , or the Iter8 slack workspace before submitting your PR.","title":"Contributing"},{"location":"contributing/#locally-serving-iter8-docs","text":"Pre-requisite: Python 3+. Use a Python 3 virtual environment to locally serve Iter8 docs as follows. python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s Browse http://localhost:8000 to view your local Iter8 docs.","title":"Locally serving Iter8 docs"},{"location":"contributing/#locally-viewing-live-changes-to-iter8-docs","text":"The overall structure of the documentation, as reflected in the nav tabs of https://iter8.tools , is located in the iter8/mkdocs/mkdocs.yml file. The markdown files for Iter8 docs are located under the iter8/mkdocs/docs folder. You will see live updates to http://localhost:8000 as you update markdown files in the iter8/mkdocs folder.","title":"Locally viewing live changes to Iter8 docs"},{"location":"contributing/#contributing-an-iter8-tutorial","text":"All iter8 tutorials include e2e tests, either as part of GitHub Actions workflows or as a standalone test script like this one if they require more resources than what is available in GitHub Actions workflows. When contributing a tutorial, please include relevant e2e tests.","title":"Contributing an Iter8 tutorial"},{"location":"contributing/#extending-iter8-in-other-ways","text":"Documentation for contributing other Iter8 extensions such as new handler tasks, analytics capabilities, and observability features is coming soon.","title":"Extending Iter8 in other ways"},{"location":"roadmap/","text":"Roadmap \u00b6 Enhanced experiments A/B, A/B/n, and Pareto testing patterns with single and multiple reward metrics Blue/green deployment pattern Experiments with support and confidence Metrics Support for more metric providers like MySQL, PostgreSQL, CouchDB, MongoDB, Google Analytics and Fortio. Enhanced MLOps experiments Customized experiments/metrics for serving frameworks like TorchServe and TFServing GitOps Integration with ArgoCD, Flux and other GitOps operators Notifications Integration with Slack, GitHub, and other RESTful services Enhancing Kubernetes and OpenShift integration Improved support for KFServing Enhanced support for Istio using new Iter8 Experiment API Support for OpenShift Serverless Enhanced Knative metrics in tutorials using OpenTelemetry collector Support for Ambassador and Kong networking layers in KNative Support for experimenting with configuration and routes in Knative Git triggered workflows and CI/CD Integration with GitHub Actions and other pipeline providers Helm tests Improved installation Multi-stack installation, Iter8 Helm chart, Iter8 Operator","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Enhanced experiments A/B, A/B/n, and Pareto testing patterns with single and multiple reward metrics Blue/green deployment pattern Experiments with support and confidence Metrics Support for more metric providers like MySQL, PostgreSQL, CouchDB, MongoDB, Google Analytics and Fortio. Enhanced MLOps experiments Customized experiments/metrics for serving frameworks like TorchServe and TFServing GitOps Integration with ArgoCD, Flux and other GitOps operators Notifications Integration with Slack, GitHub, and other RESTful services Enhancing Kubernetes and OpenShift integration Improved support for KFServing Enhanced support for Istio using new Iter8 Experiment API Support for OpenShift Serverless Enhanced Knative metrics in tutorials using OpenTelemetry collector Support for Ambassador and Kong networking layers in KNative Support for experimenting with configuration and routes in Knative Git triggered workflows and CI/CD Integration with GitHub Actions and other pipeline providers Helm tests Improved installation Multi-stack installation, Iter8 Helm chart, Iter8 Operator","title":"Roadmap"},{"location":"concepts/buildingblocks/","text":"Conceptual Building Blocks \u00b6 We introduce the conceptual building blocks of an Iter8 experiment. The concepts defined in this document are used throughout Iter8 documentation. Version \u00b6 Version is a variant of your app/ML model. Validation \u00b6 A version is considered validated if it satisfies the given set of service-level objectives or SLOs. An example of an SLO could be as follows: the 99 th -percentile tail latency of the version should be under 50 msec. Release \u00b6 Release is the process by which a new version becomes responsible for serving production traffic. Experiment \u00b6 Iter8 defines a new Kubernetes resource called experiment that automates releases, validation, and experiments with your app/ML model versions based on metrics. Iter8 experiments may be run in a production cluster or dev/test/staging clusters. Winner \u00b6 The winner is the best version among all the versions involved in an experiment. Baseline \u00b6 Every Iter8 experiment involves a version called the baseline . Typically, baseline is the latest stable version of your app/ML model that has been released. Candidate \u00b6 Candidate is a version, other than the baseline, that participates in an experiment. Version recommended for promotion \u00b6 When two or more versions participate in an experiment, Iter8 recommends a version for promotion ; if the experiment yielded a winner, then the version recommended for promotion is the winner; otherwise, the version recommended for promotion is the baseline. Metrics \u00b6 Metric providers like Prometheus , New Relic , Sysdig and Elastic can collect metrics for versions and serve them through REST APIs. Iter8 defines a new Kubernetes resource called Metric that makes it easy to use metrics in experiments from these and any other RESTful metrics provider. Objectives \u00b6 Objectives correspond to service-level objectives or SLOs. In Iter8 experiments, objectives are specified as metrics along with acceptable limits on their values. Iter8 will report how versions are performing with respect to these metrics and whether or not they satisfy the objectives. Indicators \u00b6 Indicators correspond to service-level indicators or SLIs. In Iter8 experiments, indicators are specified as a list metrics. Iter8 will report how versions are performing with respect to these metrics. Testing pattern \u00b6 Testing pattern defines the number of versions involved in the experiment (1, 2, or more), and determines how the winner is identified. Iter8 supports canary and conformance testing patterns. Canary Canary testing involves two versions, the baseline and a candidate. If the candidate is validated (i.e., it satisfies objectives specified in the experiment), then candidate is the winner; else, if baseline satisfies objectives, then baseline is the winner; else, there is no winner. Try a canary experiment . Conformance Conformance testing involves a single version, a baseline. If it is validated (i.e., it satisfies objectives) then baseline is the winner; else, there is no winner. Try a conformance experiment . Deployment pattern \u00b6 Deployment pattern determines how traffic is split between versions. Iter8 supports progressive and fixed-split deployment patterns. Progressive Progressive deployment incrementally shifts traffic towards the winner over multiple iterations. Try a progressive deployment experiment . Fixed-split Fixed-split deployment does not shift traffic between versions. Try a fixed-split deployment experiment . Traffic shaping \u00b6 Traffic shaping refers to features such as traffic mirroring/shadowing and traffic segmentation that provide fine-grained controls over how traffic is routed to and from app versions. Iter8 enables you to take total advantage of all the traffic shaping features available in the service mesh, ingress technology, or networking layer present in your Kubernetes cluster. Traffic mirroring/shadowing Traffic mirroring or shadowing enables experimenting with a dark launched version with zero-impact on end-users. Mirrored traffic is a replica of the real user requests 1 that is routed to the dark version. Metrics are collected and evaluated for the dark version, but responses from the dark version are ignored. Try an experiment with traffic mirroring/shadowing . Traffic segmentation Traffic segmentation is the ability to carve out a specific segment of the traffic to be used in an experiment, leaving the rest of the traffic unaffected by the experiment. Service meshes and ingress controllers often provide the ability to route requests dynamically to different versions based on request attributes such as user identity, URI, IP address prefixes, or origin. Iter8 can leverage this functionality in experiments to control the segment of the traffic that will participate in the experiment. For example, in the canary experiment depicted below, requests from the country Wakanda may be routed to baseline or candidate; requests that are not from Wakanda will not participate in the experiment and are routed only to the baseline. Try an experiment with traffic segmentation . Version promotion \u00b6 Iter8 can optionally promote a version at the end of an experiment, based on the version recommended for promotion . As part of the version promotion task, Iter8 can configure Kubernetes resources by installing or upgrading Helm charts, building and applying Kustomize resources, or using the kubectl CLI to apply YAML/JSON resource manifests and perform other cleanup actions such as resource deletion. Helm charts An experiment that uses helm for version promotion is illustrated below. Try an experiment that uses Helm charts . Kustomize resources An experiment that uses kustomize for version promotion is illustrated below. Try an experiment that uses Kustomize resources . Plain YAML/JSON manifests An experiment that uses plain YAML/JSON manifests and the kubectl CLI for version promotion is illustrated below. Try an experiment that uses plain YAML/JSON manifests . It is possible to mirror only a certain percentage of the requests instead of all requests. \u21a9","title":"Conceptual building blocks"},{"location":"concepts/buildingblocks/#conceptual-building-blocks","text":"We introduce the conceptual building blocks of an Iter8 experiment. The concepts defined in this document are used throughout Iter8 documentation.","title":"Conceptual Building Blocks"},{"location":"concepts/buildingblocks/#version","text":"Version is a variant of your app/ML model.","title":"Version"},{"location":"concepts/buildingblocks/#validation","text":"A version is considered validated if it satisfies the given set of service-level objectives or SLOs. An example of an SLO could be as follows: the 99 th -percentile tail latency of the version should be under 50 msec.","title":"Validation"},{"location":"concepts/buildingblocks/#release","text":"Release is the process by which a new version becomes responsible for serving production traffic.","title":"Release"},{"location":"concepts/buildingblocks/#experiment","text":"Iter8 defines a new Kubernetes resource called experiment that automates releases, validation, and experiments with your app/ML model versions based on metrics. Iter8 experiments may be run in a production cluster or dev/test/staging clusters.","title":"Experiment"},{"location":"concepts/buildingblocks/#winner","text":"The winner is the best version among all the versions involved in an experiment.","title":"Winner"},{"location":"concepts/buildingblocks/#baseline","text":"Every Iter8 experiment involves a version called the baseline . Typically, baseline is the latest stable version of your app/ML model that has been released.","title":"Baseline"},{"location":"concepts/buildingblocks/#candidate","text":"Candidate is a version, other than the baseline, that participates in an experiment.","title":"Candidate"},{"location":"concepts/buildingblocks/#version-recommended-for-promotion","text":"When two or more versions participate in an experiment, Iter8 recommends a version for promotion ; if the experiment yielded a winner, then the version recommended for promotion is the winner; otherwise, the version recommended for promotion is the baseline.","title":"Version recommended for promotion"},{"location":"concepts/buildingblocks/#metrics","text":"Metric providers like Prometheus , New Relic , Sysdig and Elastic can collect metrics for versions and serve them through REST APIs. Iter8 defines a new Kubernetes resource called Metric that makes it easy to use metrics in experiments from these and any other RESTful metrics provider.","title":"Metrics"},{"location":"concepts/buildingblocks/#objectives","text":"Objectives correspond to service-level objectives or SLOs. In Iter8 experiments, objectives are specified as metrics along with acceptable limits on their values. Iter8 will report how versions are performing with respect to these metrics and whether or not they satisfy the objectives.","title":"Objectives"},{"location":"concepts/buildingblocks/#indicators","text":"Indicators correspond to service-level indicators or SLIs. In Iter8 experiments, indicators are specified as a list metrics. Iter8 will report how versions are performing with respect to these metrics.","title":"Indicators"},{"location":"concepts/buildingblocks/#testing-pattern","text":"Testing pattern defines the number of versions involved in the experiment (1, 2, or more), and determines how the winner is identified. Iter8 supports canary and conformance testing patterns. Canary Canary testing involves two versions, the baseline and a candidate. If the candidate is validated (i.e., it satisfies objectives specified in the experiment), then candidate is the winner; else, if baseline satisfies objectives, then baseline is the winner; else, there is no winner. Try a canary experiment . Conformance Conformance testing involves a single version, a baseline. If it is validated (i.e., it satisfies objectives) then baseline is the winner; else, there is no winner. Try a conformance experiment .","title":"Testing pattern"},{"location":"concepts/buildingblocks/#deployment-pattern","text":"Deployment pattern determines how traffic is split between versions. Iter8 supports progressive and fixed-split deployment patterns. Progressive Progressive deployment incrementally shifts traffic towards the winner over multiple iterations. Try a progressive deployment experiment . Fixed-split Fixed-split deployment does not shift traffic between versions. Try a fixed-split deployment experiment .","title":"Deployment pattern"},{"location":"concepts/buildingblocks/#traffic-shaping","text":"Traffic shaping refers to features such as traffic mirroring/shadowing and traffic segmentation that provide fine-grained controls over how traffic is routed to and from app versions. Iter8 enables you to take total advantage of all the traffic shaping features available in the service mesh, ingress technology, or networking layer present in your Kubernetes cluster. Traffic mirroring/shadowing Traffic mirroring or shadowing enables experimenting with a dark launched version with zero-impact on end-users. Mirrored traffic is a replica of the real user requests 1 that is routed to the dark version. Metrics are collected and evaluated for the dark version, but responses from the dark version are ignored. Try an experiment with traffic mirroring/shadowing . Traffic segmentation Traffic segmentation is the ability to carve out a specific segment of the traffic to be used in an experiment, leaving the rest of the traffic unaffected by the experiment. Service meshes and ingress controllers often provide the ability to route requests dynamically to different versions based on request attributes such as user identity, URI, IP address prefixes, or origin. Iter8 can leverage this functionality in experiments to control the segment of the traffic that will participate in the experiment. For example, in the canary experiment depicted below, requests from the country Wakanda may be routed to baseline or candidate; requests that are not from Wakanda will not participate in the experiment and are routed only to the baseline. Try an experiment with traffic segmentation .","title":"Traffic shaping"},{"location":"concepts/buildingblocks/#version-promotion","text":"Iter8 can optionally promote a version at the end of an experiment, based on the version recommended for promotion . As part of the version promotion task, Iter8 can configure Kubernetes resources by installing or upgrading Helm charts, building and applying Kustomize resources, or using the kubectl CLI to apply YAML/JSON resource manifests and perform other cleanup actions such as resource deletion. Helm charts An experiment that uses helm for version promotion is illustrated below. Try an experiment that uses Helm charts . Kustomize resources An experiment that uses kustomize for version promotion is illustrated below. Try an experiment that uses Kustomize resources . Plain YAML/JSON manifests An experiment that uses plain YAML/JSON manifests and the kubectl CLI for version promotion is illustrated below. Try an experiment that uses plain YAML/JSON manifests . It is possible to mirror only a certain percentage of the requests instead of all requests. \u21a9","title":"Version promotion"},{"location":"concepts/features/","text":"Iter8 Features at a Glance \u00b6 Iter8 makes it easy to achieve the following goals. Automate releases, validation, and experiments over any cloud stack; tutorials available for Knative , KFServing 1 and Istio 2 . Declaratively specify testing and deployment patterns, and metrics-based objectives (SLOs) and indicators (SLIs) for evaluating app/ML model versions using an experiment - the Kubernetes custom resource defined by Iter8. Conformance and Canary testing. Progressive traffic shifting. Dark launches , traffic mirroring and traffic segmentation . Use Helm, Kustomize, and plain YAML/JSON for defining your app manifests. Use metrics from any RESTful provider including Prometheus , New Relic , Sysdig , and Elastic . Statistically rigorous evaluation of versions, traffic splitting, and promotion/rollback decisions using Bayesian learning and multi-armed bandit algorithms. Observe experiments in realtime. An initial version of Iter8 for KFServing is available here . An updated version is coming soon. \u21a9 An earlier version of Iter8 for Istio is available here . An updated version is coming soon. \u21a9","title":"Features"},{"location":"concepts/features/#iter8-features-at-a-glance","text":"Iter8 makes it easy to achieve the following goals. Automate releases, validation, and experiments over any cloud stack; tutorials available for Knative , KFServing 1 and Istio 2 . Declaratively specify testing and deployment patterns, and metrics-based objectives (SLOs) and indicators (SLIs) for evaluating app/ML model versions using an experiment - the Kubernetes custom resource defined by Iter8. Conformance and Canary testing. Progressive traffic shifting. Dark launches , traffic mirroring and traffic segmentation . Use Helm, Kustomize, and plain YAML/JSON for defining your app manifests. Use metrics from any RESTful provider including Prometheus , New Relic , Sysdig , and Elastic . Statistically rigorous evaluation of versions, traffic splitting, and promotion/rollback decisions using Bayesian learning and multi-armed bandit algorithms. Observe experiments in realtime. An initial version of Iter8 for KFServing is available here . An updated version is coming soon. \u21a9 An earlier version of Iter8 for Istio is available here . An updated version is coming soon. \u21a9","title":"Iter8 Features at a Glance"},{"location":"concepts/whatisiter8/","text":"What is Iter8? \u00b6 Iter8 is an AI-powered platform for cloud native release automation, validation, and experimentation. Iter8 makes it easy to unlock business value and guarantee SLOs by identifying the best performing app/ML model version and rolling it out safely. Iter8 is designed for developers, SREs, service operators, data scientists, and ML engineers who wish to maximize release velocity and business value with their apps/ML models while protecting end-user experience. What is an Iter8 experiment? \u00b6 Iter8 defines a Kubernetes resource called Experiment that automates the process of validating service-level objectives (SLOs), identifying the best version of your app/ML model version based on business and performance metrics, progressive traffic shifting, and promotion/rollback. 1 How does Iter8 work? \u00b6 Iter8 consists of a Go-based Kubernetes controller that orchestrates (reconciles) experiments in conjunction with a Python-based analytics service , and a Go-based task runner . Boxes with dashed boundaries in the picture are optional in an experiment. \u21a9","title":"What is Iter8?"},{"location":"concepts/whatisiter8/#what-is-iter8","text":"Iter8 is an AI-powered platform for cloud native release automation, validation, and experimentation. Iter8 makes it easy to unlock business value and guarantee SLOs by identifying the best performing app/ML model version and rolling it out safely. Iter8 is designed for developers, SREs, service operators, data scientists, and ML engineers who wish to maximize release velocity and business value with their apps/ML models while protecting end-user experience.","title":"What is Iter8?"},{"location":"concepts/whatisiter8/#what-is-an-iter8-experiment","text":"Iter8 defines a Kubernetes resource called Experiment that automates the process of validating service-level objectives (SLOs), identifying the best version of your app/ML model version based on business and performance metrics, progressive traffic shifting, and promotion/rollback. 1","title":"What is an Iter8 experiment?"},{"location":"concepts/whatisiter8/#how-does-iter8-work","text":"Iter8 consists of a Go-based Kubernetes controller that orchestrates (reconciles) experiments in conjunction with a Python-based analytics service , and a Go-based task runner . Boxes with dashed boundaries in the picture are optional in an experiment. \u21a9","title":"How does Iter8 work?"},{"location":"getting-started/changelog/","text":"Changelog \u00b6 0.4.3 (April 26 th , 2021) Documented Iter8's default RBAC profile 0.4.0 (April 23 rd , 2021) Enabled metrics from any provider to be used in Iter8 experiments Reduced Iter8's RBAC profile by moving Helm related RBAC rules to tutorials Improved Material for MkDocs integration so that npm is not required for most documentation contributions 0.3.4 (April 12 th , 2021) Introduced changelog Introduced versioning for Iter8 documentation Fixed slack unfurling issues","title":"Changelog"},{"location":"getting-started/changelog/#changelog","text":"0.4.3 (April 26 th , 2021) Documented Iter8's default RBAC profile 0.4.0 (April 23 rd , 2021) Enabled metrics from any provider to be used in Iter8 experiments Reduced Iter8's RBAC profile by moving Helm related RBAC rules to tutorials Improved Material for MkDocs integration so that npm is not required for most documentation contributions 0.3.4 (April 12 th , 2021) Introduced changelog Introduced versioning for Iter8 documentation Fixed slack unfurling issues","title":"Changelog"},{"location":"getting-started/help/","text":"Getting Help \u00b6 Read Iter8 docs . Invite yourself to the Iter8 slack workspace . File an issue or start a discussion on Iter8 GitHub repo .","title":"Getting help"},{"location":"getting-started/help/#getting-help","text":"Read Iter8 docs . Invite yourself to the Iter8 slack workspace . File an issue or start a discussion on Iter8 GitHub repo .","title":"Getting Help"},{"location":"getting-started/install/","text":"Installation \u00b6 Install Iter8 in your Kubernetes cluster using Kustomize v3+ as follows. export TAG = v0.4.5 kustomize build github.com/iter8-tools/iter8-install/core?ref = $TAG | kubectl apply -f - The above command installs Iter8's controller and analytics services in the iter8-system namespace, the Experiment and Metric CRDs, and the following RBAC permissions. Default RBAC Permissions Resource Permissions Scope experiments.iter8.tools get, list, patch, update, watch Cluster-wide experiments.iter8.tools/status get, patch, update Cluster-wide metrics.iter8.tools get, list Cluster-wide jobs.batch create, delete, get, list, watch Cluster-wide leases.coordination.k8s.io get, list, watch, create, update, patch, delete iter8-system namespace events create iter8-system namespace services.serving.knative.dev get, list, patch, update Cluster-wide virtualservices.networking.istio.io get, list, patch, update, create, delete Cluster-wide destinationrules.networking.istio.io get, list, patch, update, create, delete Cluster-wide","title":"Install"},{"location":"getting-started/install/#installation","text":"Install Iter8 in your Kubernetes cluster using Kustomize v3+ as follows. export TAG = v0.4.5 kustomize build github.com/iter8-tools/iter8-install/core?ref = $TAG | kubectl apply -f - The above command installs Iter8's controller and analytics services in the iter8-system namespace, the Experiment and Metric CRDs, and the following RBAC permissions. Default RBAC Permissions Resource Permissions Scope experiments.iter8.tools get, list, patch, update, watch Cluster-wide experiments.iter8.tools/status get, patch, update Cluster-wide metrics.iter8.tools get, list Cluster-wide jobs.batch create, delete, get, list, watch Cluster-wide leases.coordination.k8s.io get, list, watch, create, update, patch, delete iter8-system namespace events create iter8-system namespace services.serving.knative.dev get, list, patch, update Cluster-wide virtualservices.networking.istio.io get, list, patch, update, create, delete Cluster-wide destinationrules.networking.istio.io get, list, patch, update, create, delete Cluster-wide","title":"Installation"},{"location":"getting-started/quick-start/with-istio/","text":"Quick start with Istio \u00b6 An earlier version of Iter8 for Istio is available here . An updated version is coming soon and will be documented here.","title":"with Istio"},{"location":"getting-started/quick-start/with-istio/#quick-start-with-istio","text":"An earlier version of Iter8 for Istio is available here . An updated version is coming soon and will be documented here.","title":"Quick start with Istio"},{"location":"getting-started/quick-start/with-kfserving/","text":"Quick start with KFServing \u00b6 An initial version of Iter8 for KFServing is available here . An updated version is coming soon and will be documented in this site.","title":"with KFServing"},{"location":"getting-started/quick-start/with-kfserving/#quick-start-with-kfserving","text":"An initial version of Iter8 for KFServing is available here . An updated version is coming soon and will be documented in this site.","title":"Quick start with KFServing"},{"location":"getting-started/quick-start/with-knative/","text":"Quick Start with Knative \u00b6 Scenario: Canary testing and progressive deployment Canary testing enables you to reduce risk during a release by validating your new version with a small fraction of users before exposing it to all users. In this tutorial, you will: Perform canary testing. Specify service-level objectives or SLOs used by Iter8 to automatically validate your versions. Use metrics from Prometheus. Combine canary testing with progressive deployment in an Iter8 experiment. Assuming the new version is validated, Iter8 will progressively increase the traffic percentage for the new version and promote it at the end as depicted below. Before you begin, you will need... Kubernetes cluster. You can also use Minikube or Kind . The kubectl CLI. Install kubectl here . Go 1.13+ (recommended; required for using iter8ctl in Step 8 ). Install Go here . 1. Create Kubernetes cluster \u00b6 Create a local cluster using Minikube or Kind as follows, or use a managed Kubernetes service. Ensure that the cluster has sufficient resources, for example, 6 CPUs and 12GB of memory. Minikube minikube start --cpus 6 --memory 12288 Kind kind create cluster kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. 2. Clone Iter8 repo \u00b6 git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) 3. Install Knative, Iter8, and Prometheus add-on \u00b6 Knative can work with multiple networking layers. So can Iter8's Knative extension. Choose a networking layer for Knative. Contour $ITER8 /samples/knative/quickstart/platformsetup.sh contour Kourier $ITER8 /samples/knative/quickstart/platformsetup.sh kourier Gloo This step requires Python. This will install glooctl binary under $HOME/.gloo folder. $ITER8 /samples/knative/quickstart/platformsetup.sh gloo Istio $ITER8 /samples/knative/quickstart/platformsetup.sh istio 4. Create app versions \u00b6 Create baseline and candidate versions of your app, sample-app-v1 and sample-app-v2 respectively. The candidate version is also referred to as the new or canary version. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app # name of the app namespace : default # namespace of the app spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : # initially all traffic goes to sample-app-v1 and none to sample-app-v2 - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0 5. Generate requests \u00b6 In a production environment, your application would receive requests from end-users. For the purposes of this tutorial, simulate user requests using Fortio as follows. kubectl wait --for = condition = Ready ksvc/sample-app # URL_VALUE is the URL where your Knative application serves requests URL_VALUE = $( kubectl get ksvc sample-app -o json | jq .status.address.url ) sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/knative/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-t\" , \"6000s\" , \"-qps\" , \"16\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 6. Define metrics \u00b6 Define the Iter8 metrics used in this experiment. kubectl apply -f $ITER8 /samples/knative/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : 95th-percentile-tail-latency namespace : iter8-knative spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(revision_app_request_latencies_bucket{revision_name='$revision'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count namespace : iter8-knative spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',revision_name='$revision'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate namespace : iter8-knative spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',revision_name='$revision'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency namespace : iter8-knative spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_sum{revision_name='$revision'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : iter8-knative spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query The urlTemplate field in these metrics point to the Prometheus instance that was created in Step 3 above. If you wish to use these metrics in your production/staging/dev/test K8s cluster, change the urlTemplate values to match the URL of your Prometheus instance. 7. Launch experiment \u00b6 Launch the Iter8 experiment. Iter8 will orchestrate the canary release of the new version with SLO validation and progressive deployment as specified in the experiment. kubectl apply -f $ITER8 /samples/knative/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive actions : start : # run the following sequence of tasks at the start of the experiment - task : knative/init-experiment finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : kubectl args : - \"apply\" - \"-f\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" criteria : requestCount : iter8-knative/request-count # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1 - name : promote value : baseline candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candidate The process automated by Iter8 during this experiment is depicted below. 8. Observe experiment \u00b6 Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis Install iter8ctl . You can change the directory where iter8ctl binary is installed by changing GOBIN below. GO111MODULE = on GOBIN = /usr/local/bin go get github.com/iter8-tools/iter8ctl@v0.1.3 Periodically describe the experiment. while clear ; do kubectl get experiment quickstart-exp -o yaml | iter8ctl describe -f - sleep 4 done Look inside iter8ctl output The iter8ctl output will be similar to the following. ****** Overview ****** Experiment name: quickstart-exp Experiment namespace: default Target: default/sample-app Testing pattern: Canary Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Running Number of completed iterations: 3 ****** Winner Assessment ****** > If the candidate version satisfies the experiment objectives, then it is the winner. > Otherwise, if the baseline version satisfies the experiment objectives, it is the winner. > Otherwise, there is no winner. App versions in this experiment: [ current candidate ] Winning version: candidate Version recommended for promotion: candidate ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------------+---------+-----------+ | OBJECTIVE | CURRENT | CANDIDATE | +--------------------------------------------+---------+-----------+ | iter8-knative/mean-latency < = | true | true | | 50 .000 | | | +--------------------------------------------+---------+-----------+ | iter8-knative/95th-percentile-tail-latency | true | true | | < = 100 .000 | | | +--------------------------------------------+---------+-----------+ | iter8-knative/error-rate < = | true | true | | 0 .010 | | | +--------------------------------------------+---------+-----------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------------+---------+-----------+ | METRIC | CURRENT | CANDIDATE | +--------------------------------------------+---------+-----------+ | iter8-knative/request-count | 454 .523 | 27 .412 | +--------------------------------------------+---------+-----------+ | iter8-knative/mean-latency | 1 .265 | 1 .415 | | ( milliseconds ) | | | +--------------------------------------------+---------+-----------+ | request-count | 454 .523 | 27 .619 | +--------------------------------------------+---------+-----------+ | iter8-knative/95th-percentile-tail-latency | 4 .798 | 4 .928 | | ( milliseconds ) | | | +--------------------------------------------+---------+-----------+ | iter8-knative/error-rate | 0 .000 | 0 .000 | +--------------------------------------------+---------+-----------+ As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by both versions. The candidate is identified as the winner and is recommended for promotion. When the experiment completes (in ~2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment quickstart-exp --watch kubectl get experiment output The kubectl output will be similar to the following. NAME TYPE TARGET STAGE COMPLETED ITERATIONS MESSAGE quickstart-exp Canary default/sample-app Running 1 IterationUpdate: Completed Iteration 1 quickstart-exp Canary default/sample-app Running 2 IterationUpdate: Completed Iteration 2 quickstart-exp Canary default/sample-app Running 3 IterationUpdate: Completed Iteration 3 quickstart-exp Canary default/sample-app Running 4 IterationUpdate: Completed Iteration 4 quickstart-exp Canary default/sample-app Running 5 IterationUpdate: Completed Iteration 5 quickstart-exp Canary default/sample-app Running 6 IterationUpdate: Completed Iteration 6 quickstart-exp Canary default/sample-app Running 7 IterationUpdate: Completed Iteration 7 quickstart-exp Canary default/sample-app Running 8 IterationUpdate: Completed Iteration 8 quickstart-exp Canary default/sample-app Running 9 IterationUpdate: Completed Iteration 9 When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Traffic split kubectl get ksvc sample-app -o json --watch | jq .status.traffic kubectl get ksvc output The kubectl output will be similar to the following. [ { \"latestRevision\" : false, \"percent\" : 45 , \"revisionName\" : \"sample-app-v1\" , \"tag\" : \"current\" , \"url\" : \"http://current-sample-app.default.example.com\" } , { \"latestRevision\" : true, \"percent\" : 55 , \"revisionName\" : \"sample-app-v2\" , \"tag\" : \"candidate\" , \"url\" : \"http://candidate-sample-app.default.example.com\" } ] As the experiment progresses, you should see traffic progressively shift from sample-app-v1 to sample-app-v2 . When the experiment completes, all of the traffic will be sent to the winner, sample-app-v2 . Understanding what happened You created a Knative service with two revisions, sample-app-v1 (baseline) and sample-app-v2 (candidate). You generated requests for the Knative service using a Fortio job. At the start of the experiment, 100% of the requests are sent to the baseline and 0% to the candidate. You created an Iter8 experiment with canary testing and progressive deployment patterns. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, verified that the candidate satisfied all objectives, identified the candidate as the winner, progressively shifted traffic from the baseline to the candidate, and eventually promoted the candidate using the kubectl apply command embedded within its finish action. Had the candidate failed to satisfy objectives, then the baseline would have been promoted. 9. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"with Knative"},{"location":"getting-started/quick-start/with-knative/#quick-start-with-knative","text":"Scenario: Canary testing and progressive deployment Canary testing enables you to reduce risk during a release by validating your new version with a small fraction of users before exposing it to all users. In this tutorial, you will: Perform canary testing. Specify service-level objectives or SLOs used by Iter8 to automatically validate your versions. Use metrics from Prometheus. Combine canary testing with progressive deployment in an Iter8 experiment. Assuming the new version is validated, Iter8 will progressively increase the traffic percentage for the new version and promote it at the end as depicted below. Before you begin, you will need... Kubernetes cluster. You can also use Minikube or Kind . The kubectl CLI. Install kubectl here . Go 1.13+ (recommended; required for using iter8ctl in Step 8 ). Install Go here .","title":"Quick Start with Knative"},{"location":"getting-started/quick-start/with-knative/#1-create-kubernetes-cluster","text":"Create a local cluster using Minikube or Kind as follows, or use a managed Kubernetes service. Ensure that the cluster has sufficient resources, for example, 6 CPUs and 12GB of memory. Minikube minikube start --cpus 6 --memory 12288 Kind kind create cluster kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below.","title":"1. Create Kubernetes cluster"},{"location":"getting-started/quick-start/with-knative/#2-clone-iter8-repo","text":"git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd )","title":"2. Clone Iter8 repo"},{"location":"getting-started/quick-start/with-knative/#3-install-knative-iter8-and-prometheus-add-on","text":"Knative can work with multiple networking layers. So can Iter8's Knative extension. Choose a networking layer for Knative. Contour $ITER8 /samples/knative/quickstart/platformsetup.sh contour Kourier $ITER8 /samples/knative/quickstart/platformsetup.sh kourier Gloo This step requires Python. This will install glooctl binary under $HOME/.gloo folder. $ITER8 /samples/knative/quickstart/platformsetup.sh gloo Istio $ITER8 /samples/knative/quickstart/platformsetup.sh istio","title":"3. Install Knative, Iter8, and Prometheus add-on"},{"location":"getting-started/quick-start/with-knative/#4-create-app-versions","text":"Create baseline and candidate versions of your app, sample-app-v1 and sample-app-v2 respectively. The candidate version is also referred to as the new or canary version. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app # name of the app namespace : default # namespace of the app spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : # initially all traffic goes to sample-app-v1 and none to sample-app-v2 - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0","title":"4. Create app versions"},{"location":"getting-started/quick-start/with-knative/#5-generate-requests","text":"In a production environment, your application would receive requests from end-users. For the purposes of this tutorial, simulate user requests using Fortio as follows. kubectl wait --for = condition = Ready ksvc/sample-app # URL_VALUE is the URL where your Knative application serves requests URL_VALUE = $( kubectl get ksvc sample-app -o json | jq .status.address.url ) sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/knative/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-t\" , \"6000s\" , \"-qps\" , \"16\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"5. Generate requests"},{"location":"getting-started/quick-start/with-knative/#6-define-metrics","text":"Define the Iter8 metrics used in this experiment. kubectl apply -f $ITER8 /samples/knative/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : 95th-percentile-tail-latency namespace : iter8-knative spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(revision_app_request_latencies_bucket{revision_name='$revision'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count namespace : iter8-knative spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',revision_name='$revision'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate namespace : iter8-knative spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',revision_name='$revision'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency namespace : iter8-knative spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_sum{revision_name='$revision'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : iter8-knative spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query The urlTemplate field in these metrics point to the Prometheus instance that was created in Step 3 above. If you wish to use these metrics in your production/staging/dev/test K8s cluster, change the urlTemplate values to match the URL of your Prometheus instance.","title":"6. Define metrics"},{"location":"getting-started/quick-start/with-knative/#7-launch-experiment","text":"Launch the Iter8 experiment. Iter8 will orchestrate the canary release of the new version with SLO validation and progressive deployment as specified in the experiment. kubectl apply -f $ITER8 /samples/knative/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive actions : start : # run the following sequence of tasks at the start of the experiment - task : knative/init-experiment finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : kubectl args : - \"apply\" - \"-f\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" criteria : requestCount : iter8-knative/request-count # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1 - name : promote value : baseline candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candidate The process automated by Iter8 during this experiment is depicted below.","title":"7. Launch experiment"},{"location":"getting-started/quick-start/with-knative/#8-observe-experiment","text":"Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis Install iter8ctl . You can change the directory where iter8ctl binary is installed by changing GOBIN below. GO111MODULE = on GOBIN = /usr/local/bin go get github.com/iter8-tools/iter8ctl@v0.1.3 Periodically describe the experiment. while clear ; do kubectl get experiment quickstart-exp -o yaml | iter8ctl describe -f - sleep 4 done Look inside iter8ctl output The iter8ctl output will be similar to the following. ****** Overview ****** Experiment name: quickstart-exp Experiment namespace: default Target: default/sample-app Testing pattern: Canary Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Running Number of completed iterations: 3 ****** Winner Assessment ****** > If the candidate version satisfies the experiment objectives, then it is the winner. > Otherwise, if the baseline version satisfies the experiment objectives, it is the winner. > Otherwise, there is no winner. App versions in this experiment: [ current candidate ] Winning version: candidate Version recommended for promotion: candidate ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------------+---------+-----------+ | OBJECTIVE | CURRENT | CANDIDATE | +--------------------------------------------+---------+-----------+ | iter8-knative/mean-latency < = | true | true | | 50 .000 | | | +--------------------------------------------+---------+-----------+ | iter8-knative/95th-percentile-tail-latency | true | true | | < = 100 .000 | | | +--------------------------------------------+---------+-----------+ | iter8-knative/error-rate < = | true | true | | 0 .010 | | | +--------------------------------------------+---------+-----------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------------+---------+-----------+ | METRIC | CURRENT | CANDIDATE | +--------------------------------------------+---------+-----------+ | iter8-knative/request-count | 454 .523 | 27 .412 | +--------------------------------------------+---------+-----------+ | iter8-knative/mean-latency | 1 .265 | 1 .415 | | ( milliseconds ) | | | +--------------------------------------------+---------+-----------+ | request-count | 454 .523 | 27 .619 | +--------------------------------------------+---------+-----------+ | iter8-knative/95th-percentile-tail-latency | 4 .798 | 4 .928 | | ( milliseconds ) | | | +--------------------------------------------+---------+-----------+ | iter8-knative/error-rate | 0 .000 | 0 .000 | +--------------------------------------------+---------+-----------+ As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by both versions. The candidate is identified as the winner and is recommended for promotion. When the experiment completes (in ~2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment quickstart-exp --watch kubectl get experiment output The kubectl output will be similar to the following. NAME TYPE TARGET STAGE COMPLETED ITERATIONS MESSAGE quickstart-exp Canary default/sample-app Running 1 IterationUpdate: Completed Iteration 1 quickstart-exp Canary default/sample-app Running 2 IterationUpdate: Completed Iteration 2 quickstart-exp Canary default/sample-app Running 3 IterationUpdate: Completed Iteration 3 quickstart-exp Canary default/sample-app Running 4 IterationUpdate: Completed Iteration 4 quickstart-exp Canary default/sample-app Running 5 IterationUpdate: Completed Iteration 5 quickstart-exp Canary default/sample-app Running 6 IterationUpdate: Completed Iteration 6 quickstart-exp Canary default/sample-app Running 7 IterationUpdate: Completed Iteration 7 quickstart-exp Canary default/sample-app Running 8 IterationUpdate: Completed Iteration 8 quickstart-exp Canary default/sample-app Running 9 IterationUpdate: Completed Iteration 9 When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Traffic split kubectl get ksvc sample-app -o json --watch | jq .status.traffic kubectl get ksvc output The kubectl output will be similar to the following. [ { \"latestRevision\" : false, \"percent\" : 45 , \"revisionName\" : \"sample-app-v1\" , \"tag\" : \"current\" , \"url\" : \"http://current-sample-app.default.example.com\" } , { \"latestRevision\" : true, \"percent\" : 55 , \"revisionName\" : \"sample-app-v2\" , \"tag\" : \"candidate\" , \"url\" : \"http://candidate-sample-app.default.example.com\" } ] As the experiment progresses, you should see traffic progressively shift from sample-app-v1 to sample-app-v2 . When the experiment completes, all of the traffic will be sent to the winner, sample-app-v2 . Understanding what happened You created a Knative service with two revisions, sample-app-v1 (baseline) and sample-app-v2 (candidate). You generated requests for the Knative service using a Fortio job. At the start of the experiment, 100% of the requests are sent to the baseline and 0% to the candidate. You created an Iter8 experiment with canary testing and progressive deployment patterns. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, verified that the candidate satisfied all objectives, identified the candidate as the winner, progressively shifted traffic from the baseline to the candidate, and eventually promoted the candidate using the kubectl apply command embedded within its finish action. Had the candidate failed to satisfy objectives, then the baseline would have been promoted.","title":"8. Observe experiment"},{"location":"getting-started/quick-start/with-knative/#9-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"9. Cleanup"},{"location":"metrics/defining-iter8-metrics/","text":"Defining Iter8 Metrics \u00b6 This document describes how you can create Iter8 metrics and (optionally) supply authentication information that may be required by the metrics provider. Metric providers differ in the following aspects. HTTP request authentication method: no authentication, basic auth, API keys, or bearer token HTTP request method: GET or POST Format of HTTP parameters and/or JSON body used while querying them Format of the JSON response returned by the provider The logic used by Iter8 to extract the metric value from the JSON response The examples in this document focus on Prometheus, NewRelic, Sysdig, and Elastic. However, the principles illustrated here will enable you to use metrics from any provider in experiments. Defining metrics \u00b6 Note: Metrics are defined by you, the Iter8 end-user . Prometheus Prometheus does not support any authentication mechanism out-of-the-box . However, Prometheus can be setup in conjunction with a reverse proxy, which in turn can support HTTP request authentication, as described here . No Authentication The following is an example of an Iter8 metric with Prometheus as the provider. This example assumes that Prometheus can be queried by Iter8 without any authentication. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://myprometheusservice.com/api/v1 Basic auth Suppose Prometheus is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can enable Iter8 to query this Prometheus instance as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic promcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics --namespace = myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter authType : Basic secret : myns/promcredentials jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : https://my.secure.prometheus.service.com/api/v1 Brief explanation of the request-count metric Prometheus enables metric queries using HTTP GET requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of request-count , and defaulted to GET . Iter8 will query Prometheus during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named query as required by Prometheus . The value of this parameter is derived by substituting the placeholders in the value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Prometheus. The urlTemplate field provides the URL of the prometheus service. New Relic New Relic uses API Keys to authenticate requests as documented here . The API key may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. API key embedded in metric The following is an example of an Iter8 metric with Prometheus as the provider. In this example, t0p-secret-api-key is the New Relic API key. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter headerTemplates : - name : X-Query-Key value : t0p-secret-api-key jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id API key embedded in secret Suppose your New Relic API key is t0p-secret-api-key ; you wish to store this API key in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the API key. kubectl create secret generic nrcredentials -n myns --from-literal = mykey = t0p-secret-api-key The above secret contains a data field named mykey whose value is the API key. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics --namespace = myns Define metric: When defining the metric, ensure that the authType field is set to APIKey and the appropriate secret is referenced. In the headerTemplates field, include X-Query-Key as the name of a header field (as required by New Relic ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${mykey} at query time, by looking up the referenced secret named nrcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter authType : APIKey secret : myns/nrcredentials headerTemplates : - name : X-Query-Key value : ${mykey} jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id Brief explanation of the name-count metric New Relic enables metric queries using both HTTP GET or POST requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of name-count , and defaulted to GET . Iter8 will query New Relic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named nrql as required by New Relic . The value of this parameter is derived by substituting the placeholders in its value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by New Relic. The urlTemplate field provides the URL of the New Relic service. Sysdig Sysdig data API accepts HTTP POST requests and uses a bearer token for authentication as documented here . The bearer token may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. Bearer token embedded in metric The following is an example of an Iter8 metric with Sysdig as the provider. In this example, 87654321-1234-1234-1234-123456789012 is the Sysdig bearer token (also referred to as access key by Sysdig). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer 87654321-1234-1234-1234-123456789012 jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Bearer token embedded in secret Suppose your Sysdig token is 87654321-1234-1234-1234-123456789012 ; you wish to store this token in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the token. kubectl create secret generic sdcredentials -n myns --from-literal = token = 87654321 -1234-1234-1234-123456789012 The above secret contains a data field named token whose value is the Sysdig token. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics --namespace = myns Define metric: When defining the metric, ensure that the authType field is set to Bearer and the appropriate secret is referenced. In the headerTemplates field, include Authorize header field (as required by Sysdig ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${token} at query time, by looking up the referenced secret named sdcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST authType : Bearer secret : myns/sdcredentials type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer ${token} jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Brief explanation of the cpu-utilization metric Sysdig enables metric queries using both POST requests; hence, the method field of the Iter8 metric is set to POST. Iter8 will query Sysdig during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Sysdig . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Sysdig. The urlTemplate field provides the URL of the Sysdig service. Elastic Elasticsearch REST API accepts HTTP GET or POST requests and uses basic authentication as documented here . Suppose Elasticsearch is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can then enable Iter8 to query the Elasticsearch service as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic elasticcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics --namespace = myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : average-sales spec : description : An elastic example provider : elastic body : >- { \"aggs\": { \"range\": { \"date_range\": { \"field\": \"date\", \"ranges\": [ { \"from\": \"now-${elapsedTime}s/s\" } ] } }, \"items_to_sell\": { \"filter\": { \"term\": { \"version\": \"${revision}\" } }, \"aggs\": { \"avg_sales\": { \"avg\": { \"field\": \"sale_price\" } } } } } } method : POST authType : Basic secret : myns/elasticcredentials type : Gauge headerTemplates : - name : Content-Type value : application/json jqExpression : \".aggregations.items_to_sell.avg_sales.value | tonumber\" urlTemplate : https://secure.elastic.com/my/sales Brief explanation of the average sales metric Elastic enables metric queries using GET or POST requests. In the elastic example, The method field of the Iter8 metric is set to POST. Iter8 will query Elastic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Elastic . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Elastic. The urlTemplate field provides the URL of the Elastic service. Placeholder substitution \u00b6 Note: This step is automated by Iter8 . Iter8 will substitute placeholders in the metric query based on the time elapsed since the start of the experiment, and information associated with each version in the experiment. Suppose the metrics defined above are referenced within an experiment as follows. Further, suppose this experiment has started, Iter8 is about to do an iteration of this experiment, and the time elapsed since the start of the experiment is 600 seconds. Look inside sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : sample-exp spec : target : default/sample-app strategy : testingPattern : Canary criteria : # This experiment assumes that metrics have been created in the `myns` namespace requestCount : myns/request-count objectives : - metric : myns/name-count lowerLimit : 50 - metric : myns/cpu-utilization upperLimit : 90 - metric : myns/average-sales lowerLimit : \"250.0\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : userfilter value : 'usergroup!~\"wakanda\"' candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : userfilter value : 'usergroup=~\"wakanda\"' For the sample experiment above, Iter8 will use two HTTP(S) queries to fetch metric values, one for the baseline version, and another for the candidate version. Prometheus Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named query whose value equals: sum(increase(revision_app_request_latencies_count{service_name='current',usergroup!~\"wakanda\"}[600s])) or on() vector(0) New Relic Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named nrql whose value equals: SELECT count(appName) FROM PageView WHERE revisionName='sample-app-v1' SINCE 600 seconds ago Sysdig Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"last\" : 600 , \"sampling\" : 600 , \"filter\" : \"kubernetes.app.revision.name = 'sample-app-v1'\" , \"metrics\" : [ { \"id\" : \"cpu.cores.used\" , \"aggregations\" : { \"time\" : \"avg\" , \"group\" : \"sum\" } } ], \"dataSourceType\" : \"container\" , \"paging\" : { \"from\" : 0 , \"to\" : 99 } } Elastic Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"aggs\" : { \"range\" : { \"date_range\" : { \"field\" : \"date\" , \"ranges\" : [ { \"from\" : \"now-600s/s\" } ] } }, \"items_to_sell\" : { \"filter\" : { \"term\" : { \"version\" : \"sample-app-v1\" } }, \"aggs\" : { \"avg_sales\" : { \"avg\" : { \"field\" : \"sale_price\" } } } } } } The placeholder $elapsedTime has been substituted with 600, which is the time elapsed since the start of the experiment. The other placeholders have been substituted based on the versionInfo field of the baseline version in the experiment. Iter8 builds and sends an HTTP request in a similar manner for the candidate version as well. JSON response \u00b6 Note: This step is handled by the metrics provider . The metrics provider is expected to respond to Iter8's HTTP request with a JSON object. The format of this JSON object is defined by the provider. Prometheus The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } } New Relic The format of the New Relic JSON response is discussed here . A sample New Relic response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"results\" : [ { \"count\" : 80275388 } ], \"metadata\" : { \"eventTypes\" : [ \"PageView\" ], \"eventType\" : \"PageView\" , \"openEnded\" : true , \"beginTime\" : \"2014-08-03T19:00:00Z\" , \"endTime\" : \"2017-01-18T23:18:41Z\" , \"beginTimeMillis=\" : 1407092400000 , \"endTimeMillis\" : 1484781521198 , \"rawSince\" : \"'2014-08-04 00:00:00+0500'\" , \"rawUntil\" : \"`now`\" , \"rawCompareWith\" : \"\" , \"clippedTimeWindows\" : { \"Browser\" : { \"beginTimeMillis\" : 1483571921198 , \"endTimeMillis\" : 1484781521198 , \"retentionMillis\" : 1209600000 } }, \"messages\" : [], \"contents\" : [ { \"function\" : \"count\" , \"attribute\" : \"appName\" , \"simple\" : true } ] } } Sysdig The format of the Sysdig JSON response is discussed here . A sample Sysdig response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\" : [ { \"t\" : 1582756200 , \"d\" : [ 6.481 ] } ], \"start\" : 1582755600 , \"end\" : 1582756200 } Elastic The format of the Elastic JSON response is discussed here . A sample Elastic response is as follows. 1 2 3 4 5 6 7 8 { \"aggregations\" : { \"items_to_sell\" : { \"doc_count\" : 3 , \"avg_sales\" : { \"value\" : 128.33333333333334 } } } } Processing the JSON response \u00b6 Note: This step is automated by Iter8 . Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. New Relic Consider the jqExpression defined in the sample New Relic metric . Let us apply it to the sample JSON response from New Relic . echo '{ \"results\": [ { \"count\": 80275388 } ], \"metadata\": { \"eventTypes\": [ \"PageView\" ], \"eventType\": \"PageView\", \"openEnded\": true, \"beginTime\": \"2014-08-03T19:00:00Z\", \"endTime\": \"2017-01-18T23:18:41Z\", \"beginTimeMillis=\": 1407092400000, \"endTimeMillis\": 1484781521198, \"rawSince\": \"' 2014 -08-04 00 :00:00+0500 '\", \"rawUntil\": \"`now`\", \"rawCompareWith\": \"\", \"clippedTimeWindows\": { \"Browser\": { \"beginTimeMillis\": 1483571921198, \"endTimeMillis\": 1484781521198, \"retentionMillis\": 1209600000 } }, \"messages\": [], \"contents\": [ { \"function\": \"count\", \"attribute\": \"appName\", \"simple\": true } ] } }' | jq \".results[0].count | tonumber\" Executing the above command results yields 80275388 , a number, as required by Iter8. Sysdig Consider the jqExpression defined in the sample Sysdig metric . Let us apply it to the sample JSON response from Sysdig . echo '{ \"data\": [ { \"t\": 1582756200, \"d\": [ 6.481 ] } ], \"start\": 1582755600, \"end\": 1582756200 }' | jq \".data[0].d[0] | tonumber\" Executing the above command results yields 6.481 , a number, as required by Iter8. Elastic Consider the jqExpression defined in the sample Elastic metric . Let us apply it to the sample JSON response from Elastic . echo '{ \"aggregations\": { \"items_to_sell\": { \"doc_count\": 3, \"avg_sales\": { \"value\": 128.33333333333334 } } } }' | jq \".aggregations.items_to_sell.avg_sales.value | tonumber\" Executing the above command results yields 128.33333333333334 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression . Error handling \u00b6 Note: This step is automated by Iter8 . Errors may occur during Iter8's metric queries due to a number of reasons (for example, due to an invalid jqExpression supplied within the metric). If Iter8 encounters errors during its attempt to retrieve metric values, Iter8 will mark the respective metric as unavailable. Iter8 can be used with any provider that can receive an HTTP request and respond with a JSON object containing the metrics information. Documentation requests and contributions (PRs) are welcome for providers not listed here. \u21a9 In a conformance experiment, n = 1 . In canary and A/B experiments, n = 2 . In A/B/n experiments, n > 2 . \u21a9 \u21a9 \u21a9 \u21a9","title":"Defining Iter8 metrics"},{"location":"metrics/defining-iter8-metrics/#defining-iter8-metrics","text":"This document describes how you can create Iter8 metrics and (optionally) supply authentication information that may be required by the metrics provider. Metric providers differ in the following aspects. HTTP request authentication method: no authentication, basic auth, API keys, or bearer token HTTP request method: GET or POST Format of HTTP parameters and/or JSON body used while querying them Format of the JSON response returned by the provider The logic used by Iter8 to extract the metric value from the JSON response The examples in this document focus on Prometheus, NewRelic, Sysdig, and Elastic. However, the principles illustrated here will enable you to use metrics from any provider in experiments.","title":"Defining Iter8 Metrics"},{"location":"metrics/defining-iter8-metrics/#defining-metrics","text":"Note: Metrics are defined by you, the Iter8 end-user . Prometheus Prometheus does not support any authentication mechanism out-of-the-box . However, Prometheus can be setup in conjunction with a reverse proxy, which in turn can support HTTP request authentication, as described here . No Authentication The following is an example of an Iter8 metric with Prometheus as the provider. This example assumes that Prometheus can be queried by Iter8 without any authentication. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://myprometheusservice.com/api/v1 Basic auth Suppose Prometheus is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can enable Iter8 to query this Prometheus instance as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic promcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics --namespace = myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter authType : Basic secret : myns/promcredentials jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : https://my.secure.prometheus.service.com/api/v1 Brief explanation of the request-count metric Prometheus enables metric queries using HTTP GET requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of request-count , and defaulted to GET . Iter8 will query Prometheus during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named query as required by Prometheus . The value of this parameter is derived by substituting the placeholders in the value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Prometheus. The urlTemplate field provides the URL of the prometheus service. New Relic New Relic uses API Keys to authenticate requests as documented here . The API key may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. API key embedded in metric The following is an example of an Iter8 metric with Prometheus as the provider. In this example, t0p-secret-api-key is the New Relic API key. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter headerTemplates : - name : X-Query-Key value : t0p-secret-api-key jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id API key embedded in secret Suppose your New Relic API key is t0p-secret-api-key ; you wish to store this API key in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the API key. kubectl create secret generic nrcredentials -n myns --from-literal = mykey = t0p-secret-api-key The above secret contains a data field named mykey whose value is the API key. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics --namespace = myns Define metric: When defining the metric, ensure that the authType field is set to APIKey and the appropriate secret is referenced. In the headerTemplates field, include X-Query-Key as the name of a header field (as required by New Relic ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${mykey} at query time, by looking up the referenced secret named nrcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter authType : APIKey secret : myns/nrcredentials headerTemplates : - name : X-Query-Key value : ${mykey} jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id Brief explanation of the name-count metric New Relic enables metric queries using both HTTP GET or POST requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of name-count , and defaulted to GET . Iter8 will query New Relic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named nrql as required by New Relic . The value of this parameter is derived by substituting the placeholders in its value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by New Relic. The urlTemplate field provides the URL of the New Relic service. Sysdig Sysdig data API accepts HTTP POST requests and uses a bearer token for authentication as documented here . The bearer token may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. Bearer token embedded in metric The following is an example of an Iter8 metric with Sysdig as the provider. In this example, 87654321-1234-1234-1234-123456789012 is the Sysdig bearer token (also referred to as access key by Sysdig). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer 87654321-1234-1234-1234-123456789012 jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Bearer token embedded in secret Suppose your Sysdig token is 87654321-1234-1234-1234-123456789012 ; you wish to store this token in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the token. kubectl create secret generic sdcredentials -n myns --from-literal = token = 87654321 -1234-1234-1234-123456789012 The above secret contains a data field named token whose value is the Sysdig token. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics --namespace = myns Define metric: When defining the metric, ensure that the authType field is set to Bearer and the appropriate secret is referenced. In the headerTemplates field, include Authorize header field (as required by Sysdig ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${token} at query time, by looking up the referenced secret named sdcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST authType : Bearer secret : myns/sdcredentials type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer ${token} jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Brief explanation of the cpu-utilization metric Sysdig enables metric queries using both POST requests; hence, the method field of the Iter8 metric is set to POST. Iter8 will query Sysdig during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Sysdig . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Sysdig. The urlTemplate field provides the URL of the Sysdig service. Elastic Elasticsearch REST API accepts HTTP GET or POST requests and uses basic authentication as documented here . Suppose Elasticsearch is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can then enable Iter8 to query the Elasticsearch service as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic elasticcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics --namespace = myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : average-sales spec : description : An elastic example provider : elastic body : >- { \"aggs\": { \"range\": { \"date_range\": { \"field\": \"date\", \"ranges\": [ { \"from\": \"now-${elapsedTime}s/s\" } ] } }, \"items_to_sell\": { \"filter\": { \"term\": { \"version\": \"${revision}\" } }, \"aggs\": { \"avg_sales\": { \"avg\": { \"field\": \"sale_price\" } } } } } } method : POST authType : Basic secret : myns/elasticcredentials type : Gauge headerTemplates : - name : Content-Type value : application/json jqExpression : \".aggregations.items_to_sell.avg_sales.value | tonumber\" urlTemplate : https://secure.elastic.com/my/sales Brief explanation of the average sales metric Elastic enables metric queries using GET or POST requests. In the elastic example, The method field of the Iter8 metric is set to POST. Iter8 will query Elastic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Elastic . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Elastic. The urlTemplate field provides the URL of the Elastic service.","title":"Defining metrics"},{"location":"metrics/defining-iter8-metrics/#placeholder-substitution","text":"Note: This step is automated by Iter8 . Iter8 will substitute placeholders in the metric query based on the time elapsed since the start of the experiment, and information associated with each version in the experiment. Suppose the metrics defined above are referenced within an experiment as follows. Further, suppose this experiment has started, Iter8 is about to do an iteration of this experiment, and the time elapsed since the start of the experiment is 600 seconds. Look inside sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : sample-exp spec : target : default/sample-app strategy : testingPattern : Canary criteria : # This experiment assumes that metrics have been created in the `myns` namespace requestCount : myns/request-count objectives : - metric : myns/name-count lowerLimit : 50 - metric : myns/cpu-utilization upperLimit : 90 - metric : myns/average-sales lowerLimit : \"250.0\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : userfilter value : 'usergroup!~\"wakanda\"' candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : userfilter value : 'usergroup=~\"wakanda\"' For the sample experiment above, Iter8 will use two HTTP(S) queries to fetch metric values, one for the baseline version, and another for the candidate version. Prometheus Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named query whose value equals: sum(increase(revision_app_request_latencies_count{service_name='current',usergroup!~\"wakanda\"}[600s])) or on() vector(0) New Relic Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named nrql whose value equals: SELECT count(appName) FROM PageView WHERE revisionName='sample-app-v1' SINCE 600 seconds ago Sysdig Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"last\" : 600 , \"sampling\" : 600 , \"filter\" : \"kubernetes.app.revision.name = 'sample-app-v1'\" , \"metrics\" : [ { \"id\" : \"cpu.cores.used\" , \"aggregations\" : { \"time\" : \"avg\" , \"group\" : \"sum\" } } ], \"dataSourceType\" : \"container\" , \"paging\" : { \"from\" : 0 , \"to\" : 99 } } Elastic Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"aggs\" : { \"range\" : { \"date_range\" : { \"field\" : \"date\" , \"ranges\" : [ { \"from\" : \"now-600s/s\" } ] } }, \"items_to_sell\" : { \"filter\" : { \"term\" : { \"version\" : \"sample-app-v1\" } }, \"aggs\" : { \"avg_sales\" : { \"avg\" : { \"field\" : \"sale_price\" } } } } } } The placeholder $elapsedTime has been substituted with 600, which is the time elapsed since the start of the experiment. The other placeholders have been substituted based on the versionInfo field of the baseline version in the experiment. Iter8 builds and sends an HTTP request in a similar manner for the candidate version as well.","title":"Placeholder substitution"},{"location":"metrics/defining-iter8-metrics/#json-response","text":"Note: This step is handled by the metrics provider . The metrics provider is expected to respond to Iter8's HTTP request with a JSON object. The format of this JSON object is defined by the provider. Prometheus The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } } New Relic The format of the New Relic JSON response is discussed here . A sample New Relic response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"results\" : [ { \"count\" : 80275388 } ], \"metadata\" : { \"eventTypes\" : [ \"PageView\" ], \"eventType\" : \"PageView\" , \"openEnded\" : true , \"beginTime\" : \"2014-08-03T19:00:00Z\" , \"endTime\" : \"2017-01-18T23:18:41Z\" , \"beginTimeMillis=\" : 1407092400000 , \"endTimeMillis\" : 1484781521198 , \"rawSince\" : \"'2014-08-04 00:00:00+0500'\" , \"rawUntil\" : \"`now`\" , \"rawCompareWith\" : \"\" , \"clippedTimeWindows\" : { \"Browser\" : { \"beginTimeMillis\" : 1483571921198 , \"endTimeMillis\" : 1484781521198 , \"retentionMillis\" : 1209600000 } }, \"messages\" : [], \"contents\" : [ { \"function\" : \"count\" , \"attribute\" : \"appName\" , \"simple\" : true } ] } } Sysdig The format of the Sysdig JSON response is discussed here . A sample Sysdig response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\" : [ { \"t\" : 1582756200 , \"d\" : [ 6.481 ] } ], \"start\" : 1582755600 , \"end\" : 1582756200 } Elastic The format of the Elastic JSON response is discussed here . A sample Elastic response is as follows. 1 2 3 4 5 6 7 8 { \"aggregations\" : { \"items_to_sell\" : { \"doc_count\" : 3 , \"avg_sales\" : { \"value\" : 128.33333333333334 } } } }","title":"JSON response"},{"location":"metrics/defining-iter8-metrics/#processing-the-json-response","text":"Note: This step is automated by Iter8 . Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. New Relic Consider the jqExpression defined in the sample New Relic metric . Let us apply it to the sample JSON response from New Relic . echo '{ \"results\": [ { \"count\": 80275388 } ], \"metadata\": { \"eventTypes\": [ \"PageView\" ], \"eventType\": \"PageView\", \"openEnded\": true, \"beginTime\": \"2014-08-03T19:00:00Z\", \"endTime\": \"2017-01-18T23:18:41Z\", \"beginTimeMillis=\": 1407092400000, \"endTimeMillis\": 1484781521198, \"rawSince\": \"' 2014 -08-04 00 :00:00+0500 '\", \"rawUntil\": \"`now`\", \"rawCompareWith\": \"\", \"clippedTimeWindows\": { \"Browser\": { \"beginTimeMillis\": 1483571921198, \"endTimeMillis\": 1484781521198, \"retentionMillis\": 1209600000 } }, \"messages\": [], \"contents\": [ { \"function\": \"count\", \"attribute\": \"appName\", \"simple\": true } ] } }' | jq \".results[0].count | tonumber\" Executing the above command results yields 80275388 , a number, as required by Iter8. Sysdig Consider the jqExpression defined in the sample Sysdig metric . Let us apply it to the sample JSON response from Sysdig . echo '{ \"data\": [ { \"t\": 1582756200, \"d\": [ 6.481 ] } ], \"start\": 1582755600, \"end\": 1582756200 }' | jq \".data[0].d[0] | tonumber\" Executing the above command results yields 6.481 , a number, as required by Iter8. Elastic Consider the jqExpression defined in the sample Elastic metric . Let us apply it to the sample JSON response from Elastic . echo '{ \"aggregations\": { \"items_to_sell\": { \"doc_count\": 3, \"avg_sales\": { \"value\": 128.33333333333334 } } } }' | jq \".aggregations.items_to_sell.avg_sales.value | tonumber\" Executing the above command results yields 128.33333333333334 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression .","title":"Processing the JSON response"},{"location":"metrics/defining-iter8-metrics/#error-handling","text":"Note: This step is automated by Iter8 . Errors may occur during Iter8's metric queries due to a number of reasons (for example, due to an invalid jqExpression supplied within the metric). If Iter8 encounters errors during its attempt to retrieve metric values, Iter8 will mark the respective metric as unavailable. Iter8 can be used with any provider that can receive an HTTP request and respond with a JSON object containing the metrics information. Documentation requests and contributions (PRs) are welcome for providers not listed here. \u21a9 In a conformance experiment, n = 1 . In canary and A/B experiments, n = 2 . In A/B/n experiments, n > 2 . \u21a9 \u21a9 \u21a9 \u21a9","title":"Error handling"},{"location":"metrics/using-metrics/","text":"Using Metrics in Experiments \u00b6 Iter8 metrics API Iter8 defines a new Kubernetes resource called Metric that makes it easy to use metrics in experiments from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic. List metrics available in your cluster using the kubectl get metrics.iter8.tools command. Use metrics in experiments by referencing them in experiment criteria. Listing metrics \u00b6 Iter8 metrics are Kubernetes resources which means you can list them using kubectl get . kubectl get metrics.iter8.tools --all-namespaces NAMESPACE NAME TYPE DESCRIPTION iter8-knative 95th-percentile-tail-latency Gauge 95th percentile tail latency iter8-knative error-count Counter Number of error responses iter8-knative error-rate Gauge Fraction of requests with error responses iter8-knative mean-latency Gauge Mean latency iter8-knative request-count Counter Number of requests Referencing metrics \u00b6 Use metrics in experiments by referencing them in criteria section. Reference metrics using the namespace/name or name format . Sample experiment illustrating the use of metrics apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive actions : start : # run the following sequence of tasks at the start of the experiment - task : knative/init-experiment finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : kubectl args : - \"apply\" - \"-f\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" criteria : requestCount : iter8-knative/request-count # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1 - name : promote value : baseline candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candidate Observing metric values \u00b6 During an experiment, Iter8 reports the metric values observed for each version. Use iter8ctl to observe these metric values in realtime. See here for an example.","title":"Using metrics in experiments"},{"location":"metrics/using-metrics/#using-metrics-in-experiments","text":"Iter8 metrics API Iter8 defines a new Kubernetes resource called Metric that makes it easy to use metrics in experiments from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic. List metrics available in your cluster using the kubectl get metrics.iter8.tools command. Use metrics in experiments by referencing them in experiment criteria.","title":"Using Metrics in Experiments"},{"location":"metrics/using-metrics/#listing-metrics","text":"Iter8 metrics are Kubernetes resources which means you can list them using kubectl get . kubectl get metrics.iter8.tools --all-namespaces NAMESPACE NAME TYPE DESCRIPTION iter8-knative 95th-percentile-tail-latency Gauge 95th percentile tail latency iter8-knative error-count Counter Number of error responses iter8-knative error-rate Gauge Fraction of requests with error responses iter8-knative mean-latency Gauge Mean latency iter8-knative request-count Counter Number of requests","title":"Listing metrics"},{"location":"metrics/using-metrics/#referencing-metrics","text":"Use metrics in experiments by referencing them in criteria section. Reference metrics using the namespace/name or name format . Sample experiment illustrating the use of metrics apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive actions : start : # run the following sequence of tasks at the start of the experiment - task : knative/init-experiment finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : kubectl args : - \"apply\" - \"-f\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" criteria : requestCount : iter8-knative/request-count # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1 - name : promote value : baseline candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candidate","title":"Referencing metrics"},{"location":"metrics/using-metrics/#observing-metric-values","text":"During an experiment, Iter8 reports the metric values observed for each version. Use iter8ctl to observe these metric values in realtime. See here for an example.","title":"Observing metric values"},{"location":"reference/apispec/","text":"Iter8 API Specification \u00b6 Abstract The Iter8 API provides two Kubernetes custom resources to automate metrics and AI-driven experiments, progressive delivery, and rollout of Kubernetes and OpenShift apps. The Experiment resource provides expressive controls required by application developers and service operators who wish to automate new releases of their apps in a robust, principled and metrics-driven manner. These controls encompass testing, deployment, traffic shaping, and version promotion functions and can be flexibly composed to automate diverse use-cases . The Metric resource encapsulates the REST query that is used by Iter8 for retrieving a metric value from the metrics provider. Metrics are referenced in experiments. API Version This document describes version v2alpha2 of the Iter8 API. Resources \u00b6 Experiment \u00b6 Sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a canary test testingPattern : Canary actions : start : # run a sequence of tasks at the start of the experiment - task : knative/init-experiment finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : kubectl args : - \"apply\" - \"-f\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : current variables : # variables are used when querying metrics and when interpolating task inputs - name : revision value : sample-app-v1 - name : promote value : baseline candidates : - name : candidate variables : # variables are used when querying metrics and when interpolating task inputs - name : revision value : sample-app-v2 - name : promote value : candidate Metadata \u00b6 Standard Kubernetes meta.v1/ObjectMeta resource. Spec \u00b6 Field name Field type Description Required target string Identifies the app under experimentation and determines which experiments can run concurrently. Experiments that have the same target value will not be scheduled concurrently but will be run sequentially in the order of their creation timestamps. Experiments whose target values differ from each other can be scheduled by Iter8 concurrently. It is good practice to follow target naming conventions . Yes strategy Strategy The experimentation strategy which specifies how app versions are tested, how traffic is shifted during experiment, and what tasks are executed at the start and end of the experiment. Yes criteria Criteria Criteria used for evaluating versions. This section includes service-level objectives (SLOs) and indicators (SLIs). No duration Duration Duration of the experiment. No versionInfo VersionInfo Versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates. No Status \u00b6 Field name Field type Description Required conditions [] ExperimentCondition A set of conditions that express progress of an experiment. No initTime metav1.Time The time the experiment is created. No startTime metav1.Time The time when the first iteration of experiment begins No lastUpdateTime metav1.Time The time when the status was most recently updated. No stage string Indicator of the progress of an experiment. The stage is Waiting before an experiment executes its start action, Initializing while running the start action, Running while the experiment has begun its first iteration and is progressing, Finishing while any finish action is running and Completed when the experiment terminates. No completedIterations int32 Number of completed iterations of the experiment. This is undefined until the experiment reaches the Running stage. No currentWeightDistribution [] WeightData Currently observed split of traffic between versions. Expressed as percentage. No analysis Analysis Result of latest query to the Iter8 analytics service. No versionRecommendedForPromotion string The version recommended for promotion. This field is initially populated by Iter8 as the baseline version and continuously updated during the course of the experiment to match the winner. The value of this field is typically used by finish actions to promote a version at the end of an experiment. No metrics [] MetricInfo A list of metrics referenced in the criteria section of this experiment. No message string Human readable message. No Metric \u00b6 Metrics are referenced within the criteria field of the experiment spec. Metrics usage within experiments is described here . Sample metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[$elapsedTime])) or on() vector(0) description : Number of requests type : counter provider : prometheus jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Metadata \u00b6 Standard Kubernetes meta.v1/ObjectMeta resource. Spec \u00b6 Field name Field type Description Required description string Human readable description. This field is meant for informational purposes. No units string Units of measurement. This field is meant for informational purposes. No provider string Type of the metrics provider. This field is meant for informational purposes. No params [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP query parameters used by Iter8 when querying the metrics provider. Each name represents a parameter name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. No body string String used to construct the JSON body of the HTTP request. Body may be templated, in which Iter8 will attempt to substitute placeholders in the template at query time using version information. No type string Metric type. Valid values are Counter and Gauge . Default value = Gauge . A Counter metric is one whose value never decreases over time. A Gauge metric is one whose value may increase or decrease over time. No method string HTTP method (verb) used in the HTTP request. Valid values are GET and POST . Default value = GET . No authType string Identifies the type of authentication used in the HTTP request. Valid values are Basic , Bearer and APIKey which correspond to HTTP authentication with these respective methods. No sampleSize string Reference to a metric that represents the number of data points over which the value of this metric is computed. This field applies only to Gauge metrics. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace of the referring metric. No secret string Reference to a secret that contains information used for authenticating with the metrics provider. In particular, Iter8 uses data in this secret to substitute placeholders in the HTTP headers and URL while querying the provider. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace where Iter8 is installed (which is iter8-system by default). No headerTemplates [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP request headers used by Iter8 when querying the metrics provider. Each name represents a header field name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. Placeholder substitution is attempted only if authType and secret fields are present. No jqExpression string The jq expression used by Iter8 to extract the metric value from the JSON response returned by the provider. Yes urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. Yes Experiment field types \u00b6 Strategy \u00b6 Field name Field type Description Required testingPattern string Determines the logic used to evaluate the app versions and determine the winner of the experiment. Iter8 supports two testing patterns, namely, Canary and Conformance . Yes deploymentPattern string Determines if and how traffic is shifted during an experiment. This field is relevant only for experiments using the Canary testing pattern. Iter8 supports two deployment patterns, namely, Progressive and FixedSplit . No actions map[string][] TaskSpec An action is a sequence of tasks that can be executed by Iter8. spec.strategy.actions can be used to specify start and finish actions that will be run at the start and end of an experiment respectively. No TaskSpec \u00b6 Specification of a task that will be executed as part of experiment actions. Task implementations are organized into libraries as documented here . Field name Field type Description Required task string Name of the task. Task names express both the library and the task within the library in the format 'library/task' . Yes with map[string] apiextensionsv1.JSON Inputs to the task. No Criteria \u00b6 Field name Field type Description Required requestCount string Reference to the metric used to count the number of requests sent to app versions. No objectives Objective [] A list of metrics along with acceptable upper limits, lower limits, or both upper and lower limits for them. Iter8 will verify if app versions satisfy these objectives. No indicators string[] A list of metric references. Iter8 will collect and report the values of these metrics in addition to those referenced in the objectives section. No Note: References to metric resource objects within experiment criteria should be in the namespace/name format or in the name format. If the name format is used (i.e., if only the name of the metric is specified), then Iter8 searches for the metric in the namespace of the experiment resource. If Iter8 cannot find the metric, then the reference is considered invalid and the experiment will terminate in a failure. Objective \u00b6 Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes upperLimit Quantity Upper limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be below the limit. No lowerLimit Quantity Lower limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be above the limit. No Duration \u00b6 The duration of the experiment. Field name Field type Description Required intervalSeconds int32 Duration of a single iteration of the experiment in seconds. Default value = 20 seconds. No maxIterations int32 Maximum number of iterations in the experiment. In case of failure, the experiment may be terminated earlier. Default value = 15. No VersionInfo \u00b6 spec.versionInfo describes the app versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates . Field name Field type Description Required baseline VersionDetail Details of the current or baseline version. Yes candidates [] VersionDetail Details of the candidate version or versions, if any. No Number of versions Conformance experiments involve only a single version (baseline). Hence, in Conformance experiments, the candidates field of versionInfo must be omitted. A Canary experiment involves two versions, baseline and candidate . Hence, in Canary experiments, the candidates field must be a list of length one and must contain a single versionDetail object. 1 Auto-creation of VersionInfo Iter8 ships with helper tasks that can inspect an experiment resource with no or partially specified spec.versionInfo , automatically generate the remaining portion of spec.versionInfo and update the experiment with this information. See the init-experiment task in the knative task library for an example. VersionDetail \u00b6 Field name Field type Description Required name string Name of the version. Yes variables [] NamedValue Variables are name-value pairs associated with a version. Metrics and tasks within experiment specs can contain strings with placeholders. Iter8 uses variables to substitute placeholders in these strings. No weightObjRef corev1.ObjectReference Reference to a Kubernetes resource and a field-path within the resource. Iter8 uses weightObjRef to get or set weight (traffic percentage) for the version. No MetricInfo \u00b6 Field name Field type Description Required name string Identifies an Iter8 metric using the namespace/name or name format . Yes metric [] Metric Iter8 metric object referenced by name. No ExperimentCondition \u00b6 Conditions express aspects of the progress of an experiment. The Completed condition indicates whether or not an experiment has completed. The Failed condition indicates whether or not an experiment completed successfully or in failure. The TargetAcquired condition indicates that an experiment has acquired the target and is now scheduled to run. At any point in time, for any given target, Iter8 ensures that at most one experiment has the conditions TargetAcquired set to True and Completed set to False . Field name Field type Description Required type string Type of condition. Valid types are TargetAcquired , Completed and Failed . Yes status corev1.ConditionStatus status of condition, one of True , False , or Unknown . Yes lastTransitionTime metav1.Time The last time any field in the condition was changed. No reason string A reason for the change in value. No message string Human readable decription. No Analysis \u00b6 Field name Field type Description Required aggregatedMetrics AggregatedMetricsAnalysis Most recently observed metric values for all metrics referenced in the experiment criteria. No winnerAssessment WinnerAssessmentAnalysis Information about the winner of the experiment. No versionAssessments VersionAssessmentAnalysis For each version, a summary analysis identifying whether or not the version is satisfying the experiment criteria. No weights WeightsAnalysis Recommended weight distribution to be applied before the next iteration of the experiment. No VersionAssessmentAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string][]bool map of version name to a list of boolean values, one for each objective specified in the experiment criteria, indicating whether not the objective is satisified. No AggregatedMetricsAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string] AggregatedMetricsData Map from metric name to most recent data (from all versions) for the metric. Yes AggregatedMetricsData \u00b6 Field name Field type Description Required max Quantity The maximum value observed for this metric accross all versions. Yes min Quantity The minimum value observed for this metric accross all versions. Yes data map[string] AggregatedMetricsVersionData A map from version name to the most recent aggregated metrics data for that version. No AggregatedMetricsVersionData \u00b6 Field name Field type Description Required max Quantity The maximum value observed for this metric for this version over all observations. No min Quantity The minimum value observed for this metric for this version over all observations. No value Quantity The value. No sampleSize int32 The number of requests observed by this version. No WinnerAssessmentAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data WinnerAssessmentData Details on whether or not a winner has been identified and which version if so. No WinnerAssessmentData \u00b6 Field name Field type Description Required winnerFound bool Whether or not a winner has been identified. Yes winner string The name of the identified winner, if one has been found. No WeightAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data [] WeightData List of version name/value pairs representing a recommended weight for each version No WeightData \u00b6 Field name Field type Description Required name string Version name Yes value int32 Percentage of traffic being sent to the version. Yes Common field types \u00b6 NamedValue \u00b6 Field name Field type Description Required name string Name of a variable. Yes value string Value of a variable. Yes Tasks \u00b6 Tasks are an extension mechanism for enhancing the behavior of Iter8 experiments and can be specified within the spec.strategy.actions field of the experiment. Task implementations \u00b6 Iter8 currently implements two tasks that help in setting up and finishing up experiments. These tasks are organized into the knative and common task libraries. Sample experiment with start and finish actions with tasks 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # `sample-app` Knative service in `default` namespace is the target of this experiment target : default/sample-app # information about app versions participating in this experiment versionInfo : # every experiment has a baseline version # we will name it `current` baseline : name : current variables : # `revision` variable is used for fetching metrics from Prometheus - name : revision value : sample-app-v1 # `promote` variable is used by the finish task - name : promote value : base # candidate version(s) of the app # there is a single candidate in this experiment # we will name it `candidate` candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candid criteria : objectives : # mean latency should be under 50 milliseconds - metric : iter8-knative/mean-latency upperLimit : 50 # 95th percentile latency should be under 100 milliseconds - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 # error rate should be under 1% - metric : iter8-knative/error-rate upperLimit : \"0.01\" indicators : # report values for the following metrics in addition those in spec.criteria.objectives - 99th-percentile-tail-latency - 90th-percentile-tail-latency - 75th-percentile-tail-latency strategy : # canary testing => candidate `wins` if it satisfies objectives testingPattern : Canary # progressively shift traffic to candidate, assuming it satisfies objectives deploymentPattern : Progressive actions : # run tasks under the `start` action at the start of an experiment start : # the following task verifies that the `sample-app` Knative service in the `default` namespace is available and ready # it then updates the experiment resource with information needed to shift traffic between app versions - task : knative/init-experiment # run tasks under the `finish` action at the end of an experiment finish : # promote an app version # `https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml` will be applied if candidate satisfies objectives # `https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml` will be applied if candidate fails to satisfy objectives - task : common/exec # promote the winning version with : cmd : kubectl args : - \"apply\" - \"-f\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" duration : # 12 iterations, 20 seconds each intervalSeconds : 20 iterationsPerLoop : 12 knative/init-experiment \u00b6 The knative task library provides the init-experiment task. Use this task as part of the start action when experimenting with a Knative service. This task will do the following. Verify that the target Knative service resource specified in the experiment is available. The target string in the experiment must be formatted as namespace/name of the Knative service. 2 Verify that the target Knative service resource meets three conditions: Ready , ConfigurationsReady and RoutesReady . 3 Verify that revision information supplied for app versions in the experiment can be found in the Knative service. For example, the sample experiment above refers to two revisions, namely, sample-app-v1 and sample-app-v2 . The init-experiment task will inspect the status.traffic field of the target Knative service to verify that the revisions are found. Add the namespace variable to the spec.versionInfo field in the experiment. The value of this variable is the namespace of the target Knative service. Add weightObjRef clause within the spec.versionInfo field in the experiment. spec.versionInfo before and after init-experiment is executed Before 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : promote value : baseline candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candidate After 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : promote value : base - name : namespace value : default weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candid - name : namespace value : default weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent common/exec \u00b6 The common task library provides the exec task. Use this task to execute shell commands, in particular, the kubectl , helm and kustomize commands. Use the exec task as part of the finish action to promote the winning version at the end of an experiment. Use it as part of the start action to set up resources required for the experiment. kubectl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 spec : strategy : actions : start : # when using common/exec in a start action, always set disableInterpolation to true - task : common/exec # create a K8s resource with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/my/favourite/resource.yaml disableInterpolation : true finish : - task : common/exec # promote the winning version with : cmd : kubectl args : - \"apply\" - \"-f\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" Helm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 spec : strategy : actions : start : # when using common/exec in a start action, always set disableInterpolation to true - task : common/exec # install a helm chart with : cmd : /bin/sh args : - \"-c\" - | helm upgrade --install --repo https://raw.githubusercontent.com/my/favorite/helm-repo app --namespace=iter8-system app disableInterpolation : true finish : - task : common/exec with : cmd : helm args : - \"upgrade\" - \"--install\" - \"--repo\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/helm-repo\" # repo url - \"sample-app\" # release name - \"--namespace=iter8-system\" # release namespace - \"sample-app\" # chart name - \"--values=https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/{{ .promote }}-values.yaml\" # placeholder is substituted dynamically Kustomize 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 spec : strategy : actions : start : # when using common/exec in a start action, always set disableInterpolation to true - task : common/exec # create kubernetes resources with : cmd : /bin/sh args : - \"-c\" - | kustomize build github.com/my/favorite/kustomize/folder?ref=master | kubectl apply -f - disableInterpolation : true finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version using kustomize with : cmd : /bin/sh args : - \"-c\" - | kustomize build github.com/iter8-tools/iter8/samples/knative/canaryfixedsplit/{{ .name }}?ref=master | kubectl apply -f - Placeholder substitution in task inputs \u00b6 Inputs to tasks can contain placeholders, or template variables, which will be dynamically substituted when the task is executed by Iter8. For example, in the sample experiment above, one input is: \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" In this case, the placeholder is {{ .promote }} . Placeholder substitution in task inputs works as follows. Iter8 will find the version recommended for promotion. This information is stored in the status.versionRecommendedForPromotion field of the experiment. The version recommended for promotion is the winner , if a winner has been found in the experiment. Otherwise, it is the baseline version supplied in the spec.versionInfo field of the experiment. If the placeholder is {{ .name }} , Iter8 will substitute it with the name of the version recommended for promotion. Else, if it is any other variable, Iter8 will substitute it with the value of the corresponding variable for the version recommended for promotion. Variable values are specified in the variables field of the version detail. Note that variable values could have been supplied by the creator of the experiment, or by other tasks such as init-experiment that may already have been executed by Iter8 as part of the experiment. Placeholder substitution Example 1 Consider the sample experiment above. Suppose the winner of this experiment was candidate . Then: The version recommended for promotion is candidate . The placeholder in the argument to the exec task of the finish action is {{ .promote }} . The value of the placeholder for the version recommended for promotion is candid . The command executed by the exec task is then kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candid.yaml . Placeholder substitution Example 2 Consider the sample experiment above. Suppose the winner of this experiment was current . Then: The version recommended for promotion is current . The placeholder in the argument of the exec task of the finish action is {{ .promote }} . The value of the placeholder for the version recommended for promotion is base . The command executed by the exec task is then kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/base.yaml . Placeholder substitution Example 3 Consider the sample experiment above. Suppose the experiment did not yield a winner . Then: The version recommended for promotion is current . The placeholder in the argument of the exec task of the finish action is {{ .promote }} . The value of the placeholder for the version recommended for promotion is base . The command executed by the exec task is then kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/base.yaml . Disable Interpolation (always do this in a start action) \u00b6 By default, the common/exec task will attempt to find the version recommended for promotion, and use its values to substitute placeholders in the inputs to the task. However, this behavior will lead to task failure since version recommended for promotion will be generally undefined at this stage of the experiment. To use the common/exec task as part of an experiment start action, set disableInterpolation to true as illustrated in the kubectl/Helm/Kustomize samples above. Error handling in tasks \u00b6 When a task exits with an error, it will result in the failure of the experiment to which it belongs. Target naming conventions \u00b6 Knative When experimenting with a single Knative service, the convention is to use the fully qualified name (namespace/name) of the Knative service as the target string. In the sample experiment above, the app under experimentation is the Knative service named sample-app under the default namespace. Hence, the target string is default/sample-app . A/B/n experiments involve more than one candidate. Their description is coming soon. \u21a9 The init-experiment task will repeatedly attempt to find the target Knative service resource in the cluster over a period of 180 seconds. If it cannot find the service at the end of this period, it will exit with an error. \u21a9 The init-experiment task will repeatedly attempt to verify that the conditions are met over a period of 180 seconds. If it finds that the conditions are not met at the end of this period, it will exit with an error. \u21a9","title":"Iter8 API specification"},{"location":"reference/apispec/#iter8-api-specification","text":"Abstract The Iter8 API provides two Kubernetes custom resources to automate metrics and AI-driven experiments, progressive delivery, and rollout of Kubernetes and OpenShift apps. The Experiment resource provides expressive controls required by application developers and service operators who wish to automate new releases of their apps in a robust, principled and metrics-driven manner. These controls encompass testing, deployment, traffic shaping, and version promotion functions and can be flexibly composed to automate diverse use-cases . The Metric resource encapsulates the REST query that is used by Iter8 for retrieving a metric value from the metrics provider. Metrics are referenced in experiments. API Version This document describes version v2alpha2 of the Iter8 API.","title":"Iter8 API Specification"},{"location":"reference/apispec/#resources","text":"","title":"Resources"},{"location":"reference/apispec/#experiment","text":"Sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a canary test testingPattern : Canary actions : start : # run a sequence of tasks at the start of the experiment - task : knative/init-experiment finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : kubectl args : - \"apply\" - \"-f\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : current variables : # variables are used when querying metrics and when interpolating task inputs - name : revision value : sample-app-v1 - name : promote value : baseline candidates : - name : candidate variables : # variables are used when querying metrics and when interpolating task inputs - name : revision value : sample-app-v2 - name : promote value : candidate","title":"Experiment"},{"location":"reference/apispec/#metadata","text":"Standard Kubernetes meta.v1/ObjectMeta resource.","title":"Metadata"},{"location":"reference/apispec/#spec","text":"Field name Field type Description Required target string Identifies the app under experimentation and determines which experiments can run concurrently. Experiments that have the same target value will not be scheduled concurrently but will be run sequentially in the order of their creation timestamps. Experiments whose target values differ from each other can be scheduled by Iter8 concurrently. It is good practice to follow target naming conventions . Yes strategy Strategy The experimentation strategy which specifies how app versions are tested, how traffic is shifted during experiment, and what tasks are executed at the start and end of the experiment. Yes criteria Criteria Criteria used for evaluating versions. This section includes service-level objectives (SLOs) and indicators (SLIs). No duration Duration Duration of the experiment. No versionInfo VersionInfo Versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates. No","title":"Spec"},{"location":"reference/apispec/#status","text":"Field name Field type Description Required conditions [] ExperimentCondition A set of conditions that express progress of an experiment. No initTime metav1.Time The time the experiment is created. No startTime metav1.Time The time when the first iteration of experiment begins No lastUpdateTime metav1.Time The time when the status was most recently updated. No stage string Indicator of the progress of an experiment. The stage is Waiting before an experiment executes its start action, Initializing while running the start action, Running while the experiment has begun its first iteration and is progressing, Finishing while any finish action is running and Completed when the experiment terminates. No completedIterations int32 Number of completed iterations of the experiment. This is undefined until the experiment reaches the Running stage. No currentWeightDistribution [] WeightData Currently observed split of traffic between versions. Expressed as percentage. No analysis Analysis Result of latest query to the Iter8 analytics service. No versionRecommendedForPromotion string The version recommended for promotion. This field is initially populated by Iter8 as the baseline version and continuously updated during the course of the experiment to match the winner. The value of this field is typically used by finish actions to promote a version at the end of an experiment. No metrics [] MetricInfo A list of metrics referenced in the criteria section of this experiment. No message string Human readable message. No","title":"Status"},{"location":"reference/apispec/#metric","text":"Metrics are referenced within the criteria field of the experiment spec. Metrics usage within experiments is described here . Sample metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[$elapsedTime])) or on() vector(0) description : Number of requests type : counter provider : prometheus jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query","title":"Metric"},{"location":"reference/apispec/#metadata_1","text":"Standard Kubernetes meta.v1/ObjectMeta resource.","title":"Metadata"},{"location":"reference/apispec/#spec_1","text":"Field name Field type Description Required description string Human readable description. This field is meant for informational purposes. No units string Units of measurement. This field is meant for informational purposes. No provider string Type of the metrics provider. This field is meant for informational purposes. No params [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP query parameters used by Iter8 when querying the metrics provider. Each name represents a parameter name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. No body string String used to construct the JSON body of the HTTP request. Body may be templated, in which Iter8 will attempt to substitute placeholders in the template at query time using version information. No type string Metric type. Valid values are Counter and Gauge . Default value = Gauge . A Counter metric is one whose value never decreases over time. A Gauge metric is one whose value may increase or decrease over time. No method string HTTP method (verb) used in the HTTP request. Valid values are GET and POST . Default value = GET . No authType string Identifies the type of authentication used in the HTTP request. Valid values are Basic , Bearer and APIKey which correspond to HTTP authentication with these respective methods. No sampleSize string Reference to a metric that represents the number of data points over which the value of this metric is computed. This field applies only to Gauge metrics. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace of the referring metric. No secret string Reference to a secret that contains information used for authenticating with the metrics provider. In particular, Iter8 uses data in this secret to substitute placeholders in the HTTP headers and URL while querying the provider. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace where Iter8 is installed (which is iter8-system by default). No headerTemplates [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP request headers used by Iter8 when querying the metrics provider. Each name represents a header field name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. Placeholder substitution is attempted only if authType and secret fields are present. No jqExpression string The jq expression used by Iter8 to extract the metric value from the JSON response returned by the provider. Yes urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. Yes","title":"Spec"},{"location":"reference/apispec/#experiment-field-types","text":"","title":"Experiment field types"},{"location":"reference/apispec/#strategy","text":"Field name Field type Description Required testingPattern string Determines the logic used to evaluate the app versions and determine the winner of the experiment. Iter8 supports two testing patterns, namely, Canary and Conformance . Yes deploymentPattern string Determines if and how traffic is shifted during an experiment. This field is relevant only for experiments using the Canary testing pattern. Iter8 supports two deployment patterns, namely, Progressive and FixedSplit . No actions map[string][] TaskSpec An action is a sequence of tasks that can be executed by Iter8. spec.strategy.actions can be used to specify start and finish actions that will be run at the start and end of an experiment respectively. No","title":"Strategy"},{"location":"reference/apispec/#taskspec","text":"Specification of a task that will be executed as part of experiment actions. Task implementations are organized into libraries as documented here . Field name Field type Description Required task string Name of the task. Task names express both the library and the task within the library in the format 'library/task' . Yes with map[string] apiextensionsv1.JSON Inputs to the task. No","title":"TaskSpec"},{"location":"reference/apispec/#criteria","text":"Field name Field type Description Required requestCount string Reference to the metric used to count the number of requests sent to app versions. No objectives Objective [] A list of metrics along with acceptable upper limits, lower limits, or both upper and lower limits for them. Iter8 will verify if app versions satisfy these objectives. No indicators string[] A list of metric references. Iter8 will collect and report the values of these metrics in addition to those referenced in the objectives section. No Note: References to metric resource objects within experiment criteria should be in the namespace/name format or in the name format. If the name format is used (i.e., if only the name of the metric is specified), then Iter8 searches for the metric in the namespace of the experiment resource. If Iter8 cannot find the metric, then the reference is considered invalid and the experiment will terminate in a failure.","title":"Criteria"},{"location":"reference/apispec/#objective","text":"Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes upperLimit Quantity Upper limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be below the limit. No lowerLimit Quantity Lower limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be above the limit. No","title":"Objective"},{"location":"reference/apispec/#duration","text":"The duration of the experiment. Field name Field type Description Required intervalSeconds int32 Duration of a single iteration of the experiment in seconds. Default value = 20 seconds. No maxIterations int32 Maximum number of iterations in the experiment. In case of failure, the experiment may be terminated earlier. Default value = 15. No","title":"Duration"},{"location":"reference/apispec/#versioninfo","text":"spec.versionInfo describes the app versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates . Field name Field type Description Required baseline VersionDetail Details of the current or baseline version. Yes candidates [] VersionDetail Details of the candidate version or versions, if any. No Number of versions Conformance experiments involve only a single version (baseline). Hence, in Conformance experiments, the candidates field of versionInfo must be omitted. A Canary experiment involves two versions, baseline and candidate . Hence, in Canary experiments, the candidates field must be a list of length one and must contain a single versionDetail object. 1 Auto-creation of VersionInfo Iter8 ships with helper tasks that can inspect an experiment resource with no or partially specified spec.versionInfo , automatically generate the remaining portion of spec.versionInfo and update the experiment with this information. See the init-experiment task in the knative task library for an example.","title":"VersionInfo"},{"location":"reference/apispec/#versiondetail","text":"Field name Field type Description Required name string Name of the version. Yes variables [] NamedValue Variables are name-value pairs associated with a version. Metrics and tasks within experiment specs can contain strings with placeholders. Iter8 uses variables to substitute placeholders in these strings. No weightObjRef corev1.ObjectReference Reference to a Kubernetes resource and a field-path within the resource. Iter8 uses weightObjRef to get or set weight (traffic percentage) for the version. No","title":"VersionDetail"},{"location":"reference/apispec/#metricinfo","text":"Field name Field type Description Required name string Identifies an Iter8 metric using the namespace/name or name format . Yes metric [] Metric Iter8 metric object referenced by name. No","title":"MetricInfo"},{"location":"reference/apispec/#experimentcondition","text":"Conditions express aspects of the progress of an experiment. The Completed condition indicates whether or not an experiment has completed. The Failed condition indicates whether or not an experiment completed successfully or in failure. The TargetAcquired condition indicates that an experiment has acquired the target and is now scheduled to run. At any point in time, for any given target, Iter8 ensures that at most one experiment has the conditions TargetAcquired set to True and Completed set to False . Field name Field type Description Required type string Type of condition. Valid types are TargetAcquired , Completed and Failed . Yes status corev1.ConditionStatus status of condition, one of True , False , or Unknown . Yes lastTransitionTime metav1.Time The last time any field in the condition was changed. No reason string A reason for the change in value. No message string Human readable decription. No","title":"ExperimentCondition"},{"location":"reference/apispec/#analysis","text":"Field name Field type Description Required aggregatedMetrics AggregatedMetricsAnalysis Most recently observed metric values for all metrics referenced in the experiment criteria. No winnerAssessment WinnerAssessmentAnalysis Information about the winner of the experiment. No versionAssessments VersionAssessmentAnalysis For each version, a summary analysis identifying whether or not the version is satisfying the experiment criteria. No weights WeightsAnalysis Recommended weight distribution to be applied before the next iteration of the experiment. No","title":"Analysis"},{"location":"reference/apispec/#versionassessmentanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string][]bool map of version name to a list of boolean values, one for each objective specified in the experiment criteria, indicating whether not the objective is satisified. No","title":"VersionAssessmentAnalysis"},{"location":"reference/apispec/#aggregatedmetricsanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string] AggregatedMetricsData Map from metric name to most recent data (from all versions) for the metric. Yes","title":"AggregatedMetricsAnalysis"},{"location":"reference/apispec/#aggregatedmetricsdata","text":"Field name Field type Description Required max Quantity The maximum value observed for this metric accross all versions. Yes min Quantity The minimum value observed for this metric accross all versions. Yes data map[string] AggregatedMetricsVersionData A map from version name to the most recent aggregated metrics data for that version. No","title":"AggregatedMetricsData"},{"location":"reference/apispec/#aggregatedmetricsversiondata","text":"Field name Field type Description Required max Quantity The maximum value observed for this metric for this version over all observations. No min Quantity The minimum value observed for this metric for this version over all observations. No value Quantity The value. No sampleSize int32 The number of requests observed by this version. No","title":"AggregatedMetricsVersionData"},{"location":"reference/apispec/#winnerassessmentanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data WinnerAssessmentData Details on whether or not a winner has been identified and which version if so. No","title":"WinnerAssessmentAnalysis"},{"location":"reference/apispec/#winnerassessmentdata","text":"Field name Field type Description Required winnerFound bool Whether or not a winner has been identified. Yes winner string The name of the identified winner, if one has been found. No","title":"WinnerAssessmentData"},{"location":"reference/apispec/#weightanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data [] WeightData List of version name/value pairs representing a recommended weight for each version No","title":"WeightAnalysis"},{"location":"reference/apispec/#weightdata","text":"Field name Field type Description Required name string Version name Yes value int32 Percentage of traffic being sent to the version. Yes","title":"WeightData"},{"location":"reference/apispec/#common-field-types","text":"","title":"Common field types"},{"location":"reference/apispec/#namedvalue","text":"Field name Field type Description Required name string Name of a variable. Yes value string Value of a variable. Yes","title":"NamedValue"},{"location":"reference/apispec/#tasks","text":"Tasks are an extension mechanism for enhancing the behavior of Iter8 experiments and can be specified within the spec.strategy.actions field of the experiment.","title":"Tasks"},{"location":"reference/apispec/#task-implementations","text":"Iter8 currently implements two tasks that help in setting up and finishing up experiments. These tasks are organized into the knative and common task libraries. Sample experiment with start and finish actions with tasks 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # `sample-app` Knative service in `default` namespace is the target of this experiment target : default/sample-app # information about app versions participating in this experiment versionInfo : # every experiment has a baseline version # we will name it `current` baseline : name : current variables : # `revision` variable is used for fetching metrics from Prometheus - name : revision value : sample-app-v1 # `promote` variable is used by the finish task - name : promote value : base # candidate version(s) of the app # there is a single candidate in this experiment # we will name it `candidate` candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candid criteria : objectives : # mean latency should be under 50 milliseconds - metric : iter8-knative/mean-latency upperLimit : 50 # 95th percentile latency should be under 100 milliseconds - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 # error rate should be under 1% - metric : iter8-knative/error-rate upperLimit : \"0.01\" indicators : # report values for the following metrics in addition those in spec.criteria.objectives - 99th-percentile-tail-latency - 90th-percentile-tail-latency - 75th-percentile-tail-latency strategy : # canary testing => candidate `wins` if it satisfies objectives testingPattern : Canary # progressively shift traffic to candidate, assuming it satisfies objectives deploymentPattern : Progressive actions : # run tasks under the `start` action at the start of an experiment start : # the following task verifies that the `sample-app` Knative service in the `default` namespace is available and ready # it then updates the experiment resource with information needed to shift traffic between app versions - task : knative/init-experiment # run tasks under the `finish` action at the end of an experiment finish : # promote an app version # `https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml` will be applied if candidate satisfies objectives # `https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml` will be applied if candidate fails to satisfy objectives - task : common/exec # promote the winning version with : cmd : kubectl args : - \"apply\" - \"-f\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" duration : # 12 iterations, 20 seconds each intervalSeconds : 20 iterationsPerLoop : 12","title":"Task implementations"},{"location":"reference/apispec/#knativeinit-experiment","text":"The knative task library provides the init-experiment task. Use this task as part of the start action when experimenting with a Knative service. This task will do the following. Verify that the target Knative service resource specified in the experiment is available. The target string in the experiment must be formatted as namespace/name of the Knative service. 2 Verify that the target Knative service resource meets three conditions: Ready , ConfigurationsReady and RoutesReady . 3 Verify that revision information supplied for app versions in the experiment can be found in the Knative service. For example, the sample experiment above refers to two revisions, namely, sample-app-v1 and sample-app-v2 . The init-experiment task will inspect the status.traffic field of the target Knative service to verify that the revisions are found. Add the namespace variable to the spec.versionInfo field in the experiment. The value of this variable is the namespace of the target Knative service. Add weightObjRef clause within the spec.versionInfo field in the experiment. spec.versionInfo before and after init-experiment is executed Before 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : promote value : baseline candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candidate After 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : promote value : base - name : namespace value : default weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candid - name : namespace value : default weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent","title":"knative/init-experiment"},{"location":"reference/apispec/#commonexec","text":"The common task library provides the exec task. Use this task to execute shell commands, in particular, the kubectl , helm and kustomize commands. Use the exec task as part of the finish action to promote the winning version at the end of an experiment. Use it as part of the start action to set up resources required for the experiment. kubectl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 spec : strategy : actions : start : # when using common/exec in a start action, always set disableInterpolation to true - task : common/exec # create a K8s resource with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/my/favourite/resource.yaml disableInterpolation : true finish : - task : common/exec # promote the winning version with : cmd : kubectl args : - \"apply\" - \"-f\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" Helm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 spec : strategy : actions : start : # when using common/exec in a start action, always set disableInterpolation to true - task : common/exec # install a helm chart with : cmd : /bin/sh args : - \"-c\" - | helm upgrade --install --repo https://raw.githubusercontent.com/my/favorite/helm-repo app --namespace=iter8-system app disableInterpolation : true finish : - task : common/exec with : cmd : helm args : - \"upgrade\" - \"--install\" - \"--repo\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/helm-repo\" # repo url - \"sample-app\" # release name - \"--namespace=iter8-system\" # release namespace - \"sample-app\" # chart name - \"--values=https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/{{ .promote }}-values.yaml\" # placeholder is substituted dynamically Kustomize 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 spec : strategy : actions : start : # when using common/exec in a start action, always set disableInterpolation to true - task : common/exec # create kubernetes resources with : cmd : /bin/sh args : - \"-c\" - | kustomize build github.com/my/favorite/kustomize/folder?ref=master | kubectl apply -f - disableInterpolation : true finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version using kustomize with : cmd : /bin/sh args : - \"-c\" - | kustomize build github.com/iter8-tools/iter8/samples/knative/canaryfixedsplit/{{ .name }}?ref=master | kubectl apply -f -","title":"common/exec"},{"location":"reference/apispec/#placeholder-substitution-in-task-inputs","text":"Inputs to tasks can contain placeholders, or template variables, which will be dynamically substituted when the task is executed by Iter8. For example, in the sample experiment above, one input is: \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml\" In this case, the placeholder is {{ .promote }} . Placeholder substitution in task inputs works as follows. Iter8 will find the version recommended for promotion. This information is stored in the status.versionRecommendedForPromotion field of the experiment. The version recommended for promotion is the winner , if a winner has been found in the experiment. Otherwise, it is the baseline version supplied in the spec.versionInfo field of the experiment. If the placeholder is {{ .name }} , Iter8 will substitute it with the name of the version recommended for promotion. Else, if it is any other variable, Iter8 will substitute it with the value of the corresponding variable for the version recommended for promotion. Variable values are specified in the variables field of the version detail. Note that variable values could have been supplied by the creator of the experiment, or by other tasks such as init-experiment that may already have been executed by Iter8 as part of the experiment. Placeholder substitution Example 1 Consider the sample experiment above. Suppose the winner of this experiment was candidate . Then: The version recommended for promotion is candidate . The placeholder in the argument to the exec task of the finish action is {{ .promote }} . The value of the placeholder for the version recommended for promotion is candid . The command executed by the exec task is then kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candid.yaml . Placeholder substitution Example 2 Consider the sample experiment above. Suppose the winner of this experiment was current . Then: The version recommended for promotion is current . The placeholder in the argument of the exec task of the finish action is {{ .promote }} . The value of the placeholder for the version recommended for promotion is base . The command executed by the exec task is then kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/base.yaml . Placeholder substitution Example 3 Consider the sample experiment above. Suppose the experiment did not yield a winner . Then: The version recommended for promotion is current . The placeholder in the argument of the exec task of the finish action is {{ .promote }} . The value of the placeholder for the version recommended for promotion is base . The command executed by the exec task is then kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/base.yaml .","title":"Placeholder substitution in task inputs"},{"location":"reference/apispec/#disable-interpolation-always-do-this-in-a-start-action","text":"By default, the common/exec task will attempt to find the version recommended for promotion, and use its values to substitute placeholders in the inputs to the task. However, this behavior will lead to task failure since version recommended for promotion will be generally undefined at this stage of the experiment. To use the common/exec task as part of an experiment start action, set disableInterpolation to true as illustrated in the kubectl/Helm/Kustomize samples above.","title":"Disable Interpolation (always do this in a start action)"},{"location":"reference/apispec/#error-handling-in-tasks","text":"When a task exits with an error, it will result in the failure of the experiment to which it belongs.","title":"Error handling in tasks"},{"location":"reference/apispec/#target-naming-conventions","text":"Knative When experimenting with a single Knative service, the convention is to use the fully qualified name (namespace/name) of the Knative service as the target string. In the sample experiment above, the app under experimentation is the Knative service named sample-app under the default namespace. Hence, the target string is default/sample-app . A/B/n experiments involve more than one candidate. Their description is coming soon. \u21a9 The init-experiment task will repeatedly attempt to find the target Knative service resource in the cluster over a period of 180 seconds. If it cannot find the service at the end of this period, it will exit with an error. \u21a9 The init-experiment task will repeatedly attempt to verify that the conditions are met over a period of 180 seconds. If it finds that the conditions are not met at the end of this period, it will exit with an error. \u21a9","title":"Target naming conventions"},{"location":"tutorials/knative/annotations/","text":"Useful Knative Annotations \u00b6 This document discusses a few annotations that tune the behavior of Knative and are useful to incorporate in Knative services participating in Iter8 experiments. The first of these enables experimentation with cluster-local services, while the last two avoid cold start issues. Cluster-local (backend) services \u00b6 Cluster-local or backend services are private services which are only available inside the cluster. Iter8 experiments with cluster-local Knative services are similar to any other Iter8 experiments. The following example from the Knative documentation on cluster-local services shows how you can label a Knative service as cluster-local. kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local Scale boundaries \u00b6 You can configure upper and lower bounds to control Knative's autoscaling behavior. The lower bound can be used to ensure that at least one replica is available for every version (Knative revision), and cold start issues do not interfere with version assessments during an experiment. The following example from the Knative documentation on configuring scale boundaries shows how you can control the minimum number of replicas each revision should have. 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/minScale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Pod retention period \u00b6 The scale-to-zero-pod-retention-period annotation can be used to specify the minimum amount of time that the last pod will remain active after the Knative Autoscaler decides to scale pods to zero. Like the minScale annotation, this annotation is also useful for avoiding cold start issues during an experiment. For more information, see the Knative documentation on configuring scale to zero . The following example from the Iter8 traffic segmentation tutorial shows how you can use this annotation in a service. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v1 namespace : default spec : template : metadata : name : sample-app-v1-blue annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\"","title":"Useful Knative annotations"},{"location":"tutorials/knative/annotations/#useful-knative-annotations","text":"This document discusses a few annotations that tune the behavior of Knative and are useful to incorporate in Knative services participating in Iter8 experiments. The first of these enables experimentation with cluster-local services, while the last two avoid cold start issues.","title":"Useful Knative Annotations"},{"location":"tutorials/knative/annotations/#cluster-local-backend-services","text":"Cluster-local or backend services are private services which are only available inside the cluster. Iter8 experiments with cluster-local Knative services are similar to any other Iter8 experiments. The following example from the Knative documentation on cluster-local services shows how you can label a Knative service as cluster-local. kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local","title":"Cluster-local (backend) services"},{"location":"tutorials/knative/annotations/#scale-boundaries","text":"You can configure upper and lower bounds to control Knative's autoscaling behavior. The lower bound can be used to ensure that at least one replica is available for every version (Knative revision), and cold start issues do not interfere with version assessments during an experiment. The following example from the Knative documentation on configuring scale boundaries shows how you can control the minimum number of replicas each revision should have. 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/minScale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go","title":"Scale boundaries"},{"location":"tutorials/knative/annotations/#pod-retention-period","text":"The scale-to-zero-pod-retention-period annotation can be used to specify the minimum amount of time that the last pod will remain active after the Knative Autoscaler decides to scale pods to zero. Like the minScale annotation, this annotation is also useful for avoiding cold start issues during an experiment. For more information, see the Knative documentation on configuring scale to zero . The following example from the Iter8 traffic segmentation tutorial shows how you can use this annotation in a service. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v1 namespace : default spec : template : metadata : name : sample-app-v1-blue annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\"","title":"Pod retention period"},{"location":"tutorials/knative/canary-fixedsplit/","text":"Fixed Split Canary Release \u00b6 An experiment with Canary testing, FixedSplit deployment, and Kustomize based version promotion . You will create the following resources in this tutorial. A Knative app (service) with two versions (revisions). A Fortio-based traffic generator that simulates user requests. An Iter8 experiment that: verifies that candidate satisfies mean latency, 95 th percentile tail latency, and error rate objectives maintains a 75-25 split of traffic between baseline and candidate throughout the experiment eventually replaces baseline with candidate using Kustomize Before you begin, you will need... Kubernetes cluster: Ensure that you have a Kubernetes cluster with Iter8, Knative, Prometheus add-on, and Iter8's sample metrics for Knative installed. You can do this by following Steps 1, 2, 3 and 6 of the quick start tutorial for Knative . Cleanup: If you ran an Iter8 tutorial earlier, run the associated cleanup step. ITER8 environment variable: Ensure that ITER8 environment variable is set to the root directory of your cloned Iter8 repo. See Step 2 of the quick start tutorial for Knative for example. Kustomize v3+ and iter8ctl : This tutorial uses Kustomize v3+ and iter8ctl . 1. Create app versions \u00b6 kustomize build $ITER8 /samples/knative/canaryfixedsplit/baseline | kubectl apply -f - kubectl wait ksvc/sample-app --for condition = Ready --timeout = 120s kustomize build $ITER8 /samples/knative/canaryfixedsplit/experimentalservice | kubectl apply -f - Look inside output of kustomize build $ITER8/.../baseline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION Look inside output of kustomize build $ITER8/.../experimentalservice 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # This Knative service will be used for the Iter8 experiment with traffic split between baseline and candidate revision # Traffic is split 75/25 between the baseline and candidate apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app # name of the app namespace : default # namespace of the app spec : template : metadata : name : sample-app-v2 spec : containers : # Docker image used by second revision - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : # 75% goes to sample-app-v1 and 25% to sample-app-v2 - tag : current revisionName : sample-app-v1 percent : 75 - tag : candidate latestRevision : true percent : 25 2. Generate requests \u00b6 kubectl wait --for = condition = Ready ksvc/sample-app URL_VALUE = $( kubectl get ksvc sample-app -o json | jq .status.address.url ) sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/knative/canaryfixedsplit/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-t\" , \"6000s\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 3. Create Iter8 experiment \u00b6 kubectl apply -f $ITER8 /samples/knative/canaryfixedsplit/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : canary-fixedsplit spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : FixedSplit actions : start : # run the following sequence of tasks at the start of the experiment - task : knative/init-experiment finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version using kustomize with : cmd : /bin/sh args : - \"-c\" - | kustomize build github.com/iter8-tools/iter8/samples/knative/canaryfixedsplit/{{ .name }}?ref=master \\ | kubectl apply -f - criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about versions used in this experiment baseline : name : baseline variables : - name : revision value : sample-app-v1 candidates : - name : candidate variables : - name : revision value : sample-app-v2 4. Observe experiment \u00b6 Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis Periodically describe the experiment. while clear ; do kubectl get experiment canary-fixedsplit -o yaml | iter8ctl describe -f - sleep 4 done The output will look similar to the iter8ctl output in the quick start instructions. As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by both versions. The candidate is identified as the winner and is recommended for promotion. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment canary-fixedsplit --watch The output will look similar to the kubectl get experiment output in the quick start instructions. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Traffic split kubectl get ksvc sample-app -o json --watch | jq .status.traffic The output will look similar to the kubectl get ksvc output in the quick start instructions. As the experiment progresses, you should see traffic remain unchanged. When the experiment completes, and the candidate, sample-app-v2 , is identified as the winner, all of the traffic will all be sent to it. Understanding what happened You created a Knative service with two revisions, sample-app-v1 ( baseline ) and sample-app-v2 ( candidate ) using Kustomize. You generated requests for the Knative service using a Fortio job. At the start of the experiment, 75% of the requests are sent to baseline and 25% to candidate . You created an Iter8 Canary experiment with FixedSplit deployment pattern. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, verified that candidate satisfied all the objectives specified in the experiment, identified candidate as the winner , and eventually promoted the candidate using kustomize build ... | kubectl apply -f - commands. Note: Had candidate failed to satisfy objectives , then baseline would have been promoted. Note: There was no traffic shifting during experiment iterations since this used a FixedSplit deployment pattern. 5. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/canaryfixedsplit/fortio.yaml kubectl delete -f $ITER8 /samples/knative/canaryfixedsplit/experiment.yaml kustomize build $ITER8 /samples/knative/canaryfixedsplit/experimentalservice | kubectl delete -f -","title":"Fixed split canary release"},{"location":"tutorials/knative/canary-fixedsplit/#fixed-split-canary-release","text":"An experiment with Canary testing, FixedSplit deployment, and Kustomize based version promotion . You will create the following resources in this tutorial. A Knative app (service) with two versions (revisions). A Fortio-based traffic generator that simulates user requests. An Iter8 experiment that: verifies that candidate satisfies mean latency, 95 th percentile tail latency, and error rate objectives maintains a 75-25 split of traffic between baseline and candidate throughout the experiment eventually replaces baseline with candidate using Kustomize Before you begin, you will need... Kubernetes cluster: Ensure that you have a Kubernetes cluster with Iter8, Knative, Prometheus add-on, and Iter8's sample metrics for Knative installed. You can do this by following Steps 1, 2, 3 and 6 of the quick start tutorial for Knative . Cleanup: If you ran an Iter8 tutorial earlier, run the associated cleanup step. ITER8 environment variable: Ensure that ITER8 environment variable is set to the root directory of your cloned Iter8 repo. See Step 2 of the quick start tutorial for Knative for example. Kustomize v3+ and iter8ctl : This tutorial uses Kustomize v3+ and iter8ctl .","title":"Fixed Split Canary Release"},{"location":"tutorials/knative/canary-fixedsplit/#1-create-app-versions","text":"kustomize build $ITER8 /samples/knative/canaryfixedsplit/baseline | kubectl apply -f - kubectl wait ksvc/sample-app --for condition = Ready --timeout = 120s kustomize build $ITER8 /samples/knative/canaryfixedsplit/experimentalservice | kubectl apply -f - Look inside output of kustomize build $ITER8/.../baseline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION Look inside output of kustomize build $ITER8/.../experimentalservice 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # This Knative service will be used for the Iter8 experiment with traffic split between baseline and candidate revision # Traffic is split 75/25 between the baseline and candidate apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app # name of the app namespace : default # namespace of the app spec : template : metadata : name : sample-app-v2 spec : containers : # Docker image used by second revision - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : # 75% goes to sample-app-v1 and 25% to sample-app-v2 - tag : current revisionName : sample-app-v1 percent : 75 - tag : candidate latestRevision : true percent : 25","title":"1. Create app versions"},{"location":"tutorials/knative/canary-fixedsplit/#2-generate-requests","text":"kubectl wait --for = condition = Ready ksvc/sample-app URL_VALUE = $( kubectl get ksvc sample-app -o json | jq .status.address.url ) sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/knative/canaryfixedsplit/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-t\" , \"6000s\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"2. Generate requests"},{"location":"tutorials/knative/canary-fixedsplit/#3-create-iter8-experiment","text":"kubectl apply -f $ITER8 /samples/knative/canaryfixedsplit/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : canary-fixedsplit spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : FixedSplit actions : start : # run the following sequence of tasks at the start of the experiment - task : knative/init-experiment finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version using kustomize with : cmd : /bin/sh args : - \"-c\" - | kustomize build github.com/iter8-tools/iter8/samples/knative/canaryfixedsplit/{{ .name }}?ref=master \\ | kubectl apply -f - criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about versions used in this experiment baseline : name : baseline variables : - name : revision value : sample-app-v1 candidates : - name : candidate variables : - name : revision value : sample-app-v2","title":"3. Create Iter8 experiment"},{"location":"tutorials/knative/canary-fixedsplit/#4-observe-experiment","text":"Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis Periodically describe the experiment. while clear ; do kubectl get experiment canary-fixedsplit -o yaml | iter8ctl describe -f - sleep 4 done The output will look similar to the iter8ctl output in the quick start instructions. As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by both versions. The candidate is identified as the winner and is recommended for promotion. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment canary-fixedsplit --watch The output will look similar to the kubectl get experiment output in the quick start instructions. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Traffic split kubectl get ksvc sample-app -o json --watch | jq .status.traffic The output will look similar to the kubectl get ksvc output in the quick start instructions. As the experiment progresses, you should see traffic remain unchanged. When the experiment completes, and the candidate, sample-app-v2 , is identified as the winner, all of the traffic will all be sent to it. Understanding what happened You created a Knative service with two revisions, sample-app-v1 ( baseline ) and sample-app-v2 ( candidate ) using Kustomize. You generated requests for the Knative service using a Fortio job. At the start of the experiment, 75% of the requests are sent to baseline and 25% to candidate . You created an Iter8 Canary experiment with FixedSplit deployment pattern. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, verified that candidate satisfied all the objectives specified in the experiment, identified candidate as the winner , and eventually promoted the candidate using kustomize build ... | kubectl apply -f - commands. Note: Had candidate failed to satisfy objectives , then baseline would have been promoted. Note: There was no traffic shifting during experiment iterations since this used a FixedSplit deployment pattern.","title":"4. Observe experiment"},{"location":"tutorials/knative/canary-fixedsplit/#5-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/canaryfixedsplit/fortio.yaml kubectl delete -f $ITER8 /samples/knative/canaryfixedsplit/experiment.yaml kustomize build $ITER8 /samples/knative/canaryfixedsplit/experimentalservice | kubectl delete -f -","title":"5. Cleanup"},{"location":"tutorials/knative/canary-progressive/","text":"Progressive Canary Release \u00b6 An experiment with Canary testing, Progressive deployment, and Helm based version promotion . You will create the following resources in this tutorial. A Knative app (service) with two versions (revisions). A Fortio-based traffic generator that simulates user requests. An Iter8 experiment that: verifies that candidate satisfies mean latency, 95 th percentile tail latency, and error rate objectives progressively shifts traffic from baseline to candidate , subject to the limits placed by the experiment's spec.strategy.weights field eventually replaces baseline with candidate using Helm Before you begin, you will need... Kubernetes cluster: Ensure that you have a Kubernetes cluster with Iter8, Knative, Prometheus add-on, and Iter8's sample metrics for Knative installed. You can do this by following Steps 1, 2, 3 and 6 of the quick start tutorial for Knative . Cleanup: If you ran an Iter8 tutorial earlier, run the associated cleanup step. ITER8 environment variable: Ensure that ITER8 environment variable is set to the root directory of your cloned Iter8 repo. See Step 2 of the quick start tutorial for Knative for example. Helm v3 and iter8ctl : This tutorial uses Helm v3 and iter8ctl . 1. Give permissions to Iter8 to invoke helm \u00b6 Helm uses secrets to record information about releases. This tutorial uses an experiment that invokes Helm. Enable this experiment using the following RBAC. kubectl apply -f ${ ITER8 } /samples/knative/canaryprogressive/helm-rbac.yaml 2. Create app versions \u00b6 helm install --repo https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/helm-repo sample-app sample-app --namespace = default kubectl wait ksvc/sample-app --for condition = Ready --timeout = 120s helm upgrade --install --repo https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/helm-repo sample-app sample-app --values = https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/experimental-values.yaml --namespace = default Look inside values.yaml 1 2 3 4 5 # default values used for installing sample-app Helm chart # using these values will create a baseline version (revision) that gets 100% of the traffic name : \"sample-app-v1\" image : \"gcr.io/knative-samples/knative-route-demo:blue\" tVersion : \"blue\" Look inside experimental-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 # values file used for upgrading sample-app Helm chart for use in Iter8 experiment # using these values will create a candidate version (revision) # baseline still gets 100% of the traffic name : \"sample-app-v2\" image : \"gcr.io/knative-samples/knative-route-demo:green\" tVersion : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0 3. Generate requests \u00b6 kubectl wait --for = condition = Ready ksvc/sample-app URL_VALUE = $( kubectl get ksvc sample-app -o json | jq .status.address.url ) sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/knative/canaryprogressive/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-t\" , \"6000s\" , \"-qps\" , \"16\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 4. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/knative/canaryprogressive/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : canary-progressive spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive weights : # fine-tune traffic increments to candidate # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20 actions : start : # run the following sequence of tasks at the start of the experiment - task : knative/init-experiment finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version using Helm upgrade with : cmd : helm args : - \"upgrade\" - \"--install\" - \"--repo\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/helm-repo\" # repo url - \"sample-app\" # release name - \"--namespace=default\" # release namespace - \"sample-app\" # chart name - \"--values=https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/{{ .promote }}-values.yaml\" # placeholder 'promote' is dynamically substituted criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1 - name : promote value : baseline candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candidate 5. Observe experiment \u00b6 Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis Periodically describe the experiment. while clear ; do kubectl get experiment canary-progressive -o yaml | iter8ctl describe -f - sleep 4 done The output will look similar to the iter8ctl output in the quick start instructions. As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by both versions. The candidate is identified as the winner and is recommended for promotion. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment canary-progressive --watch The output will look similar to the kubectl get experiment output in the quick start instructions. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Traffic split kubectl get ksvc sample-app -o json --watch | jq .status.traffic The output will look similar to the kubectl get ksvc output in the quick start instructions. As the experiment progresses, you should see traffic progressively shift from sample-app-v1 to sample-app-v2 . When the experiment completes, all of the traffic will be sent to the winner, sample-app-v2 . Understanding what happened You created a Knative service using helm install subcommand and upgraded the service to have two revisions, sample-app-v1 ( baseline ) and sample-app-v2 ( candidate ) using helm upgrade --install subcommand. The ksvc is created in the default namespace. Likewise, the Helm release information is located in the default namespace as specified by the --namespace=default flag. You generated requests for the Knative service using a Fortio job. At the start of the experiment, 100% of the requests are sent to baseline and 0% to candidate. You created an Iter8 Canary experiment with Progressive deployment pattern. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, verified that candidate satisfied all the objectives specified in the experiment, identified candidate as the winner , progressively shifted traffic from baseline to candidate and eventually promoted the candidate using helm upgrade --install subcommand. Note: Had candidate failed to satisfy objectives , then baseline would have been promoted. Note: You limited the maximum weight (traffic %) of candidate during iterations at 75% and maximum increment allowed during a single iteration to 20% using the field spec.strategy.weights . Traffic shifts during the experiment obeyed these limits. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/canaryprogressive/experiment.yaml kubectl delete -f $ITER8 /samples/knative/canaryprogressive/fortio.yaml helm uninstall sample-app --namespace = default","title":"Progressive canary release"},{"location":"tutorials/knative/canary-progressive/#progressive-canary-release","text":"An experiment with Canary testing, Progressive deployment, and Helm based version promotion . You will create the following resources in this tutorial. A Knative app (service) with two versions (revisions). A Fortio-based traffic generator that simulates user requests. An Iter8 experiment that: verifies that candidate satisfies mean latency, 95 th percentile tail latency, and error rate objectives progressively shifts traffic from baseline to candidate , subject to the limits placed by the experiment's spec.strategy.weights field eventually replaces baseline with candidate using Helm Before you begin, you will need... Kubernetes cluster: Ensure that you have a Kubernetes cluster with Iter8, Knative, Prometheus add-on, and Iter8's sample metrics for Knative installed. You can do this by following Steps 1, 2, 3 and 6 of the quick start tutorial for Knative . Cleanup: If you ran an Iter8 tutorial earlier, run the associated cleanup step. ITER8 environment variable: Ensure that ITER8 environment variable is set to the root directory of your cloned Iter8 repo. See Step 2 of the quick start tutorial for Knative for example. Helm v3 and iter8ctl : This tutorial uses Helm v3 and iter8ctl .","title":"Progressive Canary Release"},{"location":"tutorials/knative/canary-progressive/#1-give-permissions-to-iter8-to-invoke-helm","text":"Helm uses secrets to record information about releases. This tutorial uses an experiment that invokes Helm. Enable this experiment using the following RBAC. kubectl apply -f ${ ITER8 } /samples/knative/canaryprogressive/helm-rbac.yaml","title":"1. Give permissions to Iter8 to invoke helm"},{"location":"tutorials/knative/canary-progressive/#2-create-app-versions","text":"helm install --repo https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/helm-repo sample-app sample-app --namespace = default kubectl wait ksvc/sample-app --for condition = Ready --timeout = 120s helm upgrade --install --repo https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/helm-repo sample-app sample-app --values = https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/experimental-values.yaml --namespace = default Look inside values.yaml 1 2 3 4 5 # default values used for installing sample-app Helm chart # using these values will create a baseline version (revision) that gets 100% of the traffic name : \"sample-app-v1\" image : \"gcr.io/knative-samples/knative-route-demo:blue\" tVersion : \"blue\" Look inside experimental-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 # values file used for upgrading sample-app Helm chart for use in Iter8 experiment # using these values will create a candidate version (revision) # baseline still gets 100% of the traffic name : \"sample-app-v2\" image : \"gcr.io/knative-samples/knative-route-demo:green\" tVersion : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0","title":"2. Create app versions"},{"location":"tutorials/knative/canary-progressive/#3-generate-requests","text":"kubectl wait --for = condition = Ready ksvc/sample-app URL_VALUE = $( kubectl get ksvc sample-app -o json | jq .status.address.url ) sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/knative/canaryprogressive/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-t\" , \"6000s\" , \"-qps\" , \"16\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"3. Generate requests"},{"location":"tutorials/knative/canary-progressive/#4-launch-experiment","text":"kubectl apply -f $ITER8 /samples/knative/canaryprogressive/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : canary-progressive spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive weights : # fine-tune traffic increments to candidate # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20 actions : start : # run the following sequence of tasks at the start of the experiment - task : knative/init-experiment finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version using Helm upgrade with : cmd : helm args : - \"upgrade\" - \"--install\" - \"--repo\" - \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/helm-repo\" # repo url - \"sample-app\" # release name - \"--namespace=default\" # release namespace - \"sample-app\" # chart name - \"--values=https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/canaryprogressive/{{ .promote }}-values.yaml\" # placeholder 'promote' is dynamically substituted criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1 - name : promote value : baseline candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : promote value : candidate","title":"4. Launch experiment"},{"location":"tutorials/knative/canary-progressive/#5-observe-experiment","text":"Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis Periodically describe the experiment. while clear ; do kubectl get experiment canary-progressive -o yaml | iter8ctl describe -f - sleep 4 done The output will look similar to the iter8ctl output in the quick start instructions. As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by both versions. The candidate is identified as the winner and is recommended for promotion. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment canary-progressive --watch The output will look similar to the kubectl get experiment output in the quick start instructions. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Traffic split kubectl get ksvc sample-app -o json --watch | jq .status.traffic The output will look similar to the kubectl get ksvc output in the quick start instructions. As the experiment progresses, you should see traffic progressively shift from sample-app-v1 to sample-app-v2 . When the experiment completes, all of the traffic will be sent to the winner, sample-app-v2 . Understanding what happened You created a Knative service using helm install subcommand and upgraded the service to have two revisions, sample-app-v1 ( baseline ) and sample-app-v2 ( candidate ) using helm upgrade --install subcommand. The ksvc is created in the default namespace. Likewise, the Helm release information is located in the default namespace as specified by the --namespace=default flag. You generated requests for the Knative service using a Fortio job. At the start of the experiment, 100% of the requests are sent to baseline and 0% to candidate. You created an Iter8 Canary experiment with Progressive deployment pattern. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, verified that candidate satisfied all the objectives specified in the experiment, identified candidate as the winner , progressively shifted traffic from baseline to candidate and eventually promoted the candidate using helm upgrade --install subcommand. Note: Had candidate failed to satisfy objectives , then baseline would have been promoted. Note: You limited the maximum weight (traffic %) of candidate during iterations at 75% and maximum increment allowed during a single iteration to 20% using the field spec.strategy.weights . Traffic shifts during the experiment obeyed these limits.","title":"5. Observe experiment"},{"location":"tutorials/knative/canary-progressive/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/canaryprogressive/experiment.yaml kubectl delete -f $ITER8 /samples/knative/canaryprogressive/fortio.yaml helm uninstall sample-app --namespace = default","title":"6. Cleanup"},{"location":"tutorials/knative/conformance/","text":"Conformance Testing \u00b6 An experiment with Conformance testing. You will create the following resources in this tutorial. A Knative app (service) with a single version (revision). A Fortio-based traffic generator that simulates user requests. An Iter8 experiment that verifies that baseline satisfies mean latency, 95 th percentile tail latency, and error rate objectives . Before you begin, you will need... Kubernetes cluster: Ensure that you have a Kubernetes cluster with Iter8, Knative, Prometheus add-on, and Iter8's sample metrics for Knative installed. You can do this by following Steps 1, 2, 3 and 6 of the quick start tutorial for Knative . Cleanup: If you ran an Iter8 tutorial earlier, run the associated cleanup step. ITER8 environment variable: Ensure that ITER8 environment variable is set to the root directory of your cloned Iter8 repo. See Step 2 of the quick start tutorial for Knative for example. iter8ctl : This tutorial uses iter8ctl . 1. Create app \u00b6 kubectl apply -f $ITER8 /samples/knative/conformance/baseline.yaml Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" 2. Generate requests \u00b6 kubectl wait --for = condition = Ready ksvc/sample-app URL_VALUE = $( kubectl get ksvc sample-app -o json | jq .status.address.url ) sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/knative/conformance/fortio.yaml | kubectl apply -f - Look inside fortio.yaml apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-t\" , \"6000s\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 3. Create Iter8 experiment \u00b6 kubectl apply -f $ITER8 /samples/knative/conformance/experiment.yaml Look inside experiment.yaml apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : conformance-sample spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a conformance test testingPattern : Conformance actions : start : # run the following sequence of tasks at the start of the experiment - task : knative/init-experiment criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1 4. Observe experiment \u00b6 Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis Periodically describe the experiment. while clear ; do kubectl get experiment conformance-sample -o yaml | iter8ctl describe -f - sleep 4 done The output will look similar to the iter8ctl output in the quick start instructions. As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by the version being tested. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment conformance-sample --watch The output will look similar to the kubectl get experiment output in the quick start instructions. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Understanding what happened You created a Knative service with a single revision, sample-app-v1. You generated requests for the Knative service using a Fortio job. You created an Iter8 Conformance experiment. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, and verified that baseline satisfied all the objectives specified in the experiment. 5. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/conformance/fortio.yaml kubectl delete -f $ITER8 /samples/knative/conformance/experiment.yaml kubectl delete -f $ITER8 /samples/knative/conformance/baseline.yaml","title":"Conformance testing"},{"location":"tutorials/knative/conformance/#conformance-testing","text":"An experiment with Conformance testing. You will create the following resources in this tutorial. A Knative app (service) with a single version (revision). A Fortio-based traffic generator that simulates user requests. An Iter8 experiment that verifies that baseline satisfies mean latency, 95 th percentile tail latency, and error rate objectives . Before you begin, you will need... Kubernetes cluster: Ensure that you have a Kubernetes cluster with Iter8, Knative, Prometheus add-on, and Iter8's sample metrics for Knative installed. You can do this by following Steps 1, 2, 3 and 6 of the quick start tutorial for Knative . Cleanup: If you ran an Iter8 tutorial earlier, run the associated cleanup step. ITER8 environment variable: Ensure that ITER8 environment variable is set to the root directory of your cloned Iter8 repo. See Step 2 of the quick start tutorial for Knative for example. iter8ctl : This tutorial uses iter8ctl .","title":"Conformance Testing"},{"location":"tutorials/knative/conformance/#1-create-app","text":"kubectl apply -f $ITER8 /samples/knative/conformance/baseline.yaml Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\"","title":"1. Create app"},{"location":"tutorials/knative/conformance/#2-generate-requests","text":"kubectl wait --for = condition = Ready ksvc/sample-app URL_VALUE = $( kubectl get ksvc sample-app -o json | jq .status.address.url ) sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/knative/conformance/fortio.yaml | kubectl apply -f - Look inside fortio.yaml apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-t\" , \"6000s\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"2. Generate requests"},{"location":"tutorials/knative/conformance/#3-create-iter8-experiment","text":"kubectl apply -f $ITER8 /samples/knative/conformance/experiment.yaml Look inside experiment.yaml apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : conformance-sample spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : # this experiment will perform a conformance test testingPattern : Conformance actions : start : # run the following sequence of tasks at the start of the experiment - task : knative/init-experiment criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1","title":"3. Create Iter8 experiment"},{"location":"tutorials/knative/conformance/#4-observe-experiment","text":"Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis Periodically describe the experiment. while clear ; do kubectl get experiment conformance-sample -o yaml | iter8ctl describe -f - sleep 4 done The output will look similar to the iter8ctl output in the quick start instructions. As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by the version being tested. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment conformance-sample --watch The output will look similar to the kubectl get experiment output in the quick start instructions. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Understanding what happened You created a Knative service with a single revision, sample-app-v1. You generated requests for the Knative service using a Fortio job. You created an Iter8 Conformance experiment. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, and verified that baseline satisfied all the objectives specified in the experiment.","title":"4. Observe experiment"},{"location":"tutorials/knative/conformance/#5-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/conformance/fortio.yaml kubectl delete -f $ITER8 /samples/knative/conformance/experiment.yaml kubectl delete -f $ITER8 /samples/knative/conformance/baseline.yaml","title":"5. Cleanup"},{"location":"tutorials/knative/mirroring/","text":"Conformance Testing with Traffic Mirroring \u00b6 An experiment with Conformance testing and traffic mirroring . You will create the following resources in this tutorial. A Knative sample app with live and dark versions. Istio virtual services which send all requests to the live version, mirror 40% of the requests and send the mirrored traffic to the dark version; responses from the dark version are ignored since it receives only mirrored requests. A curl-based traffic generator which simulates user requests. An Iter8 experiment that verifies that the dark version satisfies mean latency, 95 th percentile tail latency, and error rate objectives . Before you begin, you will need... Kubernetes cluster with Iter8, Knative and Istio: Ensure that you have a Kubernetes cluster with Iter8, Knative with the Istio networking layer, Prometheus add-on, and Iter8's sample metrics for Knative installed. You can do so by following Steps 1, 2, 3 and 6 of the quick start tutorial for Knative , and selecting Istio during Step 3. Cleanup: If you ran an Iter8 tutorial earlier, run the associated cleanup step. ITER8 environment variable: Ensure that ITER8 environment variable is set to the root directory of your cloned Iter8 repo. See Step 2 of the quick start tutorial for Knative for example. iter8ctl : This tutorial uses iter8ctl . 1. Create app with live and dark versions \u00b6 kubectl apply -f $ITER8 /samples/knative/mirroring/service.yaml Look inside service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - revisionName : sample-app-v1 percent : 100 - latestRevision : true percent : 0 2. Create Istio virtual services \u00b6 kubectl apply -f $ITER8 /samples/knative/mirroring/routing-rules.yaml Look inside routing-rules.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customdomain-mirroring spec : gateways : - mesh - knative-serving/knative-ingress-gateway hosts : - customdomain.com http : - rewrite : authority : customdomain.com route : - destination : host : knative-local-gateway.istio-system.svc.cluster.local mirror : host : knative-local-gateway.istio-system.svc.cluster.local mirrorPercent : 40 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customdomain-routing spec : gateways : - knative-serving/knative-local-gateway hosts : - \"*\" http : - match : - authority : prefix : customdomain.com-shadow route : - destination : host : sample-app-v2.default.svc.cluster.local port : number : 80 headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v2 - match : - authority : prefix : customdomain.com route : - destination : host : sample-app-v1.default.svc.cluster.local port : number : 80 headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1 3. Generate requests \u00b6 TEMP_DIR = $( mktemp -d ) cd $TEMP_DIR curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .8.2 sh - istio-1.8.2/bin/istioctl kube-inject -f $ITER8 /samples/knative/mirroring/curl.yaml | kubectl create -f - cd $ITER8 Look inside curl.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : batch/v1 kind : Job metadata : name : curl spec : template : spec : activeDeadlineSeconds : 6000 containers : - name : curl image : tutum/curl command : - /bin/sh - -c - | sleep 10.0 while true; do curl -sS customdomain.com sleep 0.5 done restartPolicy : Never 4. Create Iter8 experiment \u00b6 kubectl wait --for = condition = Ready ksvc/sample-app kubectl apply -f $ITER8 /samples/knative/mirroring/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : mirroring spec : target : default/sample-app strategy : testingPattern : Conformance actions : start : - task : knative/init-experiment criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about version used in this experiment baseline : name : current variables : - name : revision value : sample-app-v2 5. Observe experiment \u00b6 Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis while clear ; do kubectl get experiment mirroring -o yaml | iter8ctl describe -f - sleep 4 done The output will look similar to the iter8ctl output in the quick start instructions. As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by the version being tested. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment mirroring --watch The output will look similar to the kubectl get experiment output in the quick start instructions. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Understanding what happened You configured a Knative service with two versions of your app. In the service.yaml manifest, you specified that the live version, sample-app-v1 , should receive 100% of the production traffic and the dark version, sample-app-v2 , should receive 0% of the production traffic. You used customdomain.com as the HTTP host in this tutorial. Note: In your production cluster, use domain(s) that you own in the setup of the virtual services. You set up Istio virtual services which mapped the Knative revisions to the custom domain. The virtual services specified the following routing rules: all HTTP requests with their Host header or :authority pseudo-header set to customdomain.com would be sent to sample-app-v1 . 40% of these requests would be mirrored and sent to sample-app-v2 and responses from sample-app-v2 would be ignored. You generated traffic for customdomain.com using a curl -based job. You injected Istio sidecar injected into it to simulate traffic generation from within the cluster. The sidecar was needed in order to correctly route traffic. Note: You used Istio version 1.8.2 to inject the sidecar. This version of Istio corresponds to the one installed in Step 3 of the quick start tutorial . If you have a different version of Istio installed in your cluster, change the Istio version during sidecar injection appropriately. You created an Iter8 Conformance experiment to evaluate the dark version. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics for the dark version collected by Prometheus, and verified that the dark version satisfied all the objectives specified in experiment.yaml . 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/mirroring/curl.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/experiment.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/routing-rules.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/service.yaml","title":"Conformance testing with traffic mirroring"},{"location":"tutorials/knative/mirroring/#conformance-testing-with-traffic-mirroring","text":"An experiment with Conformance testing and traffic mirroring . You will create the following resources in this tutorial. A Knative sample app with live and dark versions. Istio virtual services which send all requests to the live version, mirror 40% of the requests and send the mirrored traffic to the dark version; responses from the dark version are ignored since it receives only mirrored requests. A curl-based traffic generator which simulates user requests. An Iter8 experiment that verifies that the dark version satisfies mean latency, 95 th percentile tail latency, and error rate objectives . Before you begin, you will need... Kubernetes cluster with Iter8, Knative and Istio: Ensure that you have a Kubernetes cluster with Iter8, Knative with the Istio networking layer, Prometheus add-on, and Iter8's sample metrics for Knative installed. You can do so by following Steps 1, 2, 3 and 6 of the quick start tutorial for Knative , and selecting Istio during Step 3. Cleanup: If you ran an Iter8 tutorial earlier, run the associated cleanup step. ITER8 environment variable: Ensure that ITER8 environment variable is set to the root directory of your cloned Iter8 repo. See Step 2 of the quick start tutorial for Knative for example. iter8ctl : This tutorial uses iter8ctl .","title":"Conformance Testing with Traffic Mirroring"},{"location":"tutorials/knative/mirroring/#1-create-app-with-live-and-dark-versions","text":"kubectl apply -f $ITER8 /samples/knative/mirroring/service.yaml Look inside service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - revisionName : sample-app-v1 percent : 100 - latestRevision : true percent : 0","title":"1. Create app with live and dark versions"},{"location":"tutorials/knative/mirroring/#2-create-istio-virtual-services","text":"kubectl apply -f $ITER8 /samples/knative/mirroring/routing-rules.yaml Look inside routing-rules.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customdomain-mirroring spec : gateways : - mesh - knative-serving/knative-ingress-gateway hosts : - customdomain.com http : - rewrite : authority : customdomain.com route : - destination : host : knative-local-gateway.istio-system.svc.cluster.local mirror : host : knative-local-gateway.istio-system.svc.cluster.local mirrorPercent : 40 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customdomain-routing spec : gateways : - knative-serving/knative-local-gateway hosts : - \"*\" http : - match : - authority : prefix : customdomain.com-shadow route : - destination : host : sample-app-v2.default.svc.cluster.local port : number : 80 headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v2 - match : - authority : prefix : customdomain.com route : - destination : host : sample-app-v1.default.svc.cluster.local port : number : 80 headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1","title":"2. Create Istio virtual services"},{"location":"tutorials/knative/mirroring/#3-generate-requests","text":"TEMP_DIR = $( mktemp -d ) cd $TEMP_DIR curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .8.2 sh - istio-1.8.2/bin/istioctl kube-inject -f $ITER8 /samples/knative/mirroring/curl.yaml | kubectl create -f - cd $ITER8 Look inside curl.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : batch/v1 kind : Job metadata : name : curl spec : template : spec : activeDeadlineSeconds : 6000 containers : - name : curl image : tutum/curl command : - /bin/sh - -c - | sleep 10.0 while true; do curl -sS customdomain.com sleep 0.5 done restartPolicy : Never","title":"3. Generate requests"},{"location":"tutorials/knative/mirroring/#4-create-iter8-experiment","text":"kubectl wait --for = condition = Ready ksvc/sample-app kubectl apply -f $ITER8 /samples/knative/mirroring/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : mirroring spec : target : default/sample-app strategy : testingPattern : Conformance actions : start : - task : knative/init-experiment criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about version used in this experiment baseline : name : current variables : - name : revision value : sample-app-v2","title":"4. Create Iter8 experiment"},{"location":"tutorials/knative/mirroring/#5-observe-experiment","text":"Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis while clear ; do kubectl get experiment mirroring -o yaml | iter8ctl describe -f - sleep 4 done The output will look similar to the iter8ctl output in the quick start instructions. As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by the version being tested. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment mirroring --watch The output will look similar to the kubectl get experiment output in the quick start instructions. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Understanding what happened You configured a Knative service with two versions of your app. In the service.yaml manifest, you specified that the live version, sample-app-v1 , should receive 100% of the production traffic and the dark version, sample-app-v2 , should receive 0% of the production traffic. You used customdomain.com as the HTTP host in this tutorial. Note: In your production cluster, use domain(s) that you own in the setup of the virtual services. You set up Istio virtual services which mapped the Knative revisions to the custom domain. The virtual services specified the following routing rules: all HTTP requests with their Host header or :authority pseudo-header set to customdomain.com would be sent to sample-app-v1 . 40% of these requests would be mirrored and sent to sample-app-v2 and responses from sample-app-v2 would be ignored. You generated traffic for customdomain.com using a curl -based job. You injected Istio sidecar injected into it to simulate traffic generation from within the cluster. The sidecar was needed in order to correctly route traffic. Note: You used Istio version 1.8.2 to inject the sidecar. This version of Istio corresponds to the one installed in Step 3 of the quick start tutorial . If you have a different version of Istio installed in your cluster, change the Istio version during sidecar injection appropriately. You created an Iter8 Conformance experiment to evaluate the dark version. In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics for the dark version collected by Prometheus, and verified that the dark version satisfied all the objectives specified in experiment.yaml .","title":"5. Observe experiment"},{"location":"tutorials/knative/mirroring/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/mirroring/curl.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/experiment.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/routing-rules.yaml kubectl delete -f $ITER8 /samples/knative/mirroring/service.yaml","title":"6. Cleanup"},{"location":"tutorials/knative/traffic-segmentation/","text":"Progressive Canary Release with Traffic Segmentation \u00b6 An experiment with Canary testing, Progressive deployment and traffic segmentation . You will create the following resources in this tutorial. Knative services implementing an app with baseline and candidate versions. An Istio virtual service which routes requests based on an HTTP header called country . All requests are routed to the baseline , except those with their country header field set to wakanda ; these may be routed to the baseline or candidate . Two curl-based traffic generators which simulate user requests; one of them sets the country HTTP header field in its requests to wakanda , and the other sets it to gondor . An Iter8 experiment which verifies that the candidate satisfies mean latency, 95 th percentile tail latency, and error rate objectives , and progressively increases the proportion of traffic with country: wakanda header that is routed to the candidate . Before you begin, you will need... Kubernetes cluster with Iter8, Knative and Istio: Ensure that you have a Kubernetes cluster with Iter8, Knative with the Istio networking layer, Prometheus add-on, and Iter8's sample metrics for Knative installed. You can do so by following Steps 1, 2, 3 and 6 of the quick start tutorial for Knative , and selecting Istio during Step 3. Cleanup: If you ran an Iter8 tutorial earlier, run the associated cleanup step. ITER8 environment variable: Ensure that ITER8 environment variable is set to the root directory of your cloned Iter8 repo. See Step 2 of the quick start tutorial for Knative for example. iter8ctl : This tutorial uses iter8ctl . 1. Create versions \u00b6 kubectl apply -f $ITER8 /samples/knative/traffic-segmentation/services.yaml Look inside services.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v1 namespace : default spec : template : metadata : name : sample-app-v1-blue annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v2 namespace : default spec : template : metadata : name : sample-app-v2-green annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" 2. Create Istio virtual service \u00b6 kubectl apply -f $ITER8 /samples/knative/traffic-segmentation/routing-rule.yaml Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-for-wakanda spec : gateways : - mesh - knative-serving/knative-ingress-gateway - knative-serving/knative-local-gateway hosts : - customdomain.com http : - match : - headers : country : exact : wakanda route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default weight : 100 - destination : host : sample-app-v2.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v2-green Host : sample-app-v2.default weight : 0 - route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default 3. Generate traffic \u00b6 TEMP_DIR = $( mktemp -d ) cd $TEMP_DIR curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .8.2 sh - istio-1.8.2/bin/istioctl kube-inject -f $ITER8 /samples/knative/traffic-segmentation/curl.yaml | kubectl create -f - cd $ITER8 Look inside curl.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : batch/v1 kind : Job metadata : name : curl spec : template : spec : activeDeadlineSeconds : 6000 containers : - name : curl-from-gondor image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS customdomain.com -H \"country: gondor\" sleep 1.0 done - name : curl-from-wakanda image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS customdomain.com -H \"country: wakanda\" sleep 0.25 done restartPolicy : Never 4. Create Iter8 experiment \u00b6 kubectl wait --for = condition = Ready ksvc/sample-app-v1 kubectl wait --for = condition = Ready ksvc/sample-app-v2 kubectl apply -f $ITER8 /samples/knative/traffic-segmentation/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : request-routing spec : # this experiment uses the fully-qualified name of the Istio virtual service as the target target : default/routing-for-wakanda strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1-blue weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : candidate variables : - name : revision value : sample-app-v2-green weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[1].weight 5. Observe experiment \u00b6 Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis while clear ; do kubectl get experiment request-routing -o yaml | iter8ctl describe -f - sleep 4 done The output will look similar to the iter8ctl output in the quick start instructions. As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by both versions. The candidate is identified as the winner and is recommended for promotion. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment request-routing --watch The output will look similar to the kubectl get experiment output in the quick start instructions. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Traffic split kubectl get vs routing-for-wakanda -o json --watch | jq .spec.http [ 0 ] .route The output shows the traffic split for the wakanda as defined in the VirtualService resource. As the experiment progresses, you should see traffic progressively shift from host sample-app-v1.default.svc.cluster.local to host sample-app-v2.default.svc.cluster.local . When the experiment completes, the traffic remains split; this experiment has no finish action to promote the winning version. Understanding what happened You configured two Knative services corresponding to two versions of your app in services.yaml . You used customdomain.com as the HTTP host in this tutorial. Note: In your production cluster, use domain(s) that you own in the setup of the virtual service. You set up an Istio virtual service which mapped the Knative services to this custom domain. The virtual service specified the following routing rules: all HTTP requests to customdomain.com with their Host header or :authority pseudo-header not set to wakanda would be routed to the baseline ; those with wakanda Host header or :authority pseudo-header may be routed to baseline and candidate . The percentage of wakandan requests sent to candidate is 0% at the beginning of the experiment. You generated traffic for customdomain.com using a curl -job with two curl -containers to simulate user requests. You injected Istio sidecar injected into it to simulate traffic generation from within the cluster. The sidecar was needed in order to correctly route traffic. One of the curl -containers sets the country header field to wakanda , and the other to gondor . Note: You used Istio version 1.8.2 to inject the sidecar. This version of Istio corresponds to the one installed in Step 3 of the quick start tutorial . If you have a different version of Istio installed in your cluster, change the Istio version during sidecar injection appropriately. You created an Iter8 Canary experiment with Progressive deployment pattern to evaluate the candidate . In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, and verified that the candidate version satisfied all the objectives specified in the experiment. It progressively increased the proportion of traffic with country: wakanda header that is routed to the candidate . 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/traffic-segmentation/experiment.yaml kubectl delete -f $ITER8 /samples/knative/traffic-segmentation/curl.yaml kubectl delete -f $ITER8 /samples/knative/traffic-segmentation/routing-rule.yaml kubectl delete -f $ITER8 /samples/knative/traffic-segmentation/services.yaml","title":"Progressive canary release with traffic segmentation"},{"location":"tutorials/knative/traffic-segmentation/#progressive-canary-release-with-traffic-segmentation","text":"An experiment with Canary testing, Progressive deployment and traffic segmentation . You will create the following resources in this tutorial. Knative services implementing an app with baseline and candidate versions. An Istio virtual service which routes requests based on an HTTP header called country . All requests are routed to the baseline , except those with their country header field set to wakanda ; these may be routed to the baseline or candidate . Two curl-based traffic generators which simulate user requests; one of them sets the country HTTP header field in its requests to wakanda , and the other sets it to gondor . An Iter8 experiment which verifies that the candidate satisfies mean latency, 95 th percentile tail latency, and error rate objectives , and progressively increases the proportion of traffic with country: wakanda header that is routed to the candidate . Before you begin, you will need... Kubernetes cluster with Iter8, Knative and Istio: Ensure that you have a Kubernetes cluster with Iter8, Knative with the Istio networking layer, Prometheus add-on, and Iter8's sample metrics for Knative installed. You can do so by following Steps 1, 2, 3 and 6 of the quick start tutorial for Knative , and selecting Istio during Step 3. Cleanup: If you ran an Iter8 tutorial earlier, run the associated cleanup step. ITER8 environment variable: Ensure that ITER8 environment variable is set to the root directory of your cloned Iter8 repo. See Step 2 of the quick start tutorial for Knative for example. iter8ctl : This tutorial uses iter8ctl .","title":"Progressive Canary Release with Traffic Segmentation"},{"location":"tutorials/knative/traffic-segmentation/#1-create-versions","text":"kubectl apply -f $ITER8 /samples/knative/traffic-segmentation/services.yaml Look inside services.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v1 namespace : default spec : template : metadata : name : sample-app-v1-blue annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v2 namespace : default spec : template : metadata : name : sample-app-v2-green annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\"","title":"1. Create versions"},{"location":"tutorials/knative/traffic-segmentation/#2-create-istio-virtual-service","text":"kubectl apply -f $ITER8 /samples/knative/traffic-segmentation/routing-rule.yaml Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-for-wakanda spec : gateways : - mesh - knative-serving/knative-ingress-gateway - knative-serving/knative-local-gateway hosts : - customdomain.com http : - match : - headers : country : exact : wakanda route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default weight : 100 - destination : host : sample-app-v2.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v2-green Host : sample-app-v2.default weight : 0 - route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default","title":"2. Create Istio virtual service"},{"location":"tutorials/knative/traffic-segmentation/#3-generate-traffic","text":"TEMP_DIR = $( mktemp -d ) cd $TEMP_DIR curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .8.2 sh - istio-1.8.2/bin/istioctl kube-inject -f $ITER8 /samples/knative/traffic-segmentation/curl.yaml | kubectl create -f - cd $ITER8 Look inside curl.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : batch/v1 kind : Job metadata : name : curl spec : template : spec : activeDeadlineSeconds : 6000 containers : - name : curl-from-gondor image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS customdomain.com -H \"country: gondor\" sleep 1.0 done - name : curl-from-wakanda image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS customdomain.com -H \"country: wakanda\" sleep 0.25 done restartPolicy : Never","title":"3. Generate traffic"},{"location":"tutorials/knative/traffic-segmentation/#4-create-iter8-experiment","text":"kubectl wait --for = condition = Ready ksvc/sample-app-v1 kubectl wait --for = condition = Ready ksvc/sample-app-v2 kubectl apply -f $ITER8 /samples/knative/traffic-segmentation/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : request-routing spec : # this experiment uses the fully-qualified name of the Istio virtual service as the target target : default/routing-for-wakanda strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about versions used in this experiment baseline : name : current variables : - name : revision value : sample-app-v1-blue weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : candidate variables : - name : revision value : sample-app-v2-green weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[1].weight","title":"4. Create Iter8 experiment"},{"location":"tutorials/knative/traffic-segmentation/#5-observe-experiment","text":"Observe the experiment in realtime. Paste commands from the tabs below in separate terminals. Metrics-based analysis while clear ; do kubectl get experiment request-routing -o yaml | iter8ctl describe -f - sleep 4 done The output will look similar to the iter8ctl output in the quick start instructions. As the experiment progresses, you should eventually see that all of the objectives reported as being satisfied by both versions. The candidate is identified as the winner and is recommended for promotion. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Experiment progress kubectl get experiment request-routing --watch The output will look similar to the kubectl get experiment output in the quick start instructions. When the experiment completes (in ~ 2 mins), you will see the experiment stage change from Running to Completed . Traffic split kubectl get vs routing-for-wakanda -o json --watch | jq .spec.http [ 0 ] .route The output shows the traffic split for the wakanda as defined in the VirtualService resource. As the experiment progresses, you should see traffic progressively shift from host sample-app-v1.default.svc.cluster.local to host sample-app-v2.default.svc.cluster.local . When the experiment completes, the traffic remains split; this experiment has no finish action to promote the winning version. Understanding what happened You configured two Knative services corresponding to two versions of your app in services.yaml . You used customdomain.com as the HTTP host in this tutorial. Note: In your production cluster, use domain(s) that you own in the setup of the virtual service. You set up an Istio virtual service which mapped the Knative services to this custom domain. The virtual service specified the following routing rules: all HTTP requests to customdomain.com with their Host header or :authority pseudo-header not set to wakanda would be routed to the baseline ; those with wakanda Host header or :authority pseudo-header may be routed to baseline and candidate . The percentage of wakandan requests sent to candidate is 0% at the beginning of the experiment. You generated traffic for customdomain.com using a curl -job with two curl -containers to simulate user requests. You injected Istio sidecar injected into it to simulate traffic generation from within the cluster. The sidecar was needed in order to correctly route traffic. One of the curl -containers sets the country header field to wakanda , and the other to gondor . Note: You used Istio version 1.8.2 to inject the sidecar. This version of Istio corresponds to the one installed in Step 3 of the quick start tutorial . If you have a different version of Istio installed in your cluster, change the Istio version during sidecar injection appropriately. You created an Iter8 Canary experiment with Progressive deployment pattern to evaluate the candidate . In each iteration, Iter8 observed the mean latency, 95 th percentile tail-latency, and error-rate metrics collected by Prometheus, and verified that the candidate version satisfied all the objectives specified in the experiment. It progressively increased the proportion of traffic with country: wakanda header that is routed to the candidate .","title":"5. Observe experiment"},{"location":"tutorials/knative/traffic-segmentation/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/traffic-segmentation/experiment.yaml kubectl delete -f $ITER8 /samples/knative/traffic-segmentation/curl.yaml kubectl delete -f $ITER8 /samples/knative/traffic-segmentation/routing-rule.yaml kubectl delete -f $ITER8 /samples/knative/traffic-segmentation/services.yaml","title":"6. Cleanup"}]}