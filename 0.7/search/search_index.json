{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Iter8 \u00b6","title":"Home"},{"location":"#iter8","text":"","title":"Iter8"},{"location":"news/","text":"News and Announcements \u00b6 Iter8 at KubeCon + CloudNativeCon Europe, May 6, 2021 Iter8 at Knative meetup, Mar 24, 2021 Kubeflow blog article by Animesh Singh and Dan Sun: Operationalize, Scale and Infuse Trust in AI Models using KFServing , Mar 8, 2021 Medium blog article by Michael Kalantar: Automated Canary Release of Microservices on Kubernetes using Tekton and iter8 , Oct 26, 2020 Medium blog article by Kusuma Chalasani: Better Performance with kruize and iter8 for your microservices application , Oct 12, 2020 Medium blog article by Srinivasan Parthasarathy: Automated Canary Release of TensorFlow Models on Kubernetes , Oct 5, 2020 Medium blog article by Sushma Ravichandran: Iter8: Take a look at the magic under the hood , Oct 1, 2020 Medium blog article by Fabio Oliveira: Iter8: Achieving Agility with Control , Aug 17 th , 2020","title":"News"},{"location":"news/#news-and-announcements","text":"Iter8 at KubeCon + CloudNativeCon Europe, May 6, 2021 Iter8 at Knative meetup, Mar 24, 2021 Kubeflow blog article by Animesh Singh and Dan Sun: Operationalize, Scale and Infuse Trust in AI Models using KFServing , Mar 8, 2021 Medium blog article by Michael Kalantar: Automated Canary Release of Microservices on Kubernetes using Tekton and iter8 , Oct 26, 2020 Medium blog article by Kusuma Chalasani: Better Performance with kruize and iter8 for your microservices application , Oct 12, 2020 Medium blog article by Srinivasan Parthasarathy: Automated Canary Release of TensorFlow Models on Kubernetes , Oct 5, 2020 Medium blog article by Sushma Ravichandran: Iter8: Take a look at the magic under the hood , Oct 1, 2020 Medium blog article by Fabio Oliveira: Iter8: Achieving Agility with Control , Aug 17 th , 2020","title":"News and Announcements"},{"location":"roadmap/","text":"Roadmap \u00b6 Enhanced experiments A/B/n, and Pareto testing strategies with single and multiple reward metrics Early termination of experiments Analytics extensibility for traffic shifting and version assessment algorithms Experiments with support and confidence Metrics Support for more metric providers like MySQL, PostgreSQL, CouchDB, MongoDB, and Google Analytics. Enhanced MLOps experiments Customized experiments/metrics for serving frameworks like TorchServe and TFServing GitOps Integration with ArgoCD, Flux and other GitOps operators Enhancing Kubernetes and OpenShift integration Support for OpenShift runtimes Enhanced Knative metrics in tutorials using OpenTelemetry collector Git triggered workflows and CI/CD Integration with GitHub Actions and other pipeline providers Improved installation Iter8 Helm chart, Iter8 Operator","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Enhanced experiments A/B/n, and Pareto testing strategies with single and multiple reward metrics Early termination of experiments Analytics extensibility for traffic shifting and version assessment algorithms Experiments with support and confidence Metrics Support for more metric providers like MySQL, PostgreSQL, CouchDB, MongoDB, and Google Analytics. Enhanced MLOps experiments Customized experiments/metrics for serving frameworks like TorchServe and TFServing GitOps Integration with ArgoCD, Flux and other GitOps operators Enhancing Kubernetes and OpenShift integration Support for OpenShift runtimes Enhanced Knative metrics in tutorials using OpenTelemetry collector Git triggered workflows and CI/CD Integration with GitHub Actions and other pipeline providers Improved installation Iter8 Helm chart, Iter8 Operator","title":"Roadmap"},{"location":"concepts/buildingblocks/","text":"Building Blocks \u00b6 We introduce the building blocks of an Iter8 experiment below. Applications and Versions \u00b6 Iter8 defines an application broadly as an entity that can be: instantiated (run) on Kubernetes, can be versioned, and for which metrics can be collected. Examples A stateless K8s application whose versions correspond to deployments . A stateful K8s application whose versions correspond to statefulsets . A Knative application whose versions correspond to revisions . A KFServing inference service, whose versions correspond to model revisions . A distributed application whose versions correspond to Helm releases . Objectives (SLOs) \u00b6 Objectives correspond to service-level objectives or SLOs. In Iter8 experiments, objectives are specified as metrics along with acceptable limits on their values. Iter8 will report how versions are performing with respect to these metrics and whether or not they satisfy the objectives. Examples The 99 th -percentile tail latency of the application should be under 50 msec. The precision of the ML model version should be over 92%. The (average) number of GPU cores consumed by a model should be under 5.0 Reward \u00b6 Reward typically corresponds to a business metric which you wish to optimize during an A/B testing experiment. In Iter8 experiments, reward is specified as a metric along with a preferred direction, which could be high or low . Examples User-engagement Conversion rate Click-through rate Revenue Precision, recall, or accuracy (for ML models) Number of GPU cores consumed by an ML model All but the last example above have a preferred direction high ; the last example is that of a reward with preferred direction low . Baseline and candidate versions \u00b6 Every Iter8 experiment involves a baseline version and may also involve zero, one or more candidate versions. Experiments often involve two versions, baseline and a candidate, with the baseline version corresponding to the stable version of your app, and the candidate version corresponds to a canary. Testing strategy \u00b6 Testing strategy determines how the winning version (winner) in an experiment is identified. SLO validation \u00b6 SLO validation experiments may involve a single version or two versions. SLO validation experiment with baseline version and no candidate (conformance testing): If baseline satisfies the objectives , it is the winner. Otherwise, there is no winner. SLO validation experiment with baseline and candidate versions: If candidate satisfies the objectives , it is the winner. Else, if baseline satisfies the objectives , it is the winner. Else, there is no winner. A/B testing \u00b6 A/B testing experiments involve a baseline version, a candidate version, and a reward metric. The version which performs best in terms of the reward metric is the winner. A/B/n testing \u00b6 A/B/n testing experiments involve a baseline version, two or more candidate versions, and a reward metric. The version which performs best in terms of the reward metric is the winner. Hybrid (A/B + SLOs) testing \u00b6 Hybrid (A/B + SLOs) testing experiments combine A/B or A/B/n testing on the one hand with SLO validation on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. If no version satisfies objectives, then there is no winner. Rollout strategy \u00b6 Rollout strategy defines how traffic is split between versions during the experiment. Iter8 makes it easy for you to take total advantage of all the traffic engineering features available in your K8s environment (i.e., supported by the ingress or service mesh technology available in your K8s cluster). A few common deployment strategies used in Iter8 experiments are described below. In the following description, v1 and v2 refer to the current and new versions of the application respectively. Simple rollout & rollout \u00b6 This pattern is modeled after the rolling update of a Kubernetes deployment . After v2 is deployed, it replaces v1 . If v2 is the winner of the experiment, it is retained. Else, v2 is rolled back and v1 is retained. All traffic flows to v2 during the experiment. BlueGreen \u00b6 After v2 is deployed, both v1 and v2 are available. All traffic is routed to v2 . If v2 is the winner of the experiment, all traffic continues to flow to v2 . Else, all traffic is routed back to v1 . Dark launch \u00b6 After v2 is deployed, it is hidden from end-users. v2 is not used to serve end-user requests but can still be experimented with. Built-in load/metrics \u00b6 During the experiment, Iter8 generates load for v2 and/or collects built-in metrics. Traffic mirroring (shadowing) \u00b6 Mirrored traffic is a replica of the real user requests 1 that is routed to v2 , and used to collect metrics for v2 . Canary \u00b6 Canary deployment involves exposing v2 to a small fraction of end-user requests during the experiment before exposing it to a larger fraction of requests or all the requests. Fixed-%-split \u00b6 A fixed % of end-user requests is sent to v2 and the rest is sent to v1 . Fixed-%-split with user segmentation \u00b6 Only a specific segment of the users participate in the experiment. A fixed % of requests from the participating segment is sent to v2 . Rest is sent to v1 . All requests from end-users in the non-participating segment is sent to v1 . Progressive traffic shift \u00b6 Traffic is incrementally shifted to the winner over multiple iterations. Progressive traffic shift with user segmentation \u00b6 Only a specific segment of the users participate in the experiment. Within this segment, traffic is incrementally shifted to the winner over multiple iterations. All requests from end-users in the non-participating segment is sent to v1 . Session affinity \u00b6 Session affinity, sometimes referred to as sticky sessions, routes all requests coming from an end-user to the same version consistently throughout the experiment. User grouping for affinity can be configured based on a number of different attributes of the request including request headers, cookies, query parameters, geo location, user agent (browser version, screen size, operating system) and language. Version promotion \u00b6 Iter8 can automatically promote the winning version at the end of an experiment. It is possible to mirror only a certain percentage of the requests instead of all requests. \u21a9","title":"Experiment building blocks"},{"location":"concepts/buildingblocks/#building-blocks","text":"We introduce the building blocks of an Iter8 experiment below.","title":"Building Blocks"},{"location":"concepts/buildingblocks/#applications-and-versions","text":"Iter8 defines an application broadly as an entity that can be: instantiated (run) on Kubernetes, can be versioned, and for which metrics can be collected. Examples A stateless K8s application whose versions correspond to deployments . A stateful K8s application whose versions correspond to statefulsets . A Knative application whose versions correspond to revisions . A KFServing inference service, whose versions correspond to model revisions . A distributed application whose versions correspond to Helm releases .","title":"Applications and Versions"},{"location":"concepts/buildingblocks/#objectives-slos","text":"Objectives correspond to service-level objectives or SLOs. In Iter8 experiments, objectives are specified as metrics along with acceptable limits on their values. Iter8 will report how versions are performing with respect to these metrics and whether or not they satisfy the objectives. Examples The 99 th -percentile tail latency of the application should be under 50 msec. The precision of the ML model version should be over 92%. The (average) number of GPU cores consumed by a model should be under 5.0","title":"Objectives (SLOs)"},{"location":"concepts/buildingblocks/#reward","text":"Reward typically corresponds to a business metric which you wish to optimize during an A/B testing experiment. In Iter8 experiments, reward is specified as a metric along with a preferred direction, which could be high or low . Examples User-engagement Conversion rate Click-through rate Revenue Precision, recall, or accuracy (for ML models) Number of GPU cores consumed by an ML model All but the last example above have a preferred direction high ; the last example is that of a reward with preferred direction low .","title":"Reward"},{"location":"concepts/buildingblocks/#baseline-and-candidate-versions","text":"Every Iter8 experiment involves a baseline version and may also involve zero, one or more candidate versions. Experiments often involve two versions, baseline and a candidate, with the baseline version corresponding to the stable version of your app, and the candidate version corresponds to a canary.","title":"Baseline and candidate versions"},{"location":"concepts/buildingblocks/#testing-strategy","text":"Testing strategy determines how the winning version (winner) in an experiment is identified.","title":"Testing strategy"},{"location":"concepts/buildingblocks/#slo-validation","text":"SLO validation experiments may involve a single version or two versions. SLO validation experiment with baseline version and no candidate (conformance testing): If baseline satisfies the objectives , it is the winner. Otherwise, there is no winner. SLO validation experiment with baseline and candidate versions: If candidate satisfies the objectives , it is the winner. Else, if baseline satisfies the objectives , it is the winner. Else, there is no winner.","title":"SLO validation"},{"location":"concepts/buildingblocks/#ab-testing","text":"A/B testing experiments involve a baseline version, a candidate version, and a reward metric. The version which performs best in terms of the reward metric is the winner.","title":"A/B testing"},{"location":"concepts/buildingblocks/#abn-testing","text":"A/B/n testing experiments involve a baseline version, two or more candidate versions, and a reward metric. The version which performs best in terms of the reward metric is the winner.","title":"A/B/n testing"},{"location":"concepts/buildingblocks/#hybrid-ab-slos-testing","text":"Hybrid (A/B + SLOs) testing experiments combine A/B or A/B/n testing on the one hand with SLO validation on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. If no version satisfies objectives, then there is no winner.","title":"Hybrid (A/B + SLOs) testing"},{"location":"concepts/buildingblocks/#rollout-strategy","text":"Rollout strategy defines how traffic is split between versions during the experiment. Iter8 makes it easy for you to take total advantage of all the traffic engineering features available in your K8s environment (i.e., supported by the ingress or service mesh technology available in your K8s cluster). A few common deployment strategies used in Iter8 experiments are described below. In the following description, v1 and v2 refer to the current and new versions of the application respectively.","title":"Rollout strategy"},{"location":"concepts/buildingblocks/#simple-rollout-rollout","text":"This pattern is modeled after the rolling update of a Kubernetes deployment . After v2 is deployed, it replaces v1 . If v2 is the winner of the experiment, it is retained. Else, v2 is rolled back and v1 is retained. All traffic flows to v2 during the experiment.","title":"Simple rollout &amp; rollout"},{"location":"concepts/buildingblocks/#bluegreen","text":"After v2 is deployed, both v1 and v2 are available. All traffic is routed to v2 . If v2 is the winner of the experiment, all traffic continues to flow to v2 . Else, all traffic is routed back to v1 .","title":"BlueGreen"},{"location":"concepts/buildingblocks/#dark-launch","text":"After v2 is deployed, it is hidden from end-users. v2 is not used to serve end-user requests but can still be experimented with.","title":"Dark launch"},{"location":"concepts/buildingblocks/#built-in-loadmetrics","text":"During the experiment, Iter8 generates load for v2 and/or collects built-in metrics.","title":"Built-in load/metrics"},{"location":"concepts/buildingblocks/#traffic-mirroring-shadowing","text":"Mirrored traffic is a replica of the real user requests 1 that is routed to v2 , and used to collect metrics for v2 .","title":"Traffic mirroring (shadowing)"},{"location":"concepts/buildingblocks/#canary","text":"Canary deployment involves exposing v2 to a small fraction of end-user requests during the experiment before exposing it to a larger fraction of requests or all the requests.","title":"Canary"},{"location":"concepts/buildingblocks/#fixed-split","text":"A fixed % of end-user requests is sent to v2 and the rest is sent to v1 .","title":"Fixed-%-split"},{"location":"concepts/buildingblocks/#fixed-split-with-user-segmentation","text":"Only a specific segment of the users participate in the experiment. A fixed % of requests from the participating segment is sent to v2 . Rest is sent to v1 . All requests from end-users in the non-participating segment is sent to v1 .","title":"Fixed-%-split with user segmentation"},{"location":"concepts/buildingblocks/#progressive-traffic-shift","text":"Traffic is incrementally shifted to the winner over multiple iterations.","title":"Progressive traffic shift"},{"location":"concepts/buildingblocks/#progressive-traffic-shift-with-user-segmentation","text":"Only a specific segment of the users participate in the experiment. Within this segment, traffic is incrementally shifted to the winner over multiple iterations. All requests from end-users in the non-participating segment is sent to v1 .","title":"Progressive traffic shift with user segmentation"},{"location":"concepts/buildingblocks/#session-affinity","text":"Session affinity, sometimes referred to as sticky sessions, routes all requests coming from an end-user to the same version consistently throughout the experiment. User grouping for affinity can be configured based on a number of different attributes of the request including request headers, cookies, query parameters, geo location, user agent (browser version, screen size, operating system) and language.","title":"Session affinity"},{"location":"concepts/buildingblocks/#version-promotion","text":"Iter8 can automatically promote the winning version at the end of an experiment. It is possible to mirror only a certain percentage of the requests instead of all requests. \u21a9","title":"Version promotion"},{"location":"concepts/whatisiter8/","text":"What is Iter8? \u00b6 Iter8 is the release engineering platform for Kubernetes applications and ML models. Iter8 is designed for DevOps and MLOps teams interested in maximizing release velocity and business value with their apps/ML models while protecting end-user experience. Use Iter8 for SLO validation, A/B testing and progressive rollouts of K8s apps/ML models. What is an Iter8 experiment? \u00b6 Iter8 defines a Kubernetes resource called Experiment that automates SLO validation, A/B(/n) testing and progressive rollouts as shown below. What is Helmex? \u00b6 Helmex, short for Helm-based experiments, is an Iter8 experimentation pattern that uses Helm. In Helmex, the Helm values file used for managing the application releases satisfy the Helmex schema . How does Iter8 work? \u00b6 Iter8 consists of a Go-based Kubernetes controller that orchestrates (reconciles) experiments in conjunction with a Python-based analytics service , and a Go-based task runner .","title":"What is Iter8?"},{"location":"concepts/whatisiter8/#what-is-iter8","text":"Iter8 is the release engineering platform for Kubernetes applications and ML models. Iter8 is designed for DevOps and MLOps teams interested in maximizing release velocity and business value with their apps/ML models while protecting end-user experience. Use Iter8 for SLO validation, A/B testing and progressive rollouts of K8s apps/ML models.","title":"What is Iter8?"},{"location":"concepts/whatisiter8/#what-is-an-iter8-experiment","text":"Iter8 defines a Kubernetes resource called Experiment that automates SLO validation, A/B(/n) testing and progressive rollouts as shown below.","title":"What is an Iter8 experiment?"},{"location":"concepts/whatisiter8/#what-is-helmex","text":"Helmex, short for Helm-based experiments, is an Iter8 experimentation pattern that uses Helm. In Helmex, the Helm values file used for managing the application releases satisfy the Helmex schema .","title":"What is Helmex?"},{"location":"concepts/whatisiter8/#how-does-iter8-work","text":"Iter8 consists of a Go-based Kubernetes controller that orchestrates (reconciles) experiments in conjunction with a Python-based analytics service , and a Go-based task runner .","title":"How does Iter8 work?"},{"location":"contributing/analytics/","text":"Extending Iter8's Analytics Functions \u00b6 Iter8's analytics functions are implemented in the iter8-analytics repo . Python virtual environment \u00b6 Use a Python 3+ virtual environment to locally develop iter8-analytics . You can create and activate a virtual environment as follows. git clone git@github.com:iter8-tools/iter8-analytics.git cd iter8-analytics python3 -m venv .venv source .venv/bin/activate Running iter8-analytics locally \u00b6 Create and activate a Python 3+ virtual environment as described above. The following instructions have been verified in a Python 3.9 virtual environment. Run them from the root folder of your iter8-analytics local repo. 1. pip install -r requirements.txt 2. pip install -e . 3. cd iter8_analytics 4. python fastapi_app.py Navigate to http://localhost:8080/docs on your browser. You can interact with the iter8-analytics service and read its API documentation here. The iter8-analytics APIs are intended to work with metric databases, and use Kubernetes secrets for obtaining the required authentication information for querying the metric DBs. Running unit tests for iter8-analytics locally \u00b6 1. pip install -r requirements.txt 2. pip install -r test-requirements.txt 3. pip install -e . 4. coverage run --source=iter8_analytics --omit=\"*/__init__.py\" -m pytest You can see the coverage report by opening htmlcov/index.html in your browser.","title":"Analytics"},{"location":"contributing/analytics/#extending-iter8s-analytics-functions","text":"Iter8's analytics functions are implemented in the iter8-analytics repo .","title":"Extending Iter8's Analytics Functions"},{"location":"contributing/analytics/#python-virtual-environment","text":"Use a Python 3+ virtual environment to locally develop iter8-analytics . You can create and activate a virtual environment as follows. git clone git@github.com:iter8-tools/iter8-analytics.git cd iter8-analytics python3 -m venv .venv source .venv/bin/activate","title":"Python virtual environment"},{"location":"contributing/analytics/#running-iter8-analytics-locally","text":"Create and activate a Python 3+ virtual environment as described above. The following instructions have been verified in a Python 3.9 virtual environment. Run them from the root folder of your iter8-analytics local repo. 1. pip install -r requirements.txt 2. pip install -e . 3. cd iter8_analytics 4. python fastapi_app.py Navigate to http://localhost:8080/docs on your browser. You can interact with the iter8-analytics service and read its API documentation here. The iter8-analytics APIs are intended to work with metric databases, and use Kubernetes secrets for obtaining the required authentication information for querying the metric DBs.","title":"Running iter8-analytics locally"},{"location":"contributing/analytics/#running-unit-tests-for-iter8-analytics-locally","text":"1. pip install -r requirements.txt 2. pip install -r test-requirements.txt 3. pip install -e . 4. coverage run --source=iter8_analytics --omit=\"*/__init__.py\" -m pytest You can see the coverage report by opening htmlcov/index.html in your browser.","title":"Running unit tests for iter8-analytics locally"},{"location":"contributing/newk8sstack/","text":"Add a K8s Stack / Service Mesh / Ingress \u00b6 Performing Iter8 experiments requires RBAC rules, which are contained in this Kustomize folder and are installed as part of the Iter8 installation. Enable Iter8 experiments over a new K8s stack by extending these RBAC rules. Step 1: Fork Iter8 \u00b6 Fork the Iter8 GitHub repo . Locally clone your forked repo. For the rest of this document, $ITER8 will refer to the root of your local Iter8 repo. Step 2: Edit kustomization.yaml \u00b6 cd $ITER8 /install/core/rbac/stacks Edit kustomization.yaml to add your K8s stack. At the time of writing, it contains the following stacks: resources : - iter8-knative - iter8-istio - iter8-kfserving - iter8-seldon - iter8-linkerd # -iter8-<your stack> # add your stack here Step 3: Create subfolder \u00b6 mkdir iter8-<your stack> cp iter8-kfserving/kustomization.yaml iter8-<your stack>/kustomization.yaml Step 4: Create RBAC rules \u00b6 cd iter8-<your stack> Foo & Istio virtual service example Suppose Iter8 experiments on your stack involves manipulation of two types of resources: The foo resource belonging to the API group bar.my.org . The Istio virtual service resource. Note: Foo and bar are merely placeholders. It can be replaced by any standard K8s resource type like deployment or service , or a custom resource type, as required. Create RBAC rules that will enable Iter8 to manipulate foo resources and Istio virtual service resources during experiments. You can do so by creating roles.yaml and rolebindings.yaml files as follows. roles.yaml # This cluster role enables manipulation of foo resources apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : foo-for-<your stack> rules : - apiGroups : - bar.my.org resources : - foo verbs : - get - list - patch - update - create - delete - watch --- # This cluster role enables manipulation of Istio virtual services apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vs-for-<your stack> rules : - apiGroups : - networking.istio.io resources : - virtualservices verbs : - get - list - patch - update - create - delete - watch rolebindings.yaml # This cluster role binding enables Iter8 controller and task runner to manipulate # foo resources in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : foo-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : foo-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers --- # This role binding enables Iter8 controller and handler to manipulate # Istio virtual services in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vs-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vs-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers You can also refer to the Istio , KFServing , Knative , and Seldon examples. Step 5: Update RBAC rules \u00b6 Update the RBAC rules that are applied to the Kubernetes cluster as part of the Iter8 installation. Step 6: Submit PR \u00b6 Sign your commit and submit your pull request to the Iter8 repo.","title":"New K8s stack"},{"location":"contributing/newk8sstack/#add-a-k8s-stack-service-mesh-ingress","text":"Performing Iter8 experiments requires RBAC rules, which are contained in this Kustomize folder and are installed as part of the Iter8 installation. Enable Iter8 experiments over a new K8s stack by extending these RBAC rules.","title":"Add a K8s Stack / Service Mesh / Ingress"},{"location":"contributing/newk8sstack/#step-1-fork-iter8","text":"Fork the Iter8 GitHub repo . Locally clone your forked repo. For the rest of this document, $ITER8 will refer to the root of your local Iter8 repo.","title":"Step 1: Fork Iter8"},{"location":"contributing/newk8sstack/#step-2-edit-kustomizationyaml","text":"cd $ITER8 /install/core/rbac/stacks Edit kustomization.yaml to add your K8s stack. At the time of writing, it contains the following stacks: resources : - iter8-knative - iter8-istio - iter8-kfserving - iter8-seldon - iter8-linkerd # -iter8-<your stack> # add your stack here","title":"Step 2: Edit kustomization.yaml"},{"location":"contributing/newk8sstack/#step-3-create-subfolder","text":"mkdir iter8-<your stack> cp iter8-kfserving/kustomization.yaml iter8-<your stack>/kustomization.yaml","title":"Step 3: Create subfolder"},{"location":"contributing/newk8sstack/#step-4-create-rbac-rules","text":"cd iter8-<your stack> Foo & Istio virtual service example Suppose Iter8 experiments on your stack involves manipulation of two types of resources: The foo resource belonging to the API group bar.my.org . The Istio virtual service resource. Note: Foo and bar are merely placeholders. It can be replaced by any standard K8s resource type like deployment or service , or a custom resource type, as required. Create RBAC rules that will enable Iter8 to manipulate foo resources and Istio virtual service resources during experiments. You can do so by creating roles.yaml and rolebindings.yaml files as follows. roles.yaml # This cluster role enables manipulation of foo resources apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : foo-for-<your stack> rules : - apiGroups : - bar.my.org resources : - foo verbs : - get - list - patch - update - create - delete - watch --- # This cluster role enables manipulation of Istio virtual services apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vs-for-<your stack> rules : - apiGroups : - networking.istio.io resources : - virtualservices verbs : - get - list - patch - update - create - delete - watch rolebindings.yaml # This cluster role binding enables Iter8 controller and task runner to manipulate # foo resources in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : foo-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : foo-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers --- # This role binding enables Iter8 controller and handler to manipulate # Istio virtual services in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vs-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vs-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers You can also refer to the Istio , KFServing , Knative , and Seldon examples.","title":"Step 4: Create RBAC rules"},{"location":"contributing/newk8sstack/#step-5-update-rbac-rules","text":"Update the RBAC rules that are applied to the Kubernetes cluster as part of the Iter8 installation.","title":"Step 5: Update RBAC rules"},{"location":"contributing/newk8sstack/#step-6-submit-pr","text":"Sign your commit and submit your pull request to the Iter8 repo.","title":"Step 6: Submit PR"},{"location":"contributing/overview/","text":"Overview \u00b6 Welcome! We are delighted that you want to contribute to Iter8! \ud83d\udc96 As you get started, you are in the best position to give us feedback on areas of our project that we need help with including: Problems found during setup of Iter8 Gaps in our quick start guide or other tutorials and documentation Bugs in our test and automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know! Ways to Contribute \u00b6 We welcome many different types of contributions including: Iter8 documentation/tutorials New features New K8s stack/service mesh/ingress Iter8 code samples for OpenShift Analytics Tasks Builds, CI Bug fixes Web design for https://iter8.tools Communications/social media/blog posts Reviewing pull requests Not everything happens through a GitHub pull request. Please come to our community meetings or contact us and let's discuss how we can work together. Come to Weekly Community Meetings! \u00b6 Everyone is welcome to join our weekly community meetings. You never need an invite to attend. In fact, we want you to join us, even if you don\u2019t have anything you feel like you want to contribute. Just being there is enough! Our community meetings are on Wednesdays from 11 AM - 12 PM EST/EDT. You can find the video conference link here . You can find the agenda here as well as the meeting notes here . If you have a topic you would like to discuss, please put it on the agenda as well as a time estimate. Community meetings will be recorded and publicly available on our YouTube channel . Find an Issue \u00b6 Iter8 issues are managed centrally here . We have good first issues for new contributors and help wanted issues suitable for any contributor. Issued labeled good first issue have extra information to help you make your first contribution. Issues labeled help wanted are issues suitable for someone who isn't a core maintainer and is good to move onto after your first pull request. Sometimes there won\u2019t be any issues with these labels. That\u2019s ok! There is likely still something for you to work on. If you want to contribute but you don\u2019t know where to start or can't find a suitable issue, you can reach out to us over the Iter8 Slack workspace for help finding something to work on. Once you see an issue that you'd like to work on, please post a comment saying that you want to work on it. Something like \"I want to work on this\" is fine. Ask for Help \u00b6 The best ways to reach us with a question when contributing is to ask on: The original GitHub issue #development channel in the Iter8 Slack workspace Bring your questions to our community meetings Pull Request Lifecycle \u00b6 Your PR is associated with one (and infrequently, with more than one) GitHub issue . You can start the submission of your PR as soon as this issue has been created. Follow the standard GitHub fork and pull request process when creating and submitting your PR. The associated GitHub issue might need to go through design discussions and may not be ready for development. Your PR might require new tests; these new or existing tests may not yet be running successfully. At this stage, keep your PR as a draft , to signal that it is not yet ready for review. Once design discussions are complete and tests pass, convert the draft PR into a regular PR to signal that it is ready for review. Additionally, post a message in the #development Slack channel of the Iter8 Slack workspace with a link to your PR. This will expedite the review. You can expect an initial review within 1-2 days of submitting a PR, and follow up reviews (if any) to happen over 2-5 days. Use the #development Slack channel of Iter8 Slack workspace to ping/bump when the pull request is ready for further review or if it appears stalled. Iter8 releases happen frequently. Once your PR is merged, you can expect your contribution to show up live in a short amount of time at https://iter8.tools . Sign Your Commits \u00b6 Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. Read GitHub's documentation on signing your commits . You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running git commit --amend -s","title":"Overview"},{"location":"contributing/overview/#overview","text":"Welcome! We are delighted that you want to contribute to Iter8! \ud83d\udc96 As you get started, you are in the best position to give us feedback on areas of our project that we need help with including: Problems found during setup of Iter8 Gaps in our quick start guide or other tutorials and documentation Bugs in our test and automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know!","title":"Overview"},{"location":"contributing/overview/#ways-to-contribute","text":"We welcome many different types of contributions including: Iter8 documentation/tutorials New features New K8s stack/service mesh/ingress Iter8 code samples for OpenShift Analytics Tasks Builds, CI Bug fixes Web design for https://iter8.tools Communications/social media/blog posts Reviewing pull requests Not everything happens through a GitHub pull request. Please come to our community meetings or contact us and let's discuss how we can work together.","title":"Ways to Contribute"},{"location":"contributing/overview/#come-to-weekly-community-meetings","text":"Everyone is welcome to join our weekly community meetings. You never need an invite to attend. In fact, we want you to join us, even if you don\u2019t have anything you feel like you want to contribute. Just being there is enough! Our community meetings are on Wednesdays from 11 AM - 12 PM EST/EDT. You can find the video conference link here . You can find the agenda here as well as the meeting notes here . If you have a topic you would like to discuss, please put it on the agenda as well as a time estimate. Community meetings will be recorded and publicly available on our YouTube channel .","title":"Come to Weekly Community Meetings!"},{"location":"contributing/overview/#find-an-issue","text":"Iter8 issues are managed centrally here . We have good first issues for new contributors and help wanted issues suitable for any contributor. Issued labeled good first issue have extra information to help you make your first contribution. Issues labeled help wanted are issues suitable for someone who isn't a core maintainer and is good to move onto after your first pull request. Sometimes there won\u2019t be any issues with these labels. That\u2019s ok! There is likely still something for you to work on. If you want to contribute but you don\u2019t know where to start or can't find a suitable issue, you can reach out to us over the Iter8 Slack workspace for help finding something to work on. Once you see an issue that you'd like to work on, please post a comment saying that you want to work on it. Something like \"I want to work on this\" is fine.","title":"Find an Issue"},{"location":"contributing/overview/#ask-for-help","text":"The best ways to reach us with a question when contributing is to ask on: The original GitHub issue #development channel in the Iter8 Slack workspace Bring your questions to our community meetings","title":"Ask for Help"},{"location":"contributing/overview/#pull-request-lifecycle","text":"Your PR is associated with one (and infrequently, with more than one) GitHub issue . You can start the submission of your PR as soon as this issue has been created. Follow the standard GitHub fork and pull request process when creating and submitting your PR. The associated GitHub issue might need to go through design discussions and may not be ready for development. Your PR might require new tests; these new or existing tests may not yet be running successfully. At this stage, keep your PR as a draft , to signal that it is not yet ready for review. Once design discussions are complete and tests pass, convert the draft PR into a regular PR to signal that it is ready for review. Additionally, post a message in the #development Slack channel of the Iter8 Slack workspace with a link to your PR. This will expedite the review. You can expect an initial review within 1-2 days of submitting a PR, and follow up reviews (if any) to happen over 2-5 days. Use the #development Slack channel of Iter8 Slack workspace to ping/bump when the pull request is ready for further review or if it appears stalled. Iter8 releases happen frequently. Once your PR is merged, you can expect your contribution to show up live in a short amount of time at https://iter8.tools .","title":"Pull Request Lifecycle"},{"location":"contributing/overview/#sign-your-commits","text":"Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. Read GitHub's documentation on signing your commits . You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running git commit --amend -s","title":"Sign Your Commits"},{"location":"contributing/tutorials/","text":"Contribute Tutorials \u00b6 MkDocs \u00b6 Iter8 documentation uses Mkdocs . The section on linking to pages and images is especially useful for Iter8 tutorial authors. Test your tutorial \u00b6 All Iter8 tutorials include e2e tests, either as part of GitHub Actions workflows or as a standalone test script like this one if they require more resources than what is available in GitHub Actions workflows. When contributing a tutorial, please include relevant e2e tests. Locally serve Iter8 docs \u00b6 Pre-requisite: Python 3+. Use a Python 3 virtual environment to locally serve Iter8 docs. Run the following commands from the top-level directory of the Iter8 repo. cd mkdocs python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s Browse http://localhost:8000 to view your local Iter8 docs. Locally view live changes to Iter8 docs \u00b6 The overall structure of the documentation, as reflected in the nav tabs of https://iter8.tools , is located in the iter8/mkdocs/mkdocs.yml file. The markdown files for Iter8 docs are located under the iter8/mkdocs/docs folder. You will see live updates to http://localhost:8000 as you update the above files.","title":"Tutorials"},{"location":"contributing/tutorials/#contribute-tutorials","text":"","title":"Contribute Tutorials"},{"location":"contributing/tutorials/#mkdocs","text":"Iter8 documentation uses Mkdocs . The section on linking to pages and images is especially useful for Iter8 tutorial authors.","title":"MkDocs"},{"location":"contributing/tutorials/#test-your-tutorial","text":"All Iter8 tutorials include e2e tests, either as part of GitHub Actions workflows or as a standalone test script like this one if they require more resources than what is available in GitHub Actions workflows. When contributing a tutorial, please include relevant e2e tests.","title":"Test your tutorial"},{"location":"contributing/tutorials/#locally-serve-iter8-docs","text":"Pre-requisite: Python 3+. Use a Python 3 virtual environment to locally serve Iter8 docs. Run the following commands from the top-level directory of the Iter8 repo. cd mkdocs python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s Browse http://localhost:8000 to view your local Iter8 docs.","title":"Locally serve Iter8 docs"},{"location":"contributing/tutorials/#locally-view-live-changes-to-iter8-docs","text":"The overall structure of the documentation, as reflected in the nav tabs of https://iter8.tools , is located in the iter8/mkdocs/mkdocs.yml file. The markdown files for Iter8 docs are located under the iter8/mkdocs/docs folder. You will see live updates to http://localhost:8000 as you update the above files.","title":"Locally view live changes to Iter8 docs"},{"location":"getting-started/first-experiment/","text":"Your First Experiment \u00b6 Scenario: Safely rollout a Kubernetes deployment with SLO validation Dark launch a candidate version of your application (a K8s service and deployment), validate that the candidate satisfies latency and error-based objectives (SLOs) , and promote the candidate. Setup K8s cluster and local environment Get Helm 3+ . This tutorial uses the Helmex pattern Setup K8s cluster Install Iter8 in K8s cluster Get iter8ctl Clone the Iter8 GitHub repo . Set the ITER8 environment variable to the root of the cloned repo. 1. Create baseline version \u00b6 Deploy the baseline version of the hello world application using Helm. cd $ITER8 /samples/first-exp/helm helm dependency update helm install my-app . \\ --set baseline.dynamic.tag = 1 .0 \\ --set candidate = null Verify that baseline version is 1.0.0 # do this in a separate terminal kubectl port-forward svc/hello 8080 :8080 curl localhost:8080 # output will be similar to the following (notice 1.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 1.0.0 Hostname: hello-bc95d9b56-xp9kv 2. Create candidate version \u00b6 Deploy the candidate version of the hello world application using Helm. helm upgrade my-app . \\ --set baseline.dynamic.tag = 1 .0 \\ --set candidate.dynamic.tag = 2 .0 \\ --install The above command creates an Iter8 experiment alongside the candidate deployment of the hello world application. The experiment will collect latency and error rate metrics for the candidate, and verify that it satisfies the mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. View application and experiment resources Use the command below to view your application and Iter8 experiment resources. helm get manifest my-app Verify that candidate version is 2.0.0 # do this in a separate terminal kubectl port-forward svc/hello-candidate 8081 :8080 curl localhost:8081 # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv 3. Observe experiment \u00b6 Describe the results of the Iter8 experiment. Wait 20 seconds before trying the following command. If the output is not as expected, try again after a few more seconds. iter8ctl describe Experiment results will look similar to this ... ****** Overview ****** Experiment name: my-experiment Experiment namespace: default Target: my-app Testing pattern: Conformance Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Completed Number of completed iterations: 1 ****** Winner Assessment ****** > If the version being validated ; i.e., the baseline version, satisfies the experiment objectives, it is the winner. > Otherwise, there is no winner. Winning version: my-app ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------+--------+ | OBJECTIVE | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency < = | true | | 50 .000 | | +--------------------------------------+--------+ | iter8-system/error-rate < = | true | | 0 .000 | | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | true | | < = 100 .000 | | +--------------------------------------+--------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------+--------+ | METRIC | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency | 1 .233 | +--------------------------------------+--------+ | iter8-system/error-rate | 0 .000 | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | 2 .311 | +--------------------------------------+--------+ | iter8-system/request-count | 40 .000 | +--------------------------------------+--------+ | iter8-system/error-count | 0 .000 | +--------------------------------------+--------+ 4. Promote winner \u00b6 Assert that the experiment completed and found a winning version. If the conditions are not satisfied, try again after a few more seconds. iter8ctl assert -c completed -c winnerFound Promote the winner as follows. helm upgrade my-app . \\ --install \\ --set baseline.dynamic.tag = 2 .0 \\ --set candidate = null Verify that baseline version is 2.0.0 # kill the port-forward commands from steps 1 and 2 # do this in a separate terminal kubectl port-forward svc/hello 8080 :8080 curl localhost:8080 # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv 5. Cleanup \u00b6 helm uninstall my-app Next Steps Use in production The Helm chart source for the Iter8 experiment is under $ITER8/helm/deploy . The Helm chart source for the sample application is under $ITER8/samples/first-exp/helm . Modify them as needed by your application for production usage. Try other Iter8 tutorials Iter8 can work in any K8s environment. Try Iter8 in the following environments. KFServing Seldon Knative Istio Linkerd","title":"Your first experiment"},{"location":"getting-started/first-experiment/#your-first-experiment","text":"Scenario: Safely rollout a Kubernetes deployment with SLO validation Dark launch a candidate version of your application (a K8s service and deployment), validate that the candidate satisfies latency and error-based objectives (SLOs) , and promote the candidate. Setup K8s cluster and local environment Get Helm 3+ . This tutorial uses the Helmex pattern Setup K8s cluster Install Iter8 in K8s cluster Get iter8ctl Clone the Iter8 GitHub repo . Set the ITER8 environment variable to the root of the cloned repo.","title":"Your First Experiment"},{"location":"getting-started/first-experiment/#1-create-baseline-version","text":"Deploy the baseline version of the hello world application using Helm. cd $ITER8 /samples/first-exp/helm helm dependency update helm install my-app . \\ --set baseline.dynamic.tag = 1 .0 \\ --set candidate = null Verify that baseline version is 1.0.0 # do this in a separate terminal kubectl port-forward svc/hello 8080 :8080 curl localhost:8080 # output will be similar to the following (notice 1.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 1.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"1. Create baseline version"},{"location":"getting-started/first-experiment/#2-create-candidate-version","text":"Deploy the candidate version of the hello world application using Helm. helm upgrade my-app . \\ --set baseline.dynamic.tag = 1 .0 \\ --set candidate.dynamic.tag = 2 .0 \\ --install The above command creates an Iter8 experiment alongside the candidate deployment of the hello world application. The experiment will collect latency and error rate metrics for the candidate, and verify that it satisfies the mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. View application and experiment resources Use the command below to view your application and Iter8 experiment resources. helm get manifest my-app Verify that candidate version is 2.0.0 # do this in a separate terminal kubectl port-forward svc/hello-candidate 8081 :8080 curl localhost:8081 # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"2. Create candidate version"},{"location":"getting-started/first-experiment/#3-observe-experiment","text":"Describe the results of the Iter8 experiment. Wait 20 seconds before trying the following command. If the output is not as expected, try again after a few more seconds. iter8ctl describe Experiment results will look similar to this ... ****** Overview ****** Experiment name: my-experiment Experiment namespace: default Target: my-app Testing pattern: Conformance Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Completed Number of completed iterations: 1 ****** Winner Assessment ****** > If the version being validated ; i.e., the baseline version, satisfies the experiment objectives, it is the winner. > Otherwise, there is no winner. Winning version: my-app ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------+--------+ | OBJECTIVE | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency < = | true | | 50 .000 | | +--------------------------------------+--------+ | iter8-system/error-rate < = | true | | 0 .000 | | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | true | | < = 100 .000 | | +--------------------------------------+--------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------+--------+ | METRIC | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency | 1 .233 | +--------------------------------------+--------+ | iter8-system/error-rate | 0 .000 | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | 2 .311 | +--------------------------------------+--------+ | iter8-system/request-count | 40 .000 | +--------------------------------------+--------+ | iter8-system/error-count | 0 .000 | +--------------------------------------+--------+","title":"3. Observe experiment"},{"location":"getting-started/first-experiment/#4-promote-winner","text":"Assert that the experiment completed and found a winning version. If the conditions are not satisfied, try again after a few more seconds. iter8ctl assert -c completed -c winnerFound Promote the winner as follows. helm upgrade my-app . \\ --install \\ --set baseline.dynamic.tag = 2 .0 \\ --set candidate = null Verify that baseline version is 2.0.0 # kill the port-forward commands from steps 1 and 2 # do this in a separate terminal kubectl port-forward svc/hello 8080 :8080 curl localhost:8080 # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"4. Promote winner"},{"location":"getting-started/first-experiment/#5-cleanup","text":"helm uninstall my-app Next Steps Use in production The Helm chart source for the Iter8 experiment is under $ITER8/helm/deploy . The Helm chart source for the sample application is under $ITER8/samples/first-exp/helm . Modify them as needed by your application for production usage. Try other Iter8 tutorials Iter8 can work in any K8s environment. Try Iter8 in the following environments. KFServing Seldon Knative Istio Linkerd","title":"5. Cleanup"},{"location":"getting-started/help/","text":"Get Help \u00b6 Read Iter8 docs . Join the Iter8 Slack workspace . File an issue or start a discussion on the Iter8 GitHub repo . Attend our weekly community meetings! Everyone is welcome to join our community meetings. Our community meetings are on Wednesdays from 11 AM to 12 PM EST/EDT. Video conference link for community meetings Agenda Meeting notes If you would like to discuss a topic, please list it on the agenda along with a time estimate. Community meetings are recorded and publicly available on our YouTube channel .","title":"Get help"},{"location":"getting-started/help/#get-help","text":"Read Iter8 docs . Join the Iter8 Slack workspace . File an issue or start a discussion on the Iter8 GitHub repo . Attend our weekly community meetings! Everyone is welcome to join our community meetings. Our community meetings are on Wednesdays from 11 AM to 12 PM EST/EDT. Video conference link for community meetings Agenda Meeting notes If you would like to discuss a topic, please list it on the agenda along with a time estimate. Community meetings are recorded and publicly available on our YouTube channel .","title":"Get Help"},{"location":"getting-started/install/","text":"Install Iter8 \u00b6 Install Iter8 in your Kubernetes cluster as follows. This step requires Kustomize v3+ . export TAG = v0.7.13 kustomize build https://github.com/iter8-tools/iter8/install/core/?ref = ${ TAG } | kubectl apply -f - kubectl wait crd -l creator = iter8 --for condition = established --timeout = 120s kustomize build https://github.com/iter8-tools/iter8/install/builtin-metrics/?ref = ${ TAG } | kubectl apply -f - kubectl wait --for = condition = Ready pods --all -n iter8-system Install iter8ctl \u00b6 Install iter8ctl CLI on your local machine as follows. This step requires Go 1.16+ . GO111MODULE = on GOBIN = /usr/local/bin go get github.com/iter8-tools/iter8ctl@v0.1.5","title":"Install Iter8"},{"location":"getting-started/install/#install-iter8","text":"Install Iter8 in your Kubernetes cluster as follows. This step requires Kustomize v3+ . export TAG = v0.7.13 kustomize build https://github.com/iter8-tools/iter8/install/core/?ref = ${ TAG } | kubectl apply -f - kubectl wait crd -l creator = iter8 --for condition = established --timeout = 120s kustomize build https://github.com/iter8-tools/iter8/install/builtin-metrics/?ref = ${ TAG } | kubectl apply -f - kubectl wait --for = condition = Ready pods --all -n iter8-system","title":"Install Iter8"},{"location":"getting-started/install/#install-iter8ctl","text":"Install iter8ctl CLI on your local machine as follows. This step requires Go 1.16+ . GO111MODULE = on GOBIN = /usr/local/bin go get github.com/iter8-tools/iter8ctl@v0.1.5","title":"Install iter8ctl"},{"location":"getting-started/setup-for-tutorials/","text":"Setup for Tutorials \u00b6 Setup common components that are required in Iter8 tutorials. Local Kubernetes cluster \u00b6 Use a managed K8s cluster or a local K8s cluster for running Iter8 tutorials. You can setup the latter using Kind or Minikube as follows. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Minikube minikube start Setting CPU and memory resources for your local K8s cluster Iter8 tutorials in certain K8s environments, especially those involving Istio, require additional CPU and memory resources. You can set this as follows. Kind A Kind cluster inherits its CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as follows. Minikube Set CPU and memory resources while starting Minikube as follows. minikube start --cpus 8 --memory 12288 Iter8 GitHub repo \u00b6 Clone the Iter8 GitHub repo and set the ITER8 environment variable as follows: git clone https://github.com/iter8-tools/iter8.git export ITER8 = ./iter8 Iter8 Helm repo \u00b6 Iter8 Helm repo contains charts that support Iter8's Helmex tutorials. Get the repo as follows: helm repo add iter8 https://iter8-tools.github.io/iter8/ --force-update","title":"Setup for tutorials"},{"location":"getting-started/setup-for-tutorials/#setup-for-tutorials","text":"Setup common components that are required in Iter8 tutorials.","title":"Setup for Tutorials"},{"location":"getting-started/setup-for-tutorials/#local-kubernetes-cluster","text":"Use a managed K8s cluster or a local K8s cluster for running Iter8 tutorials. You can setup the latter using Kind or Minikube as follows. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Minikube minikube start Setting CPU and memory resources for your local K8s cluster Iter8 tutorials in certain K8s environments, especially those involving Istio, require additional CPU and memory resources. You can set this as follows. Kind A Kind cluster inherits its CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as follows. Minikube Set CPU and memory resources while starting Minikube as follows. minikube start --cpus 8 --memory 12288","title":"Local Kubernetes cluster"},{"location":"getting-started/setup-for-tutorials/#iter8-github-repo","text":"Clone the Iter8 GitHub repo and set the ITER8 environment variable as follows: git clone https://github.com/iter8-tools/iter8.git export ITER8 = ./iter8","title":"Iter8 GitHub repo"},{"location":"getting-started/setup-for-tutorials/#iter8-helm-repo","text":"Iter8 Helm repo contains charts that support Iter8's Helmex tutorials. Get the repo as follows: helm repo add iter8 https://iter8-tools.github.io/iter8/ --force-update","title":"Iter8 Helm repo"},{"location":"metrics/builtin/","text":"Built-in Metrics \u00b6 Built-in latency/error metrics Iter8 ships with a set of nine built-in metrics that measure your application's performance in terms of latency and errors. You can collect and use these metrics in experiments without the need to configure any external databases. This feature enables you to get started with Iter8 experiments, especially, SLO validation experiments, quickly. As part of metrics collection, Iter8 will also generate HTTP requests to the application end-point. List of built-in metrics \u00b6 The following are the set of built-in Iter8 metrics. Namespace Name Type Description iter8-system request-count Counter Number of requests iter8-system error-count Gauge Number of responses with HTTP status code 4xx or 5xx iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx iter8-system mean-latency Gauge Mean response latency iter8-system latency-50 th -percentile Gauge 50 th percentile (median) response latency iter8-system latency-75 th -percentile Gauge 75 th percentile response latency iter8-system latency-90 th -percentile Gauge 90 th percentile response latency iter8-system latency-95 th -percentile Gauge 95 th percentile response latency iter8-system latency-99 th -percentile Gauge 99 th percentile response latency Collecting built-in metrics \u00b6 Use the metrics/collect task in an experiment to collect built-in metrics for your app/ML model versions. Example \u00b6 For an example of an experiment that uses built-in metrics, look inside the Knative experiment in this tutorial .","title":"Builtin metrics"},{"location":"metrics/builtin/#built-in-metrics","text":"Built-in latency/error metrics Iter8 ships with a set of nine built-in metrics that measure your application's performance in terms of latency and errors. You can collect and use these metrics in experiments without the need to configure any external databases. This feature enables you to get started with Iter8 experiments, especially, SLO validation experiments, quickly. As part of metrics collection, Iter8 will also generate HTTP requests to the application end-point.","title":"Built-in Metrics"},{"location":"metrics/builtin/#list-of-built-in-metrics","text":"The following are the set of built-in Iter8 metrics. Namespace Name Type Description iter8-system request-count Counter Number of requests iter8-system error-count Gauge Number of responses with HTTP status code 4xx or 5xx iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx iter8-system mean-latency Gauge Mean response latency iter8-system latency-50 th -percentile Gauge 50 th percentile (median) response latency iter8-system latency-75 th -percentile Gauge 75 th percentile response latency iter8-system latency-90 th -percentile Gauge 90 th percentile response latency iter8-system latency-95 th -percentile Gauge 95 th percentile response latency iter8-system latency-99 th -percentile Gauge 99 th percentile response latency","title":"List of built-in metrics"},{"location":"metrics/builtin/#collecting-built-in-metrics","text":"Use the metrics/collect task in an experiment to collect built-in metrics for your app/ML model versions.","title":"Collecting built-in metrics"},{"location":"metrics/builtin/#example","text":"For an example of an experiment that uses built-in metrics, look inside the Knative experiment in this tutorial .","title":"Example"},{"location":"metrics/custom/","text":"Custom Metrics \u00b6 Custom Iter8 metrics enable you to use data from any database for evaluating app/ML model versions within Iter8 experiments. This document describes how you can define custom Iter8 metrics and (optionally) supply authentication information that may be required by the metrics provider. Metric providers differ in the following aspects. HTTP request authentication method: no authentication, basic auth, API keys, or bearer token HTTP request method: GET or POST Format of HTTP parameters and/or JSON body used while querying them Format of the JSON response returned by the provider The logic used by Iter8 to extract the metric value from the JSON response The examples in this document focus on Prometheus, NewRelic, Sysdig, and Elastic. However, the principles illustrated here will enable you to use metrics from any provider in experiments. Metrics with/without auth \u00b6 Note: Metrics are defined by you, the Iter8 end-user . Prometheus Prometheus does not support any authentication mechanism out-of-the-box . However, Prometheus can be setup in conjunction with a reverse proxy, which in turn can support HTTP request authentication, as described here . No Authentication The following is an example of an Iter8 metric with Prometheus as the provider. This example assumes that Prometheus can be queried by Iter8 without any authentication. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://myprometheusservice.com/api/v1 Basic auth Suppose Prometheus is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can enable Iter8 to query this Prometheus instance as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic promcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter authType : Basic secret : myns/promcredentials jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : https://my.secure.prometheus.service.com/api/v1 Brief explanation of the request-count metric Prometheus enables metric queries using HTTP GET requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of request-count , and defaulted to GET . Iter8 will query Prometheus during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named query as required by Prometheus . The value of this parameter is derived by substituting the placeholders in the value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Prometheus. The urlTemplate field provides the URL of the prometheus service. New Relic New Relic uses API Keys to authenticate requests as documented here . The API key may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. API key embedded in metric The following is an example of an Iter8 metric with Prometheus as the provider. In this example, t0p-secret-api-key is the New Relic API key. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter headerTemplates : - name : X-Query-Key value : t0p-secret-api-key jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id API key embedded in secret Suppose your New Relic API key is t0p-secret-api-key ; you wish to store this API key in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the API key. kubectl create secret generic nrcredentials -n myns --from-literal = mykey = t0p-secret-api-key The above secret contains a data field named mykey whose value is the API key. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics = myns Define metric: When defining the metric, ensure that the authType field is set to APIKey and the appropriate secret is referenced. In the headerTemplates field, include X-Query-Key as the name of a header field (as required by New Relic ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${mykey} at query time, by looking up the referenced secret named nrcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter authType : APIKey secret : myns/nrcredentials headerTemplates : - name : X-Query-Key value : ${mykey} jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id Brief explanation of the name-count metric New Relic enables metric queries using both HTTP GET or POST requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of name-count , and defaulted to GET . Iter8 will query New Relic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named nrql as required by New Relic . The value of this parameter is derived by substituting the placeholders in its value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by New Relic. The urlTemplate field provides the URL of the New Relic service. Sysdig Sysdig data API accepts HTTP POST requests and uses a bearer token for authentication as documented here . The bearer token may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. Bearer token embedded in metric The following is an example of an Iter8 metric with Sysdig as the provider. In this example, 87654321-1234-1234-1234-123456789012 is the Sysdig bearer token (also referred to as access key by Sysdig). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer 87654321-1234-1234-1234-123456789012 jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Bearer token embedded in secret Suppose your Sysdig token is 87654321-1234-1234-1234-123456789012 ; you wish to store this token in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the token. kubectl create secret generic sdcredentials -n myns --from-literal = token = 87654321 -1234-1234-1234-123456789012 The above secret contains a data field named token whose value is the Sysdig token. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Bearer and the appropriate secret is referenced. In the headerTemplates field, include Authorize header field (as required by Sysdig ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${token} at query time, by looking up the referenced secret named sdcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST authType : Bearer secret : myns/sdcredentials type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer ${token} jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Brief explanation of the cpu-utilization metric Sysdig enables metric queries using both POST requests; hence, the method field of the Iter8 metric is set to POST. Iter8 will query Sysdig during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Sysdig . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Sysdig. The urlTemplate field provides the URL of the Sysdig service. Elastic Elasticsearch REST API accepts HTTP GET or POST requests and uses basic authentication as documented here . Suppose Elasticsearch is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can then enable Iter8 to query the Elasticsearch service as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic elasticcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : average-sales spec : description : An elastic example provider : elastic body : >- { \"aggs\": { \"range\": { \"date_range\": { \"field\": \"date\", \"ranges\": [ { \"from\": \"now-${elapsedTime}s/s\" } ] } }, \"items_to_sell\": { \"filter\": { \"term\": { \"version\": \"${revision}\" } }, \"aggs\": { \"avg_sales\": { \"avg\": { \"field\": \"sale_price\" } } } } } } method : POST authType : Basic secret : myns/elasticcredentials type : Gauge headerTemplates : - name : Content-Type value : application/json jqExpression : \".aggregations.items_to_sell.avg_sales.value | tonumber\" urlTemplate : https://secure.elastic.com/my/sales Brief explanation of the average sales metric Elastic enables metric queries using GET or POST requests. In the elastic example, The method field of the Iter8 metric is set to POST. Iter8 will query Elastic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Elastic . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Elastic. The urlTemplate field provides the URL of the Elastic service. Placeholder substitution \u00b6 Note: This step is automated by Iter8 . Iter8 will substitute placeholders in the metric query based on the time elapsed since the start of the experiment, and information associated with each version in the experiment. Suppose the metrics defined above are referenced within an experiment as follows. Further, suppose this experiment has started, Iter8 is about to do an iteration of this experiment, and the time elapsed since the start of the experiment is 600 seconds. Look inside sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : sample-exp spec : target : default/sample-app strategy : testingPattern : Canary criteria : # This experiment assumes that metrics have been created in the `myns` namespace requestCount : myns/request-count objectives : - metric : myns/name-count lowerLimit : 50 - metric : myns/cpu-utilization upperLimit : 90 - metric : myns/average-sales lowerLimit : \"250.0\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : userfilter value : 'usergroup!~\"wakanda\"' candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : userfilter value : 'usergroup=~\"wakanda\"' For the sample experiment above, Iter8 will use two HTTP(S) queries to fetch metric values, one for the baseline version, and another for the candidate version. Prometheus Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named query whose value equals: sum(increase(revision_app_request_latencies_count{service_name='current',usergroup!~\"wakanda\"}[600s])) or on() vector(0) New Relic Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named nrql whose value equals: SELECT count(appName) FROM PageView WHERE revisionName='sample-app-v1' SINCE 600 seconds ago Sysdig Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"last\" : 600 , \"sampling\" : 600 , \"filter\" : \"kubernetes.app.revision.name = 'sample-app-v1'\" , \"metrics\" : [ { \"id\" : \"cpu.cores.used\" , \"aggregations\" : { \"time\" : \"avg\" , \"group\" : \"sum\" } } ], \"dataSourceType\" : \"container\" , \"paging\" : { \"from\" : 0 , \"to\" : 99 } } Elastic Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"aggs\" : { \"range\" : { \"date_range\" : { \"field\" : \"date\" , \"ranges\" : [ { \"from\" : \"now-600s/s\" } ] } }, \"items_to_sell\" : { \"filter\" : { \"term\" : { \"version\" : \"sample-app-v1\" } }, \"aggs\" : { \"avg_sales\" : { \"avg\" : { \"field\" : \"sale_price\" } } } } } } The placeholder $elapsedTime has been substituted with 600, which is the time elapsed since the start of the experiment. The other placeholders have been substituted based on the versionInfo field of the baseline version in the experiment. Iter8 builds and sends an HTTP request in a similar manner for the candidate version as well. JSON response \u00b6 Note: This step is handled by the metrics provider . The metrics provider is expected to respond to Iter8's HTTP request with a JSON object. The format of this JSON object is defined by the provider. Prometheus The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } } New Relic The format of the New Relic JSON response is discussed here . A sample New Relic response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"results\" : [ { \"count\" : 80275388 } ], \"metadata\" : { \"eventTypes\" : [ \"PageView\" ], \"eventType\" : \"PageView\" , \"openEnded\" : true , \"beginTime\" : \"2014-08-03T19:00:00Z\" , \"endTime\" : \"2017-01-18T23:18:41Z\" , \"beginTimeMillis=\" : 1407092400000 , \"endTimeMillis\" : 1484781521198 , \"rawSince\" : \"'2014-08-04 00:00:00+0500'\" , \"rawUntil\" : \"`now`\" , \"rawCompareWith\" : \"\" , \"clippedTimeWindows\" : { \"Browser\" : { \"beginTimeMillis\" : 1483571921198 , \"endTimeMillis\" : 1484781521198 , \"retentionMillis\" : 1209600000 } }, \"messages\" : [], \"contents\" : [ { \"function\" : \"count\" , \"attribute\" : \"appName\" , \"simple\" : true } ] } } Sysdig The format of the Sysdig JSON response is discussed here . A sample Sysdig response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\" : [ { \"t\" : 1582756200 , \"d\" : [ 6.481 ] } ], \"start\" : 1582755600 , \"end\" : 1582756200 } Elastic The format of the Elastic JSON response is discussed here . A sample Elastic response is as follows. 1 2 3 4 5 6 7 8 { \"aggregations\" : { \"items_to_sell\" : { \"doc_count\" : 3 , \"avg_sales\" : { \"value\" : 128.33333333333334 } } } } Processing the JSON response \u00b6 Note: This step is automated by Iter8 . Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. New Relic Consider the jqExpression defined in the sample New Relic metric . Let us apply it to the sample JSON response from New Relic . echo '{ \"results\": [ { \"count\": 80275388 } ], \"metadata\": { \"eventTypes\": [ \"PageView\" ], \"eventType\": \"PageView\", \"openEnded\": true, \"beginTime\": \"2014-08-03T19:00:00Z\", \"endTime\": \"2017-01-18T23:18:41Z\", \"beginTimeMillis=\": 1407092400000, \"endTimeMillis\": 1484781521198, \"rawSince\": \"' 2014 -08-04 00 :00:00+0500 '\", \"rawUntil\": \"`now`\", \"rawCompareWith\": \"\", \"clippedTimeWindows\": { \"Browser\": { \"beginTimeMillis\": 1483571921198, \"endTimeMillis\": 1484781521198, \"retentionMillis\": 1209600000 } }, \"messages\": [], \"contents\": [ { \"function\": \"count\", \"attribute\": \"appName\", \"simple\": true } ] } }' | jq \".results[0].count | tonumber\" Executing the above command results yields 80275388 , a number, as required by Iter8. Sysdig Consider the jqExpression defined in the sample Sysdig metric . Let us apply it to the sample JSON response from Sysdig . echo '{ \"data\": [ { \"t\": 1582756200, \"d\": [ 6.481 ] } ], \"start\": 1582755600, \"end\": 1582756200 }' | jq \".data[0].d[0] | tonumber\" Executing the above command results yields 6.481 , a number, as required by Iter8. Elastic Consider the jqExpression defined in the sample Elastic metric . Let us apply it to the sample JSON response from Elastic . echo '{ \"aggregations\": { \"items_to_sell\": { \"doc_count\": 3, \"avg_sales\": { \"value\": 128.33333333333334 } } } }' | jq \".aggregations.items_to_sell.avg_sales.value | tonumber\" Executing the above command results yields 128.33333333333334 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression . Error handling \u00b6 Note: This step is automated by Iter8 . Errors may occur during Iter8's metric queries due to a number of reasons (for example, due to an invalid jqExpression supplied within the metric). If Iter8 encounters errors during its attempt to retrieve metric values, Iter8 will mark the respective metric as unavailable. Iter8 can be used with any provider that can receive an HTTP request and respond with a JSON object containing the metrics information. Documentation requests and contributions (PRs) are welcome for providers not listed here. \u21a9 In a conformance experiment, n = 1 . In canary and A/B experiments, n = 2 . In A/B/n experiments, n > 2 . \u21a9 \u21a9 \u21a9 \u21a9","title":"Custom metrics"},{"location":"metrics/custom/#custom-metrics","text":"Custom Iter8 metrics enable you to use data from any database for evaluating app/ML model versions within Iter8 experiments. This document describes how you can define custom Iter8 metrics and (optionally) supply authentication information that may be required by the metrics provider. Metric providers differ in the following aspects. HTTP request authentication method: no authentication, basic auth, API keys, or bearer token HTTP request method: GET or POST Format of HTTP parameters and/or JSON body used while querying them Format of the JSON response returned by the provider The logic used by Iter8 to extract the metric value from the JSON response The examples in this document focus on Prometheus, NewRelic, Sysdig, and Elastic. However, the principles illustrated here will enable you to use metrics from any provider in experiments.","title":"Custom Metrics"},{"location":"metrics/custom/#metrics-withwithout-auth","text":"Note: Metrics are defined by you, the Iter8 end-user . Prometheus Prometheus does not support any authentication mechanism out-of-the-box . However, Prometheus can be setup in conjunction with a reverse proxy, which in turn can support HTTP request authentication, as described here . No Authentication The following is an example of an Iter8 metric with Prometheus as the provider. This example assumes that Prometheus can be queried by Iter8 without any authentication. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://myprometheusservice.com/api/v1 Basic auth Suppose Prometheus is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can enable Iter8 to query this Prometheus instance as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic promcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter authType : Basic secret : myns/promcredentials jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : https://my.secure.prometheus.service.com/api/v1 Brief explanation of the request-count metric Prometheus enables metric queries using HTTP GET requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of request-count , and defaulted to GET . Iter8 will query Prometheus during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named query as required by Prometheus . The value of this parameter is derived by substituting the placeholders in the value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Prometheus. The urlTemplate field provides the URL of the prometheus service. New Relic New Relic uses API Keys to authenticate requests as documented here . The API key may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. API key embedded in metric The following is an example of an Iter8 metric with Prometheus as the provider. In this example, t0p-secret-api-key is the New Relic API key. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter headerTemplates : - name : X-Query-Key value : t0p-secret-api-key jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id API key embedded in secret Suppose your New Relic API key is t0p-secret-api-key ; you wish to store this API key in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the API key. kubectl create secret generic nrcredentials -n myns --from-literal = mykey = t0p-secret-api-key The above secret contains a data field named mykey whose value is the API key. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics = myns Define metric: When defining the metric, ensure that the authType field is set to APIKey and the appropriate secret is referenced. In the headerTemplates field, include X-Query-Key as the name of a header field (as required by New Relic ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${mykey} at query time, by looking up the referenced secret named nrcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter authType : APIKey secret : myns/nrcredentials headerTemplates : - name : X-Query-Key value : ${mykey} jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id Brief explanation of the name-count metric New Relic enables metric queries using both HTTP GET or POST requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of name-count , and defaulted to GET . Iter8 will query New Relic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named nrql as required by New Relic . The value of this parameter is derived by substituting the placeholders in its value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by New Relic. The urlTemplate field provides the URL of the New Relic service. Sysdig Sysdig data API accepts HTTP POST requests and uses a bearer token for authentication as documented here . The bearer token may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. Bearer token embedded in metric The following is an example of an Iter8 metric with Sysdig as the provider. In this example, 87654321-1234-1234-1234-123456789012 is the Sysdig bearer token (also referred to as access key by Sysdig). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer 87654321-1234-1234-1234-123456789012 jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Bearer token embedded in secret Suppose your Sysdig token is 87654321-1234-1234-1234-123456789012 ; you wish to store this token in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the token. kubectl create secret generic sdcredentials -n myns --from-literal = token = 87654321 -1234-1234-1234-123456789012 The above secret contains a data field named token whose value is the Sysdig token. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Bearer and the appropriate secret is referenced. In the headerTemplates field, include Authorize header field (as required by Sysdig ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${token} at query time, by looking up the referenced secret named sdcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST authType : Bearer secret : myns/sdcredentials type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer ${token} jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Brief explanation of the cpu-utilization metric Sysdig enables metric queries using both POST requests; hence, the method field of the Iter8 metric is set to POST. Iter8 will query Sysdig during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Sysdig . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Sysdig. The urlTemplate field provides the URL of the Sysdig service. Elastic Elasticsearch REST API accepts HTTP GET or POST requests and uses basic authentication as documented here . Suppose Elasticsearch is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can then enable Iter8 to query the Elasticsearch service as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic elasticcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : average-sales spec : description : An elastic example provider : elastic body : >- { \"aggs\": { \"range\": { \"date_range\": { \"field\": \"date\", \"ranges\": [ { \"from\": \"now-${elapsedTime}s/s\" } ] } }, \"items_to_sell\": { \"filter\": { \"term\": { \"version\": \"${revision}\" } }, \"aggs\": { \"avg_sales\": { \"avg\": { \"field\": \"sale_price\" } } } } } } method : POST authType : Basic secret : myns/elasticcredentials type : Gauge headerTemplates : - name : Content-Type value : application/json jqExpression : \".aggregations.items_to_sell.avg_sales.value | tonumber\" urlTemplate : https://secure.elastic.com/my/sales Brief explanation of the average sales metric Elastic enables metric queries using GET or POST requests. In the elastic example, The method field of the Iter8 metric is set to POST. Iter8 will query Elastic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Elastic . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Elastic. The urlTemplate field provides the URL of the Elastic service.","title":"Metrics with/without auth"},{"location":"metrics/custom/#placeholder-substitution","text":"Note: This step is automated by Iter8 . Iter8 will substitute placeholders in the metric query based on the time elapsed since the start of the experiment, and information associated with each version in the experiment. Suppose the metrics defined above are referenced within an experiment as follows. Further, suppose this experiment has started, Iter8 is about to do an iteration of this experiment, and the time elapsed since the start of the experiment is 600 seconds. Look inside sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : sample-exp spec : target : default/sample-app strategy : testingPattern : Canary criteria : # This experiment assumes that metrics have been created in the `myns` namespace requestCount : myns/request-count objectives : - metric : myns/name-count lowerLimit : 50 - metric : myns/cpu-utilization upperLimit : 90 - metric : myns/average-sales lowerLimit : \"250.0\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : userfilter value : 'usergroup!~\"wakanda\"' candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : userfilter value : 'usergroup=~\"wakanda\"' For the sample experiment above, Iter8 will use two HTTP(S) queries to fetch metric values, one for the baseline version, and another for the candidate version. Prometheus Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named query whose value equals: sum(increase(revision_app_request_latencies_count{service_name='current',usergroup!~\"wakanda\"}[600s])) or on() vector(0) New Relic Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named nrql whose value equals: SELECT count(appName) FROM PageView WHERE revisionName='sample-app-v1' SINCE 600 seconds ago Sysdig Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"last\" : 600 , \"sampling\" : 600 , \"filter\" : \"kubernetes.app.revision.name = 'sample-app-v1'\" , \"metrics\" : [ { \"id\" : \"cpu.cores.used\" , \"aggregations\" : { \"time\" : \"avg\" , \"group\" : \"sum\" } } ], \"dataSourceType\" : \"container\" , \"paging\" : { \"from\" : 0 , \"to\" : 99 } } Elastic Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"aggs\" : { \"range\" : { \"date_range\" : { \"field\" : \"date\" , \"ranges\" : [ { \"from\" : \"now-600s/s\" } ] } }, \"items_to_sell\" : { \"filter\" : { \"term\" : { \"version\" : \"sample-app-v1\" } }, \"aggs\" : { \"avg_sales\" : { \"avg\" : { \"field\" : \"sale_price\" } } } } } } The placeholder $elapsedTime has been substituted with 600, which is the time elapsed since the start of the experiment. The other placeholders have been substituted based on the versionInfo field of the baseline version in the experiment. Iter8 builds and sends an HTTP request in a similar manner for the candidate version as well.","title":"Placeholder substitution"},{"location":"metrics/custom/#json-response","text":"Note: This step is handled by the metrics provider . The metrics provider is expected to respond to Iter8's HTTP request with a JSON object. The format of this JSON object is defined by the provider. Prometheus The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } } New Relic The format of the New Relic JSON response is discussed here . A sample New Relic response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"results\" : [ { \"count\" : 80275388 } ], \"metadata\" : { \"eventTypes\" : [ \"PageView\" ], \"eventType\" : \"PageView\" , \"openEnded\" : true , \"beginTime\" : \"2014-08-03T19:00:00Z\" , \"endTime\" : \"2017-01-18T23:18:41Z\" , \"beginTimeMillis=\" : 1407092400000 , \"endTimeMillis\" : 1484781521198 , \"rawSince\" : \"'2014-08-04 00:00:00+0500'\" , \"rawUntil\" : \"`now`\" , \"rawCompareWith\" : \"\" , \"clippedTimeWindows\" : { \"Browser\" : { \"beginTimeMillis\" : 1483571921198 , \"endTimeMillis\" : 1484781521198 , \"retentionMillis\" : 1209600000 } }, \"messages\" : [], \"contents\" : [ { \"function\" : \"count\" , \"attribute\" : \"appName\" , \"simple\" : true } ] } } Sysdig The format of the Sysdig JSON response is discussed here . A sample Sysdig response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\" : [ { \"t\" : 1582756200 , \"d\" : [ 6.481 ] } ], \"start\" : 1582755600 , \"end\" : 1582756200 } Elastic The format of the Elastic JSON response is discussed here . A sample Elastic response is as follows. 1 2 3 4 5 6 7 8 { \"aggregations\" : { \"items_to_sell\" : { \"doc_count\" : 3 , \"avg_sales\" : { \"value\" : 128.33333333333334 } } } }","title":"JSON response"},{"location":"metrics/custom/#processing-the-json-response","text":"Note: This step is automated by Iter8 . Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. New Relic Consider the jqExpression defined in the sample New Relic metric . Let us apply it to the sample JSON response from New Relic . echo '{ \"results\": [ { \"count\": 80275388 } ], \"metadata\": { \"eventTypes\": [ \"PageView\" ], \"eventType\": \"PageView\", \"openEnded\": true, \"beginTime\": \"2014-08-03T19:00:00Z\", \"endTime\": \"2017-01-18T23:18:41Z\", \"beginTimeMillis=\": 1407092400000, \"endTimeMillis\": 1484781521198, \"rawSince\": \"' 2014 -08-04 00 :00:00+0500 '\", \"rawUntil\": \"`now`\", \"rawCompareWith\": \"\", \"clippedTimeWindows\": { \"Browser\": { \"beginTimeMillis\": 1483571921198, \"endTimeMillis\": 1484781521198, \"retentionMillis\": 1209600000 } }, \"messages\": [], \"contents\": [ { \"function\": \"count\", \"attribute\": \"appName\", \"simple\": true } ] } }' | jq \".results[0].count | tonumber\" Executing the above command results yields 80275388 , a number, as required by Iter8. Sysdig Consider the jqExpression defined in the sample Sysdig metric . Let us apply it to the sample JSON response from Sysdig . echo '{ \"data\": [ { \"t\": 1582756200, \"d\": [ 6.481 ] } ], \"start\": 1582755600, \"end\": 1582756200 }' | jq \".data[0].d[0] | tonumber\" Executing the above command results yields 6.481 , a number, as required by Iter8. Elastic Consider the jqExpression defined in the sample Elastic metric . Let us apply it to the sample JSON response from Elastic . echo '{ \"aggregations\": { \"items_to_sell\": { \"doc_count\": 3, \"avg_sales\": { \"value\": 128.33333333333334 } } } }' | jq \".aggregations.items_to_sell.avg_sales.value | tonumber\" Executing the above command results yields 128.33333333333334 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression .","title":"Processing the JSON response"},{"location":"metrics/custom/#error-handling","text":"Note: This step is automated by Iter8 . Errors may occur during Iter8's metric queries due to a number of reasons (for example, due to an invalid jqExpression supplied within the metric). If Iter8 encounters errors during its attempt to retrieve metric values, Iter8 will mark the respective metric as unavailable. Iter8 can be used with any provider that can receive an HTTP request and respond with a JSON object containing the metrics information. Documentation requests and contributions (PRs) are welcome for providers not listed here. \u21a9 In a conformance experiment, n = 1 . In canary and A/B experiments, n = 2 . In A/B/n experiments, n > 2 . \u21a9 \u21a9 \u21a9 \u21a9","title":"Error handling"},{"location":"metrics/mock/","text":"Metrics mock \u00b6 Mocking the value of a metric Iter8 enables you to mock the values of a metric. This is useful for learning purposes and quickly trying out sample Iter8 experiments without having to set up metric databases. Examples \u00b6 1 2 3 4 5 6 7 8 9 10 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement spec : mock : - name : default level : \"20.0\" - name : canary level : \"15.0\" 1 2 3 4 5 6 7 8 9 10 11 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : type : Counter mock : - name : default level : \"20.0\" - name : canary level : \"15.0\" Explanation \u00b6 When the mock field is present within a metric spec, Iter8 will mock the values for this metric. The name field refers to the name of the version. Version names should be unique. Version name should match the name of a version in the experiment's versionInfo section. If not, any value generated for the non-matching name will be ignored. You can mock both Counter and Gauge metrics. The semantics of level field are as follows: If the metric is a counter, level is x , and time elapsed since the start of the experiment is y seconds, then xy is the metric value. Note that the (mocked) metric value will keep increasing over time. If the metric is gauge, if level is x , the metric value is a random value with mean x . The expected value of the (mocked) metric will be x but its observed value may increase or decrease over time.","title":"Mock metrics"},{"location":"metrics/mock/#metrics-mock","text":"Mocking the value of a metric Iter8 enables you to mock the values of a metric. This is useful for learning purposes and quickly trying out sample Iter8 experiments without having to set up metric databases.","title":"Metrics mock"},{"location":"metrics/mock/#examples","text":"1 2 3 4 5 6 7 8 9 10 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement spec : mock : - name : default level : \"20.0\" - name : canary level : \"15.0\" 1 2 3 4 5 6 7 8 9 10 11 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : type : Counter mock : - name : default level : \"20.0\" - name : canary level : \"15.0\"","title":"Examples"},{"location":"metrics/mock/#explanation","text":"When the mock field is present within a metric spec, Iter8 will mock the values for this metric. The name field refers to the name of the version. Version names should be unique. Version name should match the name of a version in the experiment's versionInfo section. If not, any value generated for the non-matching name will be ignored. You can mock both Counter and Gauge metrics. The semantics of level field are as follows: If the metric is a counter, level is x , and time elapsed since the start of the experiment is y seconds, then xy is the metric value. Note that the (mocked) metric value will keep increasing over time. If the metric is gauge, if level is x , the metric value is a random value with mean x . The expected value of the (mocked) metric will be x but its observed value may increase or decrease over time.","title":"Explanation"},{"location":"metrics/using-metrics/","text":"Using Metrics in Experiments \u00b6 Iter8 metric resources Iter8 defines a custom Kubernetes resource (CRD) called Metric that makes it easy to define and use metrics in experiments. Iter8 installation includes a set of pre-defined built-in metrics that pertain to app/ML model latency/errors. You can also define custom metrics that enable you to utilize data from Prometheus, New Relic, Sysdig, Elastic or any other database of your choice. List metrics \u00b6 Find the set Iter8 metrics available in your cluster using kubectl get . kubectl get metrics.iter8.tools --all-namespaces NAMESPACE NAME TYPE DESCRIPTION iter8-kfserving user-engagement Gauge Average duration of a session iter8-system error-count Counter Number of responses with HTTP status code 4xx or 5xx ( Iter8 built-in metric ) iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx ( Iter8 built-in metric ) iter8-system latency-50th-percentile Gauge 50th percentile ( median ) latency ( Iter8 built-in metric ) iter8-system latency-75th-percentile Gauge 75th percentile latency ( Iter8 built-in metric ) iter8-system latency-90th-percentile Gauge 90th percentile latency ( Iter8 built-in metric ) iter8-system latency-95th-percentile Gauge 95th percentile latency ( Iter8 built-in metric ) iter8-system latency-99th-percentile Gauge 99th percentile latency ( Iter8 built-in metric ) iter8-system mean-latency Gauge Mean latency ( Iter8 built-in metric ) iter8-system request-count Counter Number of requests ( Iter8 built-in metric ) Referencing metrics within experiments \u00b6 Use metrics in experiments by referencing them in the criteria section of the experiment manifest. Reference metrics using the namespace/name or name format . Sample experiment illustrating the use of metrics kind : Experiment ... spec : ... criteria : requestCount : iter8-knative/request-count # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" Observing metric values \u00b6 During an experiment, Iter8 reports the metric values observed for each version. Use iter8ctl to observe these metric values in realtime. See here for an example.","title":"Using metrics"},{"location":"metrics/using-metrics/#using-metrics-in-experiments","text":"Iter8 metric resources Iter8 defines a custom Kubernetes resource (CRD) called Metric that makes it easy to define and use metrics in experiments. Iter8 installation includes a set of pre-defined built-in metrics that pertain to app/ML model latency/errors. You can also define custom metrics that enable you to utilize data from Prometheus, New Relic, Sysdig, Elastic or any other database of your choice.","title":"Using Metrics in Experiments"},{"location":"metrics/using-metrics/#list-metrics","text":"Find the set Iter8 metrics available in your cluster using kubectl get . kubectl get metrics.iter8.tools --all-namespaces NAMESPACE NAME TYPE DESCRIPTION iter8-kfserving user-engagement Gauge Average duration of a session iter8-system error-count Counter Number of responses with HTTP status code 4xx or 5xx ( Iter8 built-in metric ) iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx ( Iter8 built-in metric ) iter8-system latency-50th-percentile Gauge 50th percentile ( median ) latency ( Iter8 built-in metric ) iter8-system latency-75th-percentile Gauge 75th percentile latency ( Iter8 built-in metric ) iter8-system latency-90th-percentile Gauge 90th percentile latency ( Iter8 built-in metric ) iter8-system latency-95th-percentile Gauge 95th percentile latency ( Iter8 built-in metric ) iter8-system latency-99th-percentile Gauge 99th percentile latency ( Iter8 built-in metric ) iter8-system mean-latency Gauge Mean latency ( Iter8 built-in metric ) iter8-system request-count Counter Number of requests ( Iter8 built-in metric )","title":"List metrics"},{"location":"metrics/using-metrics/#referencing-metrics-within-experiments","text":"Use metrics in experiments by referencing them in the criteria section of the experiment manifest. Reference metrics using the namespace/name or name format . Sample experiment illustrating the use of metrics kind : Experiment ... spec : ... criteria : requestCount : iter8-knative/request-count # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\"","title":"Referencing metrics within experiments"},{"location":"metrics/using-metrics/#observing-metric-values","text":"During an experiment, Iter8 reports the metric values observed for each version. Use iter8ctl to observe these metric values in realtime. See here for an example.","title":"Observing metric values"},{"location":"reference/experiment/","text":"Experiment Resource \u00b6 Experiment resource Iter8's Experiment resource type enables application developers and service operators to automate A/B, A/B/n, Canary and Conformance experiments for Kubernetes apps/ML models. The controls provided by the experiment resource type encompass testing, deployment, traffic engineering, and version promotion functions . Sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-knative/request-count rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate Version This document describes version v2alpha2 of Iter8's experiment API. Metadata \u00b6 Standard Kubernetes meta.v1/ObjectMeta resource. Spec \u00b6 Field name Field type Description Required target string Identifies the app under experimentation and determines which experiments can run concurrently. Experiments that have the same target value will not be scheduled concurrently but will be run sequentially in the order of their creation timestamps. Experiments whose target values differ from each other can be scheduled by Iter8 concurrently. Yes strategy Strategy The experimentation strategy which specifies how app versions are tested, how traffic is shifted during experiment, and what tasks are executed at the start and end of the experiment. Yes criteria Criteria Criteria used for evaluating versions. This section includes (business) rewards, service-level objectives (SLOs) and indicators (SLIs). No duration Duration Duration of the experiment. No versionInfo VersionInfo Versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates. No Status \u00b6 Field name Field type Description Required conditions [] ExperimentCondition A set of conditions that express progress of an experiment. No initTime metav1.Time The time the experiment is created. No startTime metav1.Time The time when the first iteration of experiment begins No lastUpdateTime metav1.Time The time when the status was most recently updated. No stage string Indicator of the progress of an experiment. The stage is Waiting before an experiment executes its start action, Initializing while running the start action, Running while the experiment has begun its first iteration and is progressing, Finishing while any finish action is running and Completed when the experiment terminates. No completedIterations int32 Number of completed iterations of the experiment. This is undefined until the experiment reaches the Running stage. No currentWeightDistribution [] WeightData The latest observed split of traffic between versions. Expressed as percentage. Iter8 ensures that this field is current until the final iteration of the experiment. Iter8 will cease to update this field once a finish action is invoked. No analysis Analysis Result of latest query to the Iter8 analytics service. No versionRecommendedForPromotion string The version recommended for promotion. This field is initially populated by Iter8 as the baseline version and continuously updated during the course of the experiment to match the winner. The value of this field is typically used by finish actions to promote a version at the end of an experiment. No metrics [] MetricInfo A list of metrics referenced in the criteria section of this experiment. No message string Human readable message. No Experiment field types \u00b6 Strategy \u00b6 Field name Field type Description Required testingPattern string Determines the logic used to evaluate the app versions and determine the winner of the experiment. Iter8 supports two testing strategies, namely, Canary and Conformance . Yes deploymentPattern string Determines if and how traffic is shifted during an experiment. This field is relevant only for experiments using the Canary testing pattern. Iter8 supports two deployment patterns, namely, Progressive and FixedSplit . No actions map[ActionType][] TaskSpec An action is a sequence of tasks that can be executed by Iter8. ActionType is a string enum with three valid values: start , loop , and finish . The start action, if specified, is executed at the start of the experiment. The loop action, if specified, is executed during every loop of the experiment, after all the iterations within the loop have completed. The finish action, if specified, is executed at the end of the experiment after all the loops have completed. The actions field is used to specify all three types of actions. No TaskSpec \u00b6 Specification of a task that will be executed as part of experiment actions. Tasks are documented here . Field name Field type Description Required task string Name of the task. Task names express both the library and the task within the library in the format 'library/task' . Yes with map[string] apiextensionsv1.JSON Inputs to the task. No Criteria \u00b6 Field name Field type Description Required requestCount string Reference to the metric used to count the number of requests sent to app versions. No rewards Reward [] A list of metrics along with their preferred directions. Currently, this list needs to be of size one. This field can only be used in experiments with A/B and A/B/n testing patterns. No objectives Objective [] A list of metrics along with acceptable upper limits, lower limits, or both upper and lower limits for them. Iter8 will verify if app versions satisfy these objectives. No indicators string[] A list of metric references. Iter8 will collect and report the values of these metrics in addition to those referenced in the objectives section. No Note: References to metric resource objects within experiment criteria should be in the namespace/name format or in the name format. If the name format is used (i.e., if only the name of the metric is specified), then Iter8 searches for the metric in the namespace of the experiment resource. If Iter8 cannot find the metric, then the reference is considered invalid and the experiment will terminate in a failure. Objective \u00b6 Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes upperLimit Quantity Upper limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be below the limit. No lowerLimit Quantity Lower limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be above the limit. No Reward \u00b6 Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes preferredDirection string Indicates if higher values or lower values of this metric are preferable. High and Low are the two permissible values for this string. Yes Duration \u00b6 The duration of the experiment. Field name Field type Description Required maxLoops int32 Maximum number of loops in the experiment. In case of a failure, the experiment may be terminated earlier. Default value = 1. No iterationsPerLoop int32 Number of iterations per experiment loop . In case of a failure, the experiment may be terminated earlier. Default value = 15. No intervalSeconds int32 Duration of a single iteration of the experiment in seconds. Default value = 20 seconds. No Note : Suppose an experiment has maxLoops = x , iterationsPerLoop = y , and intervalSeconds = z . Assuming the experiment does not terminate early due to failures, it would take a minimum of x*y*z seconds to complete. The actual duration may be more due to additional time incurred in acquiring the target , and executing the start , loop and finish actions . VersionInfo \u00b6 spec.versionInfo describes the app versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates . Field name Field type Description Required baseline VersionDetail Details of the current or baseline version. Yes candidates [] VersionDetail Details of the candidate version or versions, if any. No Number of versions Conformance experiments involve only a single version (baseline). Hence, in these experiments, the candidates field must be omitted. A/B and Canary experiments involve two versions, a baseline and a candidate. Hence, the candidates field must be a list of length one in these experiments. A/B/n experiments involve three or more versions. Hence, in these experiments, the candidates field must be of length two or more. VersionDetail \u00b6 Field name Field type Description Required name string Name of the version. Yes variables [] NamedValue Variables are name-value pairs associated with a version. Metrics and tasks within experiment specs can contain strings with placeholders. Iter8 uses variables to substitute placeholders in these strings. No weightObjRef corev1.ObjectReference Reference to a Kubernetes resource and a field-path within the resource. Iter8 uses weightObjRef to get or set weight (traffic percentage) for the version. No MetricInfo \u00b6 Field name Field type Description Required name string Identifies an Iter8 metric using the namespace/name or name format . Yes metric [] Metric Iter8 metric object referenced by name. No ExperimentCondition \u00b6 Conditions express aspects of the progress of an experiment. The Completed condition indicates whether or not an experiment has completed. The Failed condition indicates whether or not an experiment completed successfully or in failure. The TargetAcquired condition indicates that an experiment has acquired the target and is now scheduled to run. At any point in time, for any given target, Iter8 ensures that at most one experiment has the conditions TargetAcquired set to True and Completed set to False . Field name Field type Description Required type string Type of condition. Valid types are TargetAcquired , Completed and Failed . Yes status corev1.ConditionStatus status of condition, one of True , False , or Unknown . Yes lastTransitionTime metav1.Time The last time any field in the condition was changed. No reason string A reason for the change in value. No message string Human readable decription. No Analysis \u00b6 Field name Field type Description Required aggregatedBuiltinHists AggregatedBuiltinHists This field is used to store intermediate results from the metrics/collect task that enables built-in metrics . Reserved for Iter8 internal use. No aggregatedMetrics AggregatedMetricsAnalysis Most recently observed metric values for all metrics referenced in the experiment criteria. No winnerAssessment WinnerAssessmentAnalysis Information about the winner of the experiment. No versionAssessments VersionAssessmentAnalysis For each version, a summary analysis identifying whether or not the version is satisfying the experiment criteria. No weights WeightsAnalysis Recommended weight distribution to be applied before the next iteration of the experiment. No AggregatedBuiltinHists \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 built-in metrics collect task is the only valid value for this field. Reserved for Iter8 internal use. Yes timestamp metav1.Time The time when this field was last updated. Reserved for Iter8 internal use. Yes message string Human readable message. Reserved for Iter8 internal use. No data apiextensionsv1.JSON Aggregated histogram data for storing intermediate results for built-in metics collection. Reserved for Iter8 internal use. No VersionAssessmentAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string][]bool map of version name to a list of boolean values, one for each objective specified in the experiment criteria, indicating whether not the objective is satisified. No AggregatedMetricsAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string] AggregatedMetricsData Map from metric name to most recent data (from all versions) for the metric. Yes AggregatedMetricsData \u00b6 Field name Field type Description Required max Quantity The maximum value observed for this metric accross all versions. Yes min Quantity The minimum value observed for this metric accross all versions. Yes data map[string] AggregatedMetricsVersionData A map from version name to the most recent aggregated metrics data for that version. No AggregatedMetricsVersionData \u00b6 Field name Field type Description Required max Quantity The maximum value observed for this metric for this version over all observations. No min Quantity The minimum value observed for this metric for this version over all observations. No value Quantity The value. No sampleSize int32 The number of requests observed by this version. No WinnerAssessmentAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data WinnerAssessmentData Details on whether or not a winner has been identified and which version if so. No WinnerAssessmentData \u00b6 Field name Field type Description Required winnerFound bool Whether or not a winner has been identified. Yes winner string The name of the identified winner, if one has been found. No WeightAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data [] WeightData List of version name/value pairs representing a recommended weight for each version No WeightData \u00b6 Field name Field type Description Required name string Version name Yes value int32 Percentage of traffic being sent to the version. Yes Common field types \u00b6 NamedValue \u00b6 Field name Field type Description Required name string Name of a variable. Yes value string Value of a variable. Yes A/B/n experiments involve more than one candidate. Their description is coming soon. \u21a9","title":"Experiment resource"},{"location":"reference/experiment/#experiment-resource","text":"Experiment resource Iter8's Experiment resource type enables application developers and service operators to automate A/B, A/B/n, Canary and Conformance experiments for Kubernetes apps/ML models. The controls provided by the experiment resource type encompass testing, deployment, traffic engineering, and version promotion functions . Sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-knative/request-count rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate Version This document describes version v2alpha2 of Iter8's experiment API.","title":"Experiment Resource"},{"location":"reference/experiment/#metadata","text":"Standard Kubernetes meta.v1/ObjectMeta resource.","title":"Metadata"},{"location":"reference/experiment/#spec","text":"Field name Field type Description Required target string Identifies the app under experimentation and determines which experiments can run concurrently. Experiments that have the same target value will not be scheduled concurrently but will be run sequentially in the order of their creation timestamps. Experiments whose target values differ from each other can be scheduled by Iter8 concurrently. Yes strategy Strategy The experimentation strategy which specifies how app versions are tested, how traffic is shifted during experiment, and what tasks are executed at the start and end of the experiment. Yes criteria Criteria Criteria used for evaluating versions. This section includes (business) rewards, service-level objectives (SLOs) and indicators (SLIs). No duration Duration Duration of the experiment. No versionInfo VersionInfo Versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates. No","title":"Spec"},{"location":"reference/experiment/#status","text":"Field name Field type Description Required conditions [] ExperimentCondition A set of conditions that express progress of an experiment. No initTime metav1.Time The time the experiment is created. No startTime metav1.Time The time when the first iteration of experiment begins No lastUpdateTime metav1.Time The time when the status was most recently updated. No stage string Indicator of the progress of an experiment. The stage is Waiting before an experiment executes its start action, Initializing while running the start action, Running while the experiment has begun its first iteration and is progressing, Finishing while any finish action is running and Completed when the experiment terminates. No completedIterations int32 Number of completed iterations of the experiment. This is undefined until the experiment reaches the Running stage. No currentWeightDistribution [] WeightData The latest observed split of traffic between versions. Expressed as percentage. Iter8 ensures that this field is current until the final iteration of the experiment. Iter8 will cease to update this field once a finish action is invoked. No analysis Analysis Result of latest query to the Iter8 analytics service. No versionRecommendedForPromotion string The version recommended for promotion. This field is initially populated by Iter8 as the baseline version and continuously updated during the course of the experiment to match the winner. The value of this field is typically used by finish actions to promote a version at the end of an experiment. No metrics [] MetricInfo A list of metrics referenced in the criteria section of this experiment. No message string Human readable message. No","title":"Status"},{"location":"reference/experiment/#experiment-field-types","text":"","title":"Experiment field types"},{"location":"reference/experiment/#strategy","text":"Field name Field type Description Required testingPattern string Determines the logic used to evaluate the app versions and determine the winner of the experiment. Iter8 supports two testing strategies, namely, Canary and Conformance . Yes deploymentPattern string Determines if and how traffic is shifted during an experiment. This field is relevant only for experiments using the Canary testing pattern. Iter8 supports two deployment patterns, namely, Progressive and FixedSplit . No actions map[ActionType][] TaskSpec An action is a sequence of tasks that can be executed by Iter8. ActionType is a string enum with three valid values: start , loop , and finish . The start action, if specified, is executed at the start of the experiment. The loop action, if specified, is executed during every loop of the experiment, after all the iterations within the loop have completed. The finish action, if specified, is executed at the end of the experiment after all the loops have completed. The actions field is used to specify all three types of actions. No","title":"Strategy"},{"location":"reference/experiment/#taskspec","text":"Specification of a task that will be executed as part of experiment actions. Tasks are documented here . Field name Field type Description Required task string Name of the task. Task names express both the library and the task within the library in the format 'library/task' . Yes with map[string] apiextensionsv1.JSON Inputs to the task. No","title":"TaskSpec"},{"location":"reference/experiment/#criteria","text":"Field name Field type Description Required requestCount string Reference to the metric used to count the number of requests sent to app versions. No rewards Reward [] A list of metrics along with their preferred directions. Currently, this list needs to be of size one. This field can only be used in experiments with A/B and A/B/n testing patterns. No objectives Objective [] A list of metrics along with acceptable upper limits, lower limits, or both upper and lower limits for them. Iter8 will verify if app versions satisfy these objectives. No indicators string[] A list of metric references. Iter8 will collect and report the values of these metrics in addition to those referenced in the objectives section. No Note: References to metric resource objects within experiment criteria should be in the namespace/name format or in the name format. If the name format is used (i.e., if only the name of the metric is specified), then Iter8 searches for the metric in the namespace of the experiment resource. If Iter8 cannot find the metric, then the reference is considered invalid and the experiment will terminate in a failure.","title":"Criteria"},{"location":"reference/experiment/#objective","text":"Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes upperLimit Quantity Upper limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be below the limit. No lowerLimit Quantity Lower limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be above the limit. No","title":"Objective"},{"location":"reference/experiment/#reward","text":"Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes preferredDirection string Indicates if higher values or lower values of this metric are preferable. High and Low are the two permissible values for this string. Yes","title":"Reward"},{"location":"reference/experiment/#duration","text":"The duration of the experiment. Field name Field type Description Required maxLoops int32 Maximum number of loops in the experiment. In case of a failure, the experiment may be terminated earlier. Default value = 1. No iterationsPerLoop int32 Number of iterations per experiment loop . In case of a failure, the experiment may be terminated earlier. Default value = 15. No intervalSeconds int32 Duration of a single iteration of the experiment in seconds. Default value = 20 seconds. No Note : Suppose an experiment has maxLoops = x , iterationsPerLoop = y , and intervalSeconds = z . Assuming the experiment does not terminate early due to failures, it would take a minimum of x*y*z seconds to complete. The actual duration may be more due to additional time incurred in acquiring the target , and executing the start , loop and finish actions .","title":"Duration"},{"location":"reference/experiment/#versioninfo","text":"spec.versionInfo describes the app versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates . Field name Field type Description Required baseline VersionDetail Details of the current or baseline version. Yes candidates [] VersionDetail Details of the candidate version or versions, if any. No Number of versions Conformance experiments involve only a single version (baseline). Hence, in these experiments, the candidates field must be omitted. A/B and Canary experiments involve two versions, a baseline and a candidate. Hence, the candidates field must be a list of length one in these experiments. A/B/n experiments involve three or more versions. Hence, in these experiments, the candidates field must be of length two or more.","title":"VersionInfo"},{"location":"reference/experiment/#versiondetail","text":"Field name Field type Description Required name string Name of the version. Yes variables [] NamedValue Variables are name-value pairs associated with a version. Metrics and tasks within experiment specs can contain strings with placeholders. Iter8 uses variables to substitute placeholders in these strings. No weightObjRef corev1.ObjectReference Reference to a Kubernetes resource and a field-path within the resource. Iter8 uses weightObjRef to get or set weight (traffic percentage) for the version. No","title":"VersionDetail"},{"location":"reference/experiment/#metricinfo","text":"Field name Field type Description Required name string Identifies an Iter8 metric using the namespace/name or name format . Yes metric [] Metric Iter8 metric object referenced by name. No","title":"MetricInfo"},{"location":"reference/experiment/#experimentcondition","text":"Conditions express aspects of the progress of an experiment. The Completed condition indicates whether or not an experiment has completed. The Failed condition indicates whether or not an experiment completed successfully or in failure. The TargetAcquired condition indicates that an experiment has acquired the target and is now scheduled to run. At any point in time, for any given target, Iter8 ensures that at most one experiment has the conditions TargetAcquired set to True and Completed set to False . Field name Field type Description Required type string Type of condition. Valid types are TargetAcquired , Completed and Failed . Yes status corev1.ConditionStatus status of condition, one of True , False , or Unknown . Yes lastTransitionTime metav1.Time The last time any field in the condition was changed. No reason string A reason for the change in value. No message string Human readable decription. No","title":"ExperimentCondition"},{"location":"reference/experiment/#analysis","text":"Field name Field type Description Required aggregatedBuiltinHists AggregatedBuiltinHists This field is used to store intermediate results from the metrics/collect task that enables built-in metrics . Reserved for Iter8 internal use. No aggregatedMetrics AggregatedMetricsAnalysis Most recently observed metric values for all metrics referenced in the experiment criteria. No winnerAssessment WinnerAssessmentAnalysis Information about the winner of the experiment. No versionAssessments VersionAssessmentAnalysis For each version, a summary analysis identifying whether or not the version is satisfying the experiment criteria. No weights WeightsAnalysis Recommended weight distribution to be applied before the next iteration of the experiment. No","title":"Analysis"},{"location":"reference/experiment/#aggregatedbuiltinhists","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 built-in metrics collect task is the only valid value for this field. Reserved for Iter8 internal use. Yes timestamp metav1.Time The time when this field was last updated. Reserved for Iter8 internal use. Yes message string Human readable message. Reserved for Iter8 internal use. No data apiextensionsv1.JSON Aggregated histogram data for storing intermediate results for built-in metics collection. Reserved for Iter8 internal use. No","title":"AggregatedBuiltinHists"},{"location":"reference/experiment/#versionassessmentanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string][]bool map of version name to a list of boolean values, one for each objective specified in the experiment criteria, indicating whether not the objective is satisified. No","title":"VersionAssessmentAnalysis"},{"location":"reference/experiment/#aggregatedmetricsanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string] AggregatedMetricsData Map from metric name to most recent data (from all versions) for the metric. Yes","title":"AggregatedMetricsAnalysis"},{"location":"reference/experiment/#aggregatedmetricsdata","text":"Field name Field type Description Required max Quantity The maximum value observed for this metric accross all versions. Yes min Quantity The minimum value observed for this metric accross all versions. Yes data map[string] AggregatedMetricsVersionData A map from version name to the most recent aggregated metrics data for that version. No","title":"AggregatedMetricsData"},{"location":"reference/experiment/#aggregatedmetricsversiondata","text":"Field name Field type Description Required max Quantity The maximum value observed for this metric for this version over all observations. No min Quantity The minimum value observed for this metric for this version over all observations. No value Quantity The value. No sampleSize int32 The number of requests observed by this version. No","title":"AggregatedMetricsVersionData"},{"location":"reference/experiment/#winnerassessmentanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data WinnerAssessmentData Details on whether or not a winner has been identified and which version if so. No","title":"WinnerAssessmentAnalysis"},{"location":"reference/experiment/#winnerassessmentdata","text":"Field name Field type Description Required winnerFound bool Whether or not a winner has been identified. Yes winner string The name of the identified winner, if one has been found. No","title":"WinnerAssessmentData"},{"location":"reference/experiment/#weightanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data [] WeightData List of version name/value pairs representing a recommended weight for each version No","title":"WeightAnalysis"},{"location":"reference/experiment/#weightdata","text":"Field name Field type Description Required name string Version name Yes value int32 Percentage of traffic being sent to the version. Yes","title":"WeightData"},{"location":"reference/experiment/#common-field-types","text":"","title":"Common field types"},{"location":"reference/experiment/#namedvalue","text":"Field name Field type Description Required name string Name of a variable. Yes value string Value of a variable. Yes A/B/n experiments involve more than one candidate. Their description is coming soon. \u21a9","title":"NamedValue"},{"location":"reference/helmex-schema/","text":"Helmex Schema \u00b6 The Helmex schema for the Helm values.yaml file is described below. It is intended for applications that are templated using Helm and use Iter8 experiments during releases. In addition to the below requirements, an application may impose additional application-specific schema requirements on values.yaml . Top-level fields \u00b6 Field name Field type Description Required baseline Version Information about the baseline version of the application. The baseline version typically corresponds to the stable version of the application. Yes candidate Version Information about the candidate version. Iter8 experiment resource will be created only if this field is present. If this field is modified, any existing experiment for the application will be replaced by a new experiment. No Version \u00b6 Field name Field type Description Required dynamic Dynamic Information associated with a specific version. For example, each time the baseline version of the application changes, the baseline.dynamic field in the Helm values file should change. Yes Dynamic \u00b6 Field name Field type Description Required id string Alpha-numeric string that uniquely identifies a version. This optional field is strongly recommended for every version. No. Example \u00b6 The following Helm values file is an instance of the Helmex schema. # values meant for both baseline and candidate versions of the application; common : application : hello repo : \"gcr.io/google-samples/hello-app\" serviceType : ClusterIP servicePortInfo : port : 8080 regularLabels : app.kubernetes.io/managed-by : Iter8 selectorLabels : app.kubernetes.io/name : hello # values meant for baseline version of the application only; # baseline version is required by Helmex schema baseline : name : hello selectorLabels : app.kubernetes.io/track : baseline # required field for baseline version dynamic : # unique alpha-numeric version ID is strongly recommended id : \"mn82l82\" tag : \"1.0\" # values meant for candidate version of the application only; # optional section; Iter8 experiment will be deployed if this section is present candidate : name : hello-candidate selectorLabels : app.kubernetes.io/track : candidate # required field for candidate version # if candidate is promoted, the dynamic field from candidate will be copied over to baseline, and candidate will be set to null dynamic : # unique alpha-numeric version ID is strongly recommended id : \"8s72oa\" tag : \"2.0\" # this section is used in the creation of the Iter8 experiment # the specific experiment section below is used in the context of an SLO validation experiment experiment : # The SLO validation experiment will collect Iter8's built-in latency and error metrics. # There will be 8.0 * 5 = 40 queries sent during metrics collection. # time is the duration over which queries are sent during metrics collection. time : 5s # QPS is number of queries per second sent during metrics collection. QPS : 8.0 # (msec) acceptable limit for mean latency of the application limitMeanLatency : 500.0 # (msec) acceptable error rate for the application (1%) limitErrorRate : 0.01 # (msec) acceptable limit for 95th percentile latency of the application limit95thPercentileLatency : 1000.0","title":"Helmex schema"},{"location":"reference/helmex-schema/#helmex-schema","text":"The Helmex schema for the Helm values.yaml file is described below. It is intended for applications that are templated using Helm and use Iter8 experiments during releases. In addition to the below requirements, an application may impose additional application-specific schema requirements on values.yaml .","title":"Helmex Schema"},{"location":"reference/helmex-schema/#top-level-fields","text":"Field name Field type Description Required baseline Version Information about the baseline version of the application. The baseline version typically corresponds to the stable version of the application. Yes candidate Version Information about the candidate version. Iter8 experiment resource will be created only if this field is present. If this field is modified, any existing experiment for the application will be replaced by a new experiment. No","title":"Top-level fields"},{"location":"reference/helmex-schema/#version","text":"Field name Field type Description Required dynamic Dynamic Information associated with a specific version. For example, each time the baseline version of the application changes, the baseline.dynamic field in the Helm values file should change. Yes","title":"Version"},{"location":"reference/helmex-schema/#dynamic","text":"Field name Field type Description Required id string Alpha-numeric string that uniquely identifies a version. This optional field is strongly recommended for every version. No.","title":"Dynamic"},{"location":"reference/helmex-schema/#example","text":"The following Helm values file is an instance of the Helmex schema. # values meant for both baseline and candidate versions of the application; common : application : hello repo : \"gcr.io/google-samples/hello-app\" serviceType : ClusterIP servicePortInfo : port : 8080 regularLabels : app.kubernetes.io/managed-by : Iter8 selectorLabels : app.kubernetes.io/name : hello # values meant for baseline version of the application only; # baseline version is required by Helmex schema baseline : name : hello selectorLabels : app.kubernetes.io/track : baseline # required field for baseline version dynamic : # unique alpha-numeric version ID is strongly recommended id : \"mn82l82\" tag : \"1.0\" # values meant for candidate version of the application only; # optional section; Iter8 experiment will be deployed if this section is present candidate : name : hello-candidate selectorLabels : app.kubernetes.io/track : candidate # required field for candidate version # if candidate is promoted, the dynamic field from candidate will be copied over to baseline, and candidate will be set to null dynamic : # unique alpha-numeric version ID is strongly recommended id : \"8s72oa\" tag : \"2.0\" # this section is used in the creation of the Iter8 experiment # the specific experiment section below is used in the context of an SLO validation experiment experiment : # The SLO validation experiment will collect Iter8's built-in latency and error metrics. # There will be 8.0 * 5 = 40 queries sent during metrics collection. # time is the duration over which queries are sent during metrics collection. time : 5s # QPS is number of queries per second sent during metrics collection. QPS : 8.0 # (msec) acceptable limit for mean latency of the application limitMeanLatency : 500.0 # (msec) acceptable error rate for the application (1%) limitErrorRate : 0.01 # (msec) acceptable limit for 95th percentile latency of the application limit95thPercentileLatency : 1000.0","title":"Example"},{"location":"reference/metrics/","text":"Metric Resource \u00b6 Metric resource Iter8 defines the Metric resource type, which encapsulates the REST query that is used to retrieve a metric value from the metrics provider. Metric resources are referenced in experiments. Version This document describes version v2alpha2 of Iter8's metric API. Metrics usage is documented here . Iter8 provides a set of built-in metrics that are documented here . Creation of custom metrics is documented here . It is also possible to mock metric values; mock metrics are documented here . Sample metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[$elapsedTime])) or on() vector(0) description : Number of requests type : counter provider : prometheus jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Metadata \u00b6 Standard Kubernetes meta.v1/ObjectMeta resource. Spec \u00b6 Field name Field type Description Required description string Human readable description. This field is meant for informational purposes. No units string Units of measurement. This field is meant for informational purposes. No provider string Type of the metrics provider (example, prometheus , newrelic , sysdig , elastic , ...). The keyword iter8 is reserved for Iter8 built-in metrics. No params [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP query parameters used by Iter8 when querying the metrics provider. Each name represents a parameter name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. No body string String used to construct the JSON body of the HTTP request. Body may be templated, in which Iter8 will attempt to substitute placeholders in the template at query time using version information. No type string Metric type. Valid values are Counter and Gauge . Default value = Gauge . A Counter metric is one whose value never decreases over time. A Gauge metric is one whose value may increase or decrease over time. No method string HTTP method (verb) used in the HTTP request. Valid values are GET and POST . Default value = GET . No authType string Identifies the type of authentication used in the HTTP request. Valid values are Basic , Bearer and APIKey which correspond to HTTP authentication with these respective methods. No sampleSize string Reference to a metric that represents the number of data points over which the value of this metric is computed. This field applies only to Gauge metrics. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace of the referring metric. No secret string Reference to a secret that contains information used for authenticating with the metrics provider. In particular, Iter8 uses data in this secret to substitute placeholders in the HTTP headers and URL while querying the provider. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace where Iter8 is installed (which is iter8-system by default). No headerTemplates [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP request headers used by Iter8 when querying the metrics provider. Each name represents a header field name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. Placeholder substitution is attempted only if authType and secret fields are present. No jqExpression string The jq expression used by Iter8 to extract the metric value from the JSON response returned by the provider. This field may be omitted if the metric is mocked . No urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. This field may be omitted if the metric is mocked . No urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. This field may be omitted if the metric is mocked . No mock [] NamedLevel Information on how to mock this metric. The presence of this fields indicates that this metric will be mocked; the absence of this field indicates that this is a real metric. No Named Level \u00b6 Field name Field type Description Required name string Name of the version. Version names should be unique. Version name should match the name of a version in the experiment's versionInfo section. If not, any value generated for the non-matching name will be ignored. Yes level string Level of a version. The semantics of level are as follows. If the metric is a counter, if level is x , and time elapsed since the start of the experiment is y seconds, then xy is the mocked metric value. This will keep increasing the metric value over time. If the metric is gauge, if level is x , the metric value is a random value with mean x . The expected value of the mocked metric is x ; the actual value may increase or decrease over time. Yes","title":"Metric resource"},{"location":"reference/metrics/#metric-resource","text":"Metric resource Iter8 defines the Metric resource type, which encapsulates the REST query that is used to retrieve a metric value from the metrics provider. Metric resources are referenced in experiments. Version This document describes version v2alpha2 of Iter8's metric API. Metrics usage is documented here . Iter8 provides a set of built-in metrics that are documented here . Creation of custom metrics is documented here . It is also possible to mock metric values; mock metrics are documented here . Sample metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[$elapsedTime])) or on() vector(0) description : Number of requests type : counter provider : prometheus jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query","title":"Metric Resource"},{"location":"reference/metrics/#metadata","text":"Standard Kubernetes meta.v1/ObjectMeta resource.","title":"Metadata"},{"location":"reference/metrics/#spec","text":"Field name Field type Description Required description string Human readable description. This field is meant for informational purposes. No units string Units of measurement. This field is meant for informational purposes. No provider string Type of the metrics provider (example, prometheus , newrelic , sysdig , elastic , ...). The keyword iter8 is reserved for Iter8 built-in metrics. No params [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP query parameters used by Iter8 when querying the metrics provider. Each name represents a parameter name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. No body string String used to construct the JSON body of the HTTP request. Body may be templated, in which Iter8 will attempt to substitute placeholders in the template at query time using version information. No type string Metric type. Valid values are Counter and Gauge . Default value = Gauge . A Counter metric is one whose value never decreases over time. A Gauge metric is one whose value may increase or decrease over time. No method string HTTP method (verb) used in the HTTP request. Valid values are GET and POST . Default value = GET . No authType string Identifies the type of authentication used in the HTTP request. Valid values are Basic , Bearer and APIKey which correspond to HTTP authentication with these respective methods. No sampleSize string Reference to a metric that represents the number of data points over which the value of this metric is computed. This field applies only to Gauge metrics. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace of the referring metric. No secret string Reference to a secret that contains information used for authenticating with the metrics provider. In particular, Iter8 uses data in this secret to substitute placeholders in the HTTP headers and URL while querying the provider. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace where Iter8 is installed (which is iter8-system by default). No headerTemplates [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP request headers used by Iter8 when querying the metrics provider. Each name represents a header field name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. Placeholder substitution is attempted only if authType and secret fields are present. No jqExpression string The jq expression used by Iter8 to extract the metric value from the JSON response returned by the provider. This field may be omitted if the metric is mocked . No urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. This field may be omitted if the metric is mocked . No urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. This field may be omitted if the metric is mocked . No mock [] NamedLevel Information on how to mock this metric. The presence of this fields indicates that this metric will be mocked; the absence of this field indicates that this is a real metric. No","title":"Spec"},{"location":"reference/metrics/#named-level","text":"Field name Field type Description Required name string Name of the version. Version names should be unique. Version name should match the name of a version in the experiment's versionInfo section. If not, any value generated for the non-matching name will be ignored. Yes level string Level of a version. The semantics of level are as follows. If the metric is a counter, if level is x , and time elapsed since the start of the experiment is y seconds, then xy is the mocked metric value. This will keep increasing the metric value over time. If the metric is gauge, if level is x , the metric value is a random value with mean x . The expected value of the mocked metric is x ; the actual value may increase or decrease over time. Yes","title":"Named Level"},{"location":"reference/tasks/common-bash/","text":"common/bash \u00b6 The common/bash task executes a bash script. The script can be written to use placeholders that are dynamically substituted at runtime . For example, the common/bash task can be used as part of a finish action to promote the winning version at the end of an experiment. Example \u00b6 The following (partially-specified) experiment executes the one line script kubectl apply using a YAML manifest at the end of the experiment. The URL for the manifest contains a placeholder {{ .promotionManifest }} , which is dynamically substituted at the end of the experiment. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/bash # promote the winning version with : script : | kubectl apply -f {{ .promotionManifest }} ... versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 variables : - name : promotionManifest value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml candidates : - name : sample-app-v2 variables : - name : promotionManifest value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml Inputs \u00b6 Field name Field type Description Required script string The bash script that will be executed Yes Result \u00b6 The script will be executed. In the example above, a YAML file corresponding to the baseline or candidate version will be applied to the cluster. If this task exits with a non-zero error code, the experiment to which it belongs will fail. Dynamic Variable Substitution \u00b6 The script input to the common/bash task may contain placeholders, or template variables, which will be dynamically substituted when the task is executed by Iter8. For example, in the task: - task: common/bash # promote the winning version with: script: | kubectl apply -f {{ .promoteManifest }} {{ .promotionManifest}} is a placeholder. Placeholders are specified using the Go language specification for data-driven templates . In particular, placeholders are specified between double curly braces. The common/bash task supports placeholders for: Values of variables of the version recommended for promotion. To specify such placeholders, use the name of the variable as defined in the versionInfo section of the experiment definition. For example, in the above example, {{ .promotionManifest }} is a placeholder for the value of the variable with the name promotionManifest of the version Iter8 recommends for promotion (see .status.versionRecommendedForPromotion ). Values defined in the experiment itself. To specify such placeholders, use the prefix .this . For example, {{ .this.metadata.name }} is a placeholder for the name of the experiment. If Iter8 cannot evaluate a placeholder expression, a blank value (\"\") will be substituted.","title":"common/bash"},{"location":"reference/tasks/common-bash/#commonbash","text":"The common/bash task executes a bash script. The script can be written to use placeholders that are dynamically substituted at runtime . For example, the common/bash task can be used as part of a finish action to promote the winning version at the end of an experiment.","title":"common/bash"},{"location":"reference/tasks/common-bash/#example","text":"The following (partially-specified) experiment executes the one line script kubectl apply using a YAML manifest at the end of the experiment. The URL for the manifest contains a placeholder {{ .promotionManifest }} , which is dynamically substituted at the end of the experiment. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/bash # promote the winning version with : script : | kubectl apply -f {{ .promotionManifest }} ... versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 variables : - name : promotionManifest value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml candidates : - name : sample-app-v2 variables : - name : promotionManifest value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml","title":"Example"},{"location":"reference/tasks/common-bash/#inputs","text":"Field name Field type Description Required script string The bash script that will be executed Yes","title":"Inputs"},{"location":"reference/tasks/common-bash/#result","text":"The script will be executed. In the example above, a YAML file corresponding to the baseline or candidate version will be applied to the cluster. If this task exits with a non-zero error code, the experiment to which it belongs will fail.","title":"Result"},{"location":"reference/tasks/common-bash/#dynamic-variable-substitution","text":"The script input to the common/bash task may contain placeholders, or template variables, which will be dynamically substituted when the task is executed by Iter8. For example, in the task: - task: common/bash # promote the winning version with: script: | kubectl apply -f {{ .promoteManifest }} {{ .promotionManifest}} is a placeholder. Placeholders are specified using the Go language specification for data-driven templates . In particular, placeholders are specified between double curly braces. The common/bash task supports placeholders for: Values of variables of the version recommended for promotion. To specify such placeholders, use the name of the variable as defined in the versionInfo section of the experiment definition. For example, in the above example, {{ .promotionManifest }} is a placeholder for the value of the variable with the name promotionManifest of the version Iter8 recommends for promotion (see .status.versionRecommendedForPromotion ). Values defined in the experiment itself. To specify such placeholders, use the prefix .this . For example, {{ .this.metadata.name }} is a placeholder for the name of the experiment. If Iter8 cannot evaluate a placeholder expression, a blank value (\"\") will be substituted.","title":"Dynamic Variable Substitution"},{"location":"reference/tasks/common-exec/","text":"common/exec (deprecated) \u00b6 The common/exec task executes a shell command with arguments. Arguments may be specified using placeholders that are dynamically substituted at runtime. The common/exec task can be used as part of a finish action to promote the winning version at the end of an experiment. common/exec is deprecated; use common/bash instead. Example \u00b6 The following (partially-specified) experiment executes kubectl apply using a YAML manifest at the end of the experiment. The URL for the manifest contains a placeholder {{ .promote }} , which is dynamically substituted at the end of the experiment. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml ... versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 variables : - name : promote value : baseline candidates : - name : sample-app-v2 variables : - name : promote value : candidate Inputs \u00b6 Field name Field type Description Required cmd string The command that should be executed Yes args []string A list of command line arguments that should be passed to cmd . No disableInterpolation bool Flag indicating whether or not to disable placeholder subsitution. For details, see below . Default is false . No Result \u00b6 The command with the supplied arguments will be executed. In the example above, a YAML file corresponding to the baseline or candidate version will be applied to the cluster. If this task exits with a non-zero error code, the experiment to which it belongs will fail. Dynamic Variable Substitution \u00b6 The common/exec task supports dynamic variable subsitution only using the variables of the version recommended for promotion. Instead of defaulting to a blank value when Iter8 has not determined a version to recommend for promotion (that is, in start tasks), this task supports the disableInterpolation option to prevent dynamic variable substitution.","title":"common/exec"},{"location":"reference/tasks/common-exec/#commonexec-deprecated","text":"The common/exec task executes a shell command with arguments. Arguments may be specified using placeholders that are dynamically substituted at runtime. The common/exec task can be used as part of a finish action to promote the winning version at the end of an experiment. common/exec is deprecated; use common/bash instead.","title":"common/exec (deprecated)"},{"location":"reference/tasks/common-exec/#example","text":"The following (partially-specified) experiment executes kubectl apply using a YAML manifest at the end of the experiment. The URL for the manifest contains a placeholder {{ .promote }} , which is dynamically substituted at the end of the experiment. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml ... versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 variables : - name : promote value : baseline candidates : - name : sample-app-v2 variables : - name : promote value : candidate","title":"Example"},{"location":"reference/tasks/common-exec/#inputs","text":"Field name Field type Description Required cmd string The command that should be executed Yes args []string A list of command line arguments that should be passed to cmd . No disableInterpolation bool Flag indicating whether or not to disable placeholder subsitution. For details, see below . Default is false . No","title":"Inputs"},{"location":"reference/tasks/common-exec/#result","text":"The command with the supplied arguments will be executed. In the example above, a YAML file corresponding to the baseline or candidate version will be applied to the cluster. If this task exits with a non-zero error code, the experiment to which it belongs will fail.","title":"Result"},{"location":"reference/tasks/common-exec/#dynamic-variable-substitution","text":"The common/exec task supports dynamic variable subsitution only using the variables of the version recommended for promotion. Instead of defaulting to a blank value when Iter8 has not determined a version to recommend for promotion (that is, in start tasks), this task supports the disableInterpolation option to prevent dynamic variable substitution.","title":"Dynamic Variable Substitution"},{"location":"reference/tasks/common-readiness/","text":"common/readiness \u00b6 The common/readiness task can be used to verify that Kubernetes resources (for example, deployments, Knative services, or Istio virtual services) that are required for the experiment are available and ready. This task is intended to be included in the start action of an experiment. Example \u00b6 The following is an experiment snippet with a common/readiness task. ... spec : strategy : actions : start : - task : common/readiness with : # verify that the following deployments exist objRefs : - kind : Deployment name : hello namespace : default # verify that the deployment is available waitFor : condition=available - kind : Deployment name : hello-candidate namespace : default # verify that the deployment is available waitFor : condition=available ... # `common/readiness` task will also inspect the versionInfo section. # If resources are specified as part of weightObjRef fields within versionInfo, # the task will verify the existence of these resources as well. versionInfo : baseline : name : stable weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : default name : hello-routing fieldPath : .spec.http[0].route[0].weight candidates : - name : candidate weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : default name : hello-routing fieldPath : .spec.http[0].route[1].weight Inputs \u00b6 Field name Field type Description Required initialDelaySeconds int Verification will be attempted only after this initial delay. Default value is 5 . No numRetries int If the task cannot verify the existence/conditions of the resources after the first attempt, it will retry with further attempts. Total number of attempts = 1 + numRetries. Default value for numRetries is 12 . No intervalSeconds int Time interval between each attempt. Default value is 5 . No objRefs [] ObjRef A list of Kubernetes object references along with any associated conditions which need to be verified. No ObjRef \u00b6 Field name Field type Description Required kind string Kind of the Kubernetes resource. Specified in the TYPE[.VERSION][.GROUP] format used by the kubectl get command Yes namespace string Namespace of the Kubernetes resource. Default value is the namespace of the experiment resource. No name string Name of the Kubernetes resource. Yes waitFor string Any value that is accepted by the --for flag of the kubectl wait command . No Result \u00b6 The task will succeed if all the specified resources are verified to exist (along with any associated conditions) within the specified number of attempts. Otherwise, the task will fail, resulting in the failure of the experiment.","title":"common/readiness"},{"location":"reference/tasks/common-readiness/#commonreadiness","text":"The common/readiness task can be used to verify that Kubernetes resources (for example, deployments, Knative services, or Istio virtual services) that are required for the experiment are available and ready. This task is intended to be included in the start action of an experiment.","title":"common/readiness"},{"location":"reference/tasks/common-readiness/#example","text":"The following is an experiment snippet with a common/readiness task. ... spec : strategy : actions : start : - task : common/readiness with : # verify that the following deployments exist objRefs : - kind : Deployment name : hello namespace : default # verify that the deployment is available waitFor : condition=available - kind : Deployment name : hello-candidate namespace : default # verify that the deployment is available waitFor : condition=available ... # `common/readiness` task will also inspect the versionInfo section. # If resources are specified as part of weightObjRef fields within versionInfo, # the task will verify the existence of these resources as well. versionInfo : baseline : name : stable weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : default name : hello-routing fieldPath : .spec.http[0].route[0].weight candidates : - name : candidate weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : default name : hello-routing fieldPath : .spec.http[0].route[1].weight","title":"Example"},{"location":"reference/tasks/common-readiness/#inputs","text":"Field name Field type Description Required initialDelaySeconds int Verification will be attempted only after this initial delay. Default value is 5 . No numRetries int If the task cannot verify the existence/conditions of the resources after the first attempt, it will retry with further attempts. Total number of attempts = 1 + numRetries. Default value for numRetries is 12 . No intervalSeconds int Time interval between each attempt. Default value is 5 . No objRefs [] ObjRef A list of Kubernetes object references along with any associated conditions which need to be verified. No","title":"Inputs"},{"location":"reference/tasks/common-readiness/#objref","text":"Field name Field type Description Required kind string Kind of the Kubernetes resource. Specified in the TYPE[.VERSION][.GROUP] format used by the kubectl get command Yes namespace string Namespace of the Kubernetes resource. Default value is the namespace of the experiment resource. No name string Name of the Kubernetes resource. Yes waitFor string Any value that is accepted by the --for flag of the kubectl wait command . No","title":"ObjRef"},{"location":"reference/tasks/common-readiness/#result","text":"The task will succeed if all the specified resources are verified to exist (along with any associated conditions) within the specified number of attempts. Otherwise, the task will fail, resulting in the failure of the experiment.","title":"Result"},{"location":"reference/tasks/gitops-helmex-update/","text":"gitops/helmex-update \u00b6 The github/helmex-update task can be used to update Helm values file in a GitHub repo. This task requires the Helm values file to conform to the Helmex schema . This task is intended to be included in the finish action of an experiment. Example \u00b6 The following is an experiment snippet with a gitops/helmex-update task. ... spec : strategy : actions : finish : - task : gitops/helmex-update with : # GitHub repo containing the values.yaml file gitRepo : \"https://github.com/ghuser/iter8.git\" # Path to values.yaml file filePath : \"samples/second-exp/values.yaml\" # GitHub username username : \"ghuser\" # Branch modified by this task branch : \"gitops-test\" # Secret containing the personal access token needed for git push secretName : \"my-secret\" # Namespace containing the above secret secretNamespace : \"default\" Inputs \u00b6 Field name Field type Description Required gitRepo string GitHub repo containing the values.yaml file. The repo needs to begin with the prefix https:// . Yes filePath string Path to the Helm values file, relative to the root of this repo. Yes username string GitHub username. For organization account, this can also be an org name. Yes branch string Branch to be updated by this task. Default value is main . No secretName string This task requires a personal access token in order to modify the GitHub repo. secretName is the name of the Kubernetes secret which contains this token. Default value is ghtoken . No secretNamespace string Namespace where the above secret is located. Default value is the namespace of the experiment. No Note: The task above expects to find a key named token within the secret's Data section; i.e., secret.Data[\"token\"] needs to be the GitHub personal access token. In addition, the iter8-handler service account in the iter8-system namespace needs to be given read permissions using RBAC rules for this secret. Result \u00b6 The version recommended for promotion by Iter8 will be promoted as the new baseline in the values.yaml file. Suppose the values.yaml file in the GitHub repo is the same as the one in this example . Baseline is promoted Assuming baseline is recommended for promotion by Iter8, the new values.file in the GitHub repo after this task executes, will look as follows. Notice how the dynamic field differs between the two scenarios. common : application : hello repo : \"gcr.io/google-samples/hello-app\" serviceType : ClusterIP servicePortInfo : port : 8080 regularLabels : app.kubernetes.io/managed-by : Iter8 selectorLabels : app.kubernetes.io/name : hello baseline : name : hello selectorLabels : app.kubernetes.io/track : baseline dynamic : id : \"mn82l82\" tag : \"1.0\" # even though there is an experiment section below, there will be # no Iter8 experiment in the cluster, since there is no candidate version experiment : time : 5s QPS : 8.0 limitMeanLatency : 500.0 limitErrorRate : 0.01 limit95thPercentileLatency : 1000.0 Candidate is promoted Assuming candidate is recommended for promotion by Iter8, the new values.file in the GitHub repo after this task executes, will look as follows. Notice how the dynamic field differs between the two scenarios. common : application : hello repo : \"gcr.io/google-samples/hello-app\" serviceType : ClusterIP servicePortInfo : port : 8080 regularLabels : app.kubernetes.io/managed-by : Iter8 selectorLabels : app.kubernetes.io/name : hello baseline : name : hello selectorLabels : app.kubernetes.io/track : baseline dynamic : id : \"8s72oa\" tag : \"2.0\" # even though there is an experiment section below, there will be # no Iter8 experiment in the cluster, since there is no candidate version experiment : time : 5s QPS : 8.0 limitMeanLatency : 500.0 limitErrorRate : 0.01 limit95thPercentileLatency : 1000.0","title":"gitops/helmex-update"},{"location":"reference/tasks/gitops-helmex-update/#gitopshelmex-update","text":"The github/helmex-update task can be used to update Helm values file in a GitHub repo. This task requires the Helm values file to conform to the Helmex schema . This task is intended to be included in the finish action of an experiment.","title":"gitops/helmex-update"},{"location":"reference/tasks/gitops-helmex-update/#example","text":"The following is an experiment snippet with a gitops/helmex-update task. ... spec : strategy : actions : finish : - task : gitops/helmex-update with : # GitHub repo containing the values.yaml file gitRepo : \"https://github.com/ghuser/iter8.git\" # Path to values.yaml file filePath : \"samples/second-exp/values.yaml\" # GitHub username username : \"ghuser\" # Branch modified by this task branch : \"gitops-test\" # Secret containing the personal access token needed for git push secretName : \"my-secret\" # Namespace containing the above secret secretNamespace : \"default\"","title":"Example"},{"location":"reference/tasks/gitops-helmex-update/#inputs","text":"Field name Field type Description Required gitRepo string GitHub repo containing the values.yaml file. The repo needs to begin with the prefix https:// . Yes filePath string Path to the Helm values file, relative to the root of this repo. Yes username string GitHub username. For organization account, this can also be an org name. Yes branch string Branch to be updated by this task. Default value is main . No secretName string This task requires a personal access token in order to modify the GitHub repo. secretName is the name of the Kubernetes secret which contains this token. Default value is ghtoken . No secretNamespace string Namespace where the above secret is located. Default value is the namespace of the experiment. No Note: The task above expects to find a key named token within the secret's Data section; i.e., secret.Data[\"token\"] needs to be the GitHub personal access token. In addition, the iter8-handler service account in the iter8-system namespace needs to be given read permissions using RBAC rules for this secret.","title":"Inputs"},{"location":"reference/tasks/gitops-helmex-update/#result","text":"The version recommended for promotion by Iter8 will be promoted as the new baseline in the values.yaml file. Suppose the values.yaml file in the GitHub repo is the same as the one in this example . Baseline is promoted Assuming baseline is recommended for promotion by Iter8, the new values.file in the GitHub repo after this task executes, will look as follows. Notice how the dynamic field differs between the two scenarios. common : application : hello repo : \"gcr.io/google-samples/hello-app\" serviceType : ClusterIP servicePortInfo : port : 8080 regularLabels : app.kubernetes.io/managed-by : Iter8 selectorLabels : app.kubernetes.io/name : hello baseline : name : hello selectorLabels : app.kubernetes.io/track : baseline dynamic : id : \"mn82l82\" tag : \"1.0\" # even though there is an experiment section below, there will be # no Iter8 experiment in the cluster, since there is no candidate version experiment : time : 5s QPS : 8.0 limitMeanLatency : 500.0 limitErrorRate : 0.01 limit95thPercentileLatency : 1000.0 Candidate is promoted Assuming candidate is recommended for promotion by Iter8, the new values.file in the GitHub repo after this task executes, will look as follows. Notice how the dynamic field differs between the two scenarios. common : application : hello repo : \"gcr.io/google-samples/hello-app\" serviceType : ClusterIP servicePortInfo : port : 8080 regularLabels : app.kubernetes.io/managed-by : Iter8 selectorLabels : app.kubernetes.io/name : hello baseline : name : hello selectorLabels : app.kubernetes.io/track : baseline dynamic : id : \"8s72oa\" tag : \"2.0\" # even though there is an experiment section below, there will be # no Iter8 experiment in the cluster, since there is no candidate version experiment : time : 5s QPS : 8.0 limitMeanLatency : 500.0 limitErrorRate : 0.01 limit95thPercentileLatency : 1000.0","title":"Result"},{"location":"reference/tasks/metrics-collect/","text":"metrics/collect \u00b6 The metrics/collect task enables collection of built-in metrics . It generates a stream of HTTP requests to one or more app/ML model versions, and collects latency and error metrics. Example \u00b6 The following start action contains a metrics/collect task which is executed at the start of the experiment. The task sends a certain number of HTTP requests to each version specified in the task, and collects built-in latency/error metrics for them. start : - task : metrics/collect with : versions : # Version names must be unique. # If the version name matches the name of a version in the experiment's `versionInfo` section, # then the version is considered *real*. # If the version name does not match the name of a version in the experiment's `versionInfo` section, # then the version is considered *pseudo*. # Built-in metrics collected for real versions can be used within the experiment's `criteria` section. # Pseudo versions are useful if the intent is only to generate load (`GET` and `POST` requests). # Built-in metrics collected for pseudo versions cannot be used with the experiment's `criteria` section. - name : iter8-app # URL is where this version receives HTTP requests url : http://iter8-app.default.svc:8000 - name : iter8-app-candidate url : http://iter8-app-candidate.default.svc:8000 Inputs \u00b6 Field name Field type Description Required time string Duration of the metrics/collect task run. Specified in the Go duration string format . Default value is 5s . No payloadURL string URL of JSON-encoded data. If this field is specified, the metrics collector will send HTTP POST requests to versions, and the POST requests will contain this JSON data as payload. No versions [] Version A non-empty list of versions. Yes loadOnly bool If set to true, this task will send requests without collecting metrics. Default value is false . No Version \u00b6 Field name Field type Description Required name string Name of the version. Version names must be unique. If the version name matches the name of a version in the experiment's versionInfo section, then the version is considered real . If the version name does not match the name of a version in the experiment's versionInfo section, then the version is considered pseudo . Built-in metrics collected for real versions can be used within the experiment's criteria section. Pseudo versions are useful if the intent is only to generate load ( GET and POST requests). Built-in metrics collected for pseudo versions cannot be used with the experiment's criteria section. Yes qps float How many queries per second will be sent to this version. Default is 8.0. No headers map[string]string HTTP headers to be used in requests sent to this version. No url string HTTP URL of this version. Yes Result \u00b6 This task will run for the specified duration ( time ), send requests to each version ( versions ) at the specified rate ( qps ), and will collect built-in metrics for each version. Built-in metric values are stored in the metrics field of the experiment status in the same manner as custom metric values. The task may result in an error, for instance, if one or more required fields are missing or if URLs are mis-specified. In this case, the experiment to which it belongs will fail. Start vs loop actions \u00b6 If this task is embedded in start actions, it will run once at the beginning of the experiment. If this task is embedded in loop actions, it will run in each loop of the experiment. The results from each run will be aggregated. Load generation without metrics collection \u00b6 You can use this task to send HTTP GET and POST requests to app/ML model versions without collecting metrics by setting the loadOnly input to true .","title":"metrics/collect"},{"location":"reference/tasks/metrics-collect/#metricscollect","text":"The metrics/collect task enables collection of built-in metrics . It generates a stream of HTTP requests to one or more app/ML model versions, and collects latency and error metrics.","title":"metrics/collect"},{"location":"reference/tasks/metrics-collect/#example","text":"The following start action contains a metrics/collect task which is executed at the start of the experiment. The task sends a certain number of HTTP requests to each version specified in the task, and collects built-in latency/error metrics for them. start : - task : metrics/collect with : versions : # Version names must be unique. # If the version name matches the name of a version in the experiment's `versionInfo` section, # then the version is considered *real*. # If the version name does not match the name of a version in the experiment's `versionInfo` section, # then the version is considered *pseudo*. # Built-in metrics collected for real versions can be used within the experiment's `criteria` section. # Pseudo versions are useful if the intent is only to generate load (`GET` and `POST` requests). # Built-in metrics collected for pseudo versions cannot be used with the experiment's `criteria` section. - name : iter8-app # URL is where this version receives HTTP requests url : http://iter8-app.default.svc:8000 - name : iter8-app-candidate url : http://iter8-app-candidate.default.svc:8000","title":"Example"},{"location":"reference/tasks/metrics-collect/#inputs","text":"Field name Field type Description Required time string Duration of the metrics/collect task run. Specified in the Go duration string format . Default value is 5s . No payloadURL string URL of JSON-encoded data. If this field is specified, the metrics collector will send HTTP POST requests to versions, and the POST requests will contain this JSON data as payload. No versions [] Version A non-empty list of versions. Yes loadOnly bool If set to true, this task will send requests without collecting metrics. Default value is false . No","title":"Inputs"},{"location":"reference/tasks/metrics-collect/#version","text":"Field name Field type Description Required name string Name of the version. Version names must be unique. If the version name matches the name of a version in the experiment's versionInfo section, then the version is considered real . If the version name does not match the name of a version in the experiment's versionInfo section, then the version is considered pseudo . Built-in metrics collected for real versions can be used within the experiment's criteria section. Pseudo versions are useful if the intent is only to generate load ( GET and POST requests). Built-in metrics collected for pseudo versions cannot be used with the experiment's criteria section. Yes qps float How many queries per second will be sent to this version. Default is 8.0. No headers map[string]string HTTP headers to be used in requests sent to this version. No url string HTTP URL of this version. Yes","title":"Version"},{"location":"reference/tasks/metrics-collect/#result","text":"This task will run for the specified duration ( time ), send requests to each version ( versions ) at the specified rate ( qps ), and will collect built-in metrics for each version. Built-in metric values are stored in the metrics field of the experiment status in the same manner as custom metric values. The task may result in an error, for instance, if one or more required fields are missing or if URLs are mis-specified. In this case, the experiment to which it belongs will fail.","title":"Result"},{"location":"reference/tasks/metrics-collect/#start-vs-loop-actions","text":"If this task is embedded in start actions, it will run once at the beginning of the experiment. If this task is embedded in loop actions, it will run in each loop of the experiment. The results from each run will be aggregated.","title":"Start vs loop actions"},{"location":"reference/tasks/metrics-collect/#load-generation-without-metrics-collection","text":"You can use this task to send HTTP GET and POST requests to app/ML model versions without collecting metrics by setting the loadOnly input to true .","title":"Load generation without metrics collection"},{"location":"reference/tasks/notification-http/","text":"notification/http \u00b6 The notification/http task can be used to send an HTTP request either as a form of notification or to trigger an action. For example, this task can be used to trigger a GitHub Action or a Tekton pipeline. Example \u00b6 The following is an experiment snippet with a notification/http task that triggers a GitHub action that takes two inputs. Here the inputs are set to the name and namespace of the experiment. ... spec : strategy : actions : finish : - task : notification/http with : url : https://api.github.com/repos/ORG/REPO/actions/workflows/ACTION.yaml/dispatches body : | { \"ref\":\"master\", \"inputs\":{ \"name\": \"{{.this.metadata.name}}\", \"home\":\"{{.this.metadata.namespace}}\" } } secret : default/github-token authType : Bearer headers : - name : Accept value : application/vnd.github.v3+json ... Inputs \u00b6 Field name Field type Description Required url string URL to which request is to be made. May contain placeholders that will be subsituted at runtime. Yes method string HTTP request method to use; either POST or GET . Default value is POST . No authtype string Type of authorization to use. Valid values are Basic , Bearer and APIKey . If not set, no authorization is used. No secret string Name of a secret (in form of namespace/name ). Values are used to dynamically substitute placeholders. No headers [] NamedValue A list of name-value pairs that are converted into headers as part of the request. Values may contain placeholders that will be subsituted at runtime. No body string The body of the request to be sent. May contain placeholders that will be subsituted at runtime. No ignoreFailure bool A flag indicating whether or not to ignore failures. If failures are not ignored, they cause the experiment to fail. Default is true . No The url , headers and body may all contain placeholders that will be substituted at runtime. A placeholder prefixed by .secret will use value from the secret. A placeholder prefixed by .this will use a value from the experiment. If not set, the default body will be of the following form: { \"summary\" : { \"winnerFound\" : true , \"winner\" : \"candidate\" , \"versionRecommededForPromotion\" : \"candidate\" , \"lastRecommendedWeights\" : [ { \"name\" : \"candiate\" \"weight\" : 95 }, { \"name\" : \"baseline\" \"weight\" : 5 } ] }, \"experiment\" : <JSON represe ntat io n o f t he experime nt objec t > } In the default body, the winner will be set only if winnerFound is true . The versionRecommededForPromotion field will be omitted in start actions but will be included thereafter. The weights are the last weights recommended by the analytics engine. Note that this may not match the current weights. No authoriziation is provided if authtype is not set. If it is set, the behavior is as follows: Basic : the task expects fields named username and password in the secret . These will be used to add an appropriate Authorization header to the request. Bearer : the expects a field named token in the secret . This will be used to construct a suitable Authorization header to the request. APIKey : the task expects the header field to explicitly specify any needed authorization headers. Placeholders can be used to explicitly refer to any required values from the secret . By default, a Content-type header with value application/json is included in the request. This can be replaced by specifying a different value. For example to set it by text/plain by: ... headers : - name : Content-type value : text/plain ... Result \u00b6 The task will create and send an HTTP request.","title":"notification/http"},{"location":"reference/tasks/notification-http/#notificationhttp","text":"The notification/http task can be used to send an HTTP request either as a form of notification or to trigger an action. For example, this task can be used to trigger a GitHub Action or a Tekton pipeline.","title":"notification/http"},{"location":"reference/tasks/notification-http/#example","text":"The following is an experiment snippet with a notification/http task that triggers a GitHub action that takes two inputs. Here the inputs are set to the name and namespace of the experiment. ... spec : strategy : actions : finish : - task : notification/http with : url : https://api.github.com/repos/ORG/REPO/actions/workflows/ACTION.yaml/dispatches body : | { \"ref\":\"master\", \"inputs\":{ \"name\": \"{{.this.metadata.name}}\", \"home\":\"{{.this.metadata.namespace}}\" } } secret : default/github-token authType : Bearer headers : - name : Accept value : application/vnd.github.v3+json ...","title":"Example"},{"location":"reference/tasks/notification-http/#inputs","text":"Field name Field type Description Required url string URL to which request is to be made. May contain placeholders that will be subsituted at runtime. Yes method string HTTP request method to use; either POST or GET . Default value is POST . No authtype string Type of authorization to use. Valid values are Basic , Bearer and APIKey . If not set, no authorization is used. No secret string Name of a secret (in form of namespace/name ). Values are used to dynamically substitute placeholders. No headers [] NamedValue A list of name-value pairs that are converted into headers as part of the request. Values may contain placeholders that will be subsituted at runtime. No body string The body of the request to be sent. May contain placeholders that will be subsituted at runtime. No ignoreFailure bool A flag indicating whether or not to ignore failures. If failures are not ignored, they cause the experiment to fail. Default is true . No The url , headers and body may all contain placeholders that will be substituted at runtime. A placeholder prefixed by .secret will use value from the secret. A placeholder prefixed by .this will use a value from the experiment. If not set, the default body will be of the following form: { \"summary\" : { \"winnerFound\" : true , \"winner\" : \"candidate\" , \"versionRecommededForPromotion\" : \"candidate\" , \"lastRecommendedWeights\" : [ { \"name\" : \"candiate\" \"weight\" : 95 }, { \"name\" : \"baseline\" \"weight\" : 5 } ] }, \"experiment\" : <JSON represe ntat io n o f t he experime nt objec t > } In the default body, the winner will be set only if winnerFound is true . The versionRecommededForPromotion field will be omitted in start actions but will be included thereafter. The weights are the last weights recommended by the analytics engine. Note that this may not match the current weights. No authoriziation is provided if authtype is not set. If it is set, the behavior is as follows: Basic : the task expects fields named username and password in the secret . These will be used to add an appropriate Authorization header to the request. Bearer : the expects a field named token in the secret . This will be used to construct a suitable Authorization header to the request. APIKey : the task expects the header field to explicitly specify any needed authorization headers. Placeholders can be used to explicitly refer to any required values from the secret . By default, a Content-type header with value application/json is included in the request. This can be replaced by specifying a different value. For example to set it by text/plain by: ... headers : - name : Content-type value : text/plain ...","title":"Inputs"},{"location":"reference/tasks/notification-http/#result","text":"The task will create and send an HTTP request.","title":"Result"},{"location":"reference/tasks/notification-slack/","text":"notification/slack \u00b6 The notification/slack task posts a Slack message about current state of the experiment. Example \u00b6 The following task notifies a Slack channel with id C0138103183 and using the token contained in the secret slack-token in the ns namespace. task : notification/slack with : channel : C0138103183 secret : ns/slack-token Inputs \u00b6 Field name Field type Description Required channel string Name of the Slack channel to which messages should be posted. Yes secret string Identifies a secret containing a token to be used for authentication. Expressed as namespace/name . If namespace is not specified, the namespace of the experiment is used. Yes ignoreFailure bool A flag indicating whether or not to ignore failures. If failures are not ignored, they cause the experiment to fail. Default is true . No Result \u00b6 A Slack message describing the experiment will be posted to the specified channel. Below is a sample Slack notification from this task. Requirements \u00b6 Slack API token \u00b6 An API token allowing posting messages to the desired Slack channel is needed. To obtain a suitable token, see Sending messages using Incoming Webhooks . Once you have the token, store it in a Kubernetes secret. For example, to create the secret slack-secret in the default namespace: kubectl create secret generic slack-secret --from-literal = token = <slack token> Permission to read secret with Slack token \u00b6 The Iter8 task runner needs permission to read the identified secret. For example the following RBAC changes will allow the task runner read the secret from the default namespace: kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/tasks/rbac/read-secrets.yaml Inspect role and rolebinding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # This role enables reading of secrets apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : iter8-secret-reader rules : - apiGroups : - \"\" resources : - secrets verbs : [ \"get\" , \"list\" ] --- # This role binding enables Iter8 handler to read secrets in the default namespace. # To change the namespace apply to the target namespace apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : iter8-secret-reader-handler roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : iter8-secret-reader subjects : - kind : ServiceAccount name : iter8-handlers namespace : iter8-system Slack channel ID \u00b6 A Slack channel is identified by an id. To find the id, open the Slack channel in a web browser. The channel id is the portion of the URL of the form: CXXXXXXXX","title":"notification/slack"},{"location":"reference/tasks/notification-slack/#notificationslack","text":"The notification/slack task posts a Slack message about current state of the experiment.","title":"notification/slack"},{"location":"reference/tasks/notification-slack/#example","text":"The following task notifies a Slack channel with id C0138103183 and using the token contained in the secret slack-token in the ns namespace. task : notification/slack with : channel : C0138103183 secret : ns/slack-token","title":"Example"},{"location":"reference/tasks/notification-slack/#inputs","text":"Field name Field type Description Required channel string Name of the Slack channel to which messages should be posted. Yes secret string Identifies a secret containing a token to be used for authentication. Expressed as namespace/name . If namespace is not specified, the namespace of the experiment is used. Yes ignoreFailure bool A flag indicating whether or not to ignore failures. If failures are not ignored, they cause the experiment to fail. Default is true . No","title":"Inputs"},{"location":"reference/tasks/notification-slack/#result","text":"A Slack message describing the experiment will be posted to the specified channel. Below is a sample Slack notification from this task.","title":"Result"},{"location":"reference/tasks/notification-slack/#requirements","text":"","title":"Requirements"},{"location":"reference/tasks/notification-slack/#slack-api-token","text":"An API token allowing posting messages to the desired Slack channel is needed. To obtain a suitable token, see Sending messages using Incoming Webhooks . Once you have the token, store it in a Kubernetes secret. For example, to create the secret slack-secret in the default namespace: kubectl create secret generic slack-secret --from-literal = token = <slack token>","title":"Slack API token"},{"location":"reference/tasks/notification-slack/#permission-to-read-secret-with-slack-token","text":"The Iter8 task runner needs permission to read the identified secret. For example the following RBAC changes will allow the task runner read the secret from the default namespace: kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/tasks/rbac/read-secrets.yaml Inspect role and rolebinding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # This role enables reading of secrets apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : iter8-secret-reader rules : - apiGroups : - \"\" resources : - secrets verbs : [ \"get\" , \"list\" ] --- # This role binding enables Iter8 handler to read secrets in the default namespace. # To change the namespace apply to the target namespace apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : iter8-secret-reader-handler roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : iter8-secret-reader subjects : - kind : ServiceAccount name : iter8-handlers namespace : iter8-system","title":"Permission to read secret with Slack token"},{"location":"reference/tasks/notification-slack/#slack-channel-id","text":"A Slack channel is identified by an id. To find the id, open the Slack channel in a web browser. The channel id is the portion of the URL of the form: CXXXXXXXX","title":"Slack channel ID"},{"location":"reference/tasks/overview/","text":"Tasks \u00b6 Tasks are an extension mechanism for enhancing the behavior of Iter8 experiments and can be specified within the spec.strategy.actions field of the experiment. Tasks are grouped into libraries, namely, common , metrics and notification . They are referenced in Iter8 experiments using the libraryName/taskName format. The following tasks are available for use in Iter8 experiments. common/readiness : Check if K8s resources needed for an experiment are available and/or ready. common/bash : Execute a bash script. common/exec (deprecated): Execute a shell command. metrics/collect : Generate GET/POST requests for the application versions and collect latency and error metrics. This task supports Iter8's built-in metrics feature. gitops/helmex-update : Update Helm values in a GitHub repo using git push . The Helm values file must satisfy the Helmex schema . notification/http : Send a HTTP request to a specified target. notification/slack : Send a slack notification with a summary of the experiment. Actions \u00b6 Tasks specified within start, finish, or loop actions with be executed by Iter8 at the start of an experiment, end of an experiment, or periodically during each loop of the experiment respectively. See spec.strategy.actions for details about experiment actions.","title":"Task overview"},{"location":"reference/tasks/overview/#tasks","text":"Tasks are an extension mechanism for enhancing the behavior of Iter8 experiments and can be specified within the spec.strategy.actions field of the experiment. Tasks are grouped into libraries, namely, common , metrics and notification . They are referenced in Iter8 experiments using the libraryName/taskName format. The following tasks are available for use in Iter8 experiments. common/readiness : Check if K8s resources needed for an experiment are available and/or ready. common/bash : Execute a bash script. common/exec (deprecated): Execute a shell command. metrics/collect : Generate GET/POST requests for the application versions and collect latency and error metrics. This task supports Iter8's built-in metrics feature. gitops/helmex-update : Update Helm values in a GitHub repo using git push . The Helm values file must satisfy the Helmex schema . notification/http : Send a HTTP request to a specified target. notification/slack : Send a slack notification with a summary of the experiment.","title":"Tasks"},{"location":"reference/tasks/overview/#actions","text":"Tasks specified within start, finish, or loop actions with be executed by Iter8 at the start of an experiment, end of an experiment, or periodically during each loop of the experiment respectively. See spec.strategy.actions for details about experiment actions.","title":"Actions"},{"location":"tutorials/istio/platform-setup/","text":"Platform Setup for Istio \u00b6 1. Create Kubernetes cluster \u00b6 Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288 2. Clone Iter8 repo \u00b6 git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) 3. Install Istio, Iter8 and Telemetry \u00b6 Setup Istio, Iter8, and Prometheus add-on within your cluster. $ITER8 /samples/istio/quickstart/platformsetup.sh","title":"Platform setup"},{"location":"tutorials/istio/platform-setup/#platform-setup-for-istio","text":"","title":"Platform Setup for Istio"},{"location":"tutorials/istio/platform-setup/#1-create-kubernetes-cluster","text":"Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288","title":"1. Create Kubernetes cluster"},{"location":"tutorials/istio/platform-setup/#2-clone-iter8-repo","text":"git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd )","title":"2. Clone Iter8 repo"},{"location":"tutorials/istio/platform-setup/#3-install-istio-iter8-and-telemetry","text":"Setup Istio, Iter8, and Prometheus add-on within your cluster. $ITER8 /samples/istio/quickstart/platformsetup.sh","title":"3. Install Istio, Iter8 and Telemetry"},{"location":"tutorials/istio/quick-start/","text":"Hybrid (A/B + SLOs) testing \u00b6 Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Specify latency and error-rate based objectives; data for these metrics will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster. 1. Create application versions \u00b6 Deploy the bookinfo microservice application including two versions of the productpage microservice. kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Look inside productpage-v2.yaml (v1 is similar) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v2 labels : app : productpage version : v2 spec : replicas : 1 selector : matchLabels : app : productpage version : v2 template : metadata : annotations : sidecar.istio.io/inject : \"true\" labels : app : productpage version : v2 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v2\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"green\" - name : reward_min value : \"10\" - name : reward_max value : \"20\" - name : port value : \"9080\" 2. Generate requests \u00b6 Generate requests to your app using Fortio as follows. # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 3. Define metrics \u00b6 Iter8 introduces a Kubernetes CRD called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. Define the Iter8 metrics used in this experiment as follows. For the purpose of this tutorial, you will mock the user-engagement metric. The latency and error metrics will be provided by Prometheus. kubectl apply -f $ITER8 /samples/istio/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 apiVersion : v1 kind : Namespace metadata : labels : creator : iter8 stack : istio name : iter8-istio --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-istio spec : mock : - name : productpage-v1 level : 15.0 - name : productpage-v2 level : 20.0 --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count namespace : iter8-istio spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate namespace : iter8-istio spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : le500ms-latency-percentile namespace : iter8-istio spec : description : Less than 500 ms latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_bucket{le='500',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_request_duration_milliseconds_bucket{le='+Inf',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-istio/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency namespace : iter8-istio spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_sum{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : iter8-istio spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked metric used in this tutorial with any custom metric you wish to optimize. Documentation on defining custom metrics is here . 4. Launch experiment \u00b6 Iter8 defines a custom K8s resource called Experiment that automates a variety of release engineering and experimentation strategies for K8s applications and ML models. Launch the Hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. kubectl apply -f $ITER8 /samples/istio/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/experiment.yaml kubectl delete namespace bookinfo-iter8","title":"Quick start"},{"location":"tutorials/istio/quick-start/#hybrid-ab-slos-testing","text":"Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Specify latency and error-rate based objectives; data for these metrics will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster.","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/istio/quick-start/#1-create-application-versions","text":"Deploy the bookinfo microservice application including two versions of the productpage microservice. kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Look inside productpage-v2.yaml (v1 is similar) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v2 labels : app : productpage version : v2 spec : replicas : 1 selector : matchLabels : app : productpage version : v2 template : metadata : annotations : sidecar.istio.io/inject : \"true\" labels : app : productpage version : v2 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v2\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"green\" - name : reward_min value : \"10\" - name : reward_max value : \"20\" - name : port value : \"9080\"","title":"1. Create application versions"},{"location":"tutorials/istio/quick-start/#2-generate-requests","text":"Generate requests to your app using Fortio as follows. # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"2. Generate requests"},{"location":"tutorials/istio/quick-start/#3-define-metrics","text":"Iter8 introduces a Kubernetes CRD called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. Define the Iter8 metrics used in this experiment as follows. For the purpose of this tutorial, you will mock the user-engagement metric. The latency and error metrics will be provided by Prometheus. kubectl apply -f $ITER8 /samples/istio/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 apiVersion : v1 kind : Namespace metadata : labels : creator : iter8 stack : istio name : iter8-istio --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-istio spec : mock : - name : productpage-v1 level : 15.0 - name : productpage-v2 level : 20.0 --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count namespace : iter8-istio spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate namespace : iter8-istio spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : le500ms-latency-percentile namespace : iter8-istio spec : description : Less than 500 ms latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_bucket{le='500',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_request_duration_milliseconds_bucket{le='+Inf',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-istio/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency namespace : iter8-istio spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_sum{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : iter8-istio spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked metric used in this tutorial with any custom metric you wish to optimize. Documentation on defining custom metrics is here .","title":"3. Define metrics"},{"location":"tutorials/istio/quick-start/#4-launch-experiment","text":"Iter8 defines a custom K8s resource called Experiment that automates a variety of release engineering and experimentation strategies for K8s applications and ML models. Launch the Hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. kubectl apply -f $ITER8 /samples/istio/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight","title":"4. Launch experiment"},{"location":"tutorials/istio/quick-start/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/istio/quick-start/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/experiment.yaml kubectl delete namespace bookinfo-iter8","title":"6. Cleanup"},{"location":"tutorials/istio/gitops/argocd/","text":"GitOps with Argo CD \u00b6 Scenario: GitOps GitOps methodology is being widely used in CI/CD pipelines in Kubernetes-based environments to ease cluster management tasks. When using this methodology, the desired states of one or more clusters are kept in a Git repo, and a CD pipeline tool will continuously monitor for changes in the repo and sync them to the clusters. Additionally, it is preferred that Git repos are structured in a certain way so that the code repo is separated from the environment (Env) repo. Commits to the code repo trigger the CI pipeline tool to build, test, lint, and eventually push newly built images to an image repository. The Env repo contains configuration files that describe how various resources should be deployed in the cluster. Subsequently, configurations in the Env repo are updated to point to the newly built images. And finally, the CD pipeline tool will sync the new desired states to the clusters. This process is shown below: Scenario: Iter8+Gitops Iter8 can be used in the context of GitOps (shown below) so that new versions of an application can be progressively rolled out, or even rolled back when problems are detected. In this tutorial, we will use Argo CD as the CD pipeline tool and Istio as the underlying service mesh, We assume the reader has at least a basic understanding of how Iter8 works from the quick start tutorial . Since the Env repo is at the heart of GitOps, we will focus mainly on how to setup and manage the Env repo during application updates. In this tutorial, we will cover the following topics. How to setup an Env repo to work with Iter8+GitOps How to update the Env repo to start an Iter8 experiment How to cleanup the Env repo after an Iter8 experiment is finished Iter8 GitOps Guarantees Unlike other progressive delivery tools, Iter8 adheres to GitOps' guarantees by ensuring the actual state is always in sync with the desired state. App versions that fail promotion criteria will never get promoted, even if the cluster had to be recreated from scratch. This important GitOps property is often not guaranteed by other tools! Step 1. Create K8s cluster \u00b6 If you don't already have a K8s cluster, create a Minikube or Kind K8s cluster locally . Step 2. Fork repo \u00b6 As you will need to make changes to the Env repo to test new app versions, you will need your own copy of the repo. Fork this repo: https://github.com/iter8-tools/iter8 Now you should have your own Env repo at: https://github.com/[YOUR_ORG]/iter8 Step 3. Install platform components \u00b6 In your K8s cluster, you will need to install Istio, Prometheus, Iter8, and Argo CD. Replace [YOUR_ORG] with your GitHub organization or username and run the following script to install these: git clone https://github.com/ [ YOUR_ORG ] /iter8.git cd iter8 export ITER8 = $( pwd ) $ITER8 /samples/istio/gitops/platformsetup.sh Now, do the same replacement operation to update references in the repo so they will point at your forked repo. MacOS find $ITER8 /samples/istio/gitops -name \"*\" -type f | xargs sed -i '' \"s/MY_ORG/YOUR_ORG/\" git commit -a -m \"update references\" git push origin head Linux find $ITER8 /samples/istio/gitops -name \"*\" -type f | xargs sed -i \"s/MY_ORG/YOUR_ORG/\" git commit -a -m \"update references\" git push origin head Step 4. Argo CD Setup \u00b6 The output from the previous step will provide instructions on how to access the Argo CD UI to setup your Argo CD app. You might see something similar to: Your Argo CD installation is complete Run the following commands: 1 . kubectl port-forward svc/argocd-server -n argocd 8080 :443 2 . Open a browser with URL: http://localhost:8080 with the following credential Username: 'admin' , Password: 'xxxxxxxxxx' Start the port-forward in a new terminal, and access the Argo CD UI from your browser. After logging in, you should see Argo CD showing no application is currently installed. To install the bookinfo application we will use for this tutorial, run the following: kubectl apply -f $ITER8 /samples/istio/gitops/argocd-app.yaml Now the Argo CD UI should show that a new app called gitops has been created. Make sure it is showing both Healthy and Synced - this might take a few minutes. Step 5. Setup GitHub token \u00b6 At the end of an experiment, Iter8 will need to update the Env repo so the winner of the experiment becomes the new baseline (it will also need to perform various clean up tasks in the Env repo -- we will discuss these later). To accomplish this, Iter8 will need to have access to your Env repo, so it can make the necessary changes by creating PRs. First, login to GitHub , and from the upper right corner of the page, go to Settings > Developer settings > Personal access token > Generate new token. Make sure the token is granted access for repo.public_repo . Now create a K8s secret from the token so that Iter8 can use it at runtime. Run the following (replace the token string with your own token): kubectl create secret generic iter8-token --from-literal = token = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Step 6. Start experiment \u00b6 When new images become available and/or new configurations need to be tested, the CI pipeline tool (or some other entity) will make changes to the Env repo, so the new desired states can be deployed into the cluster. To use Iter8 to perform progressive traffic shifts, the CI pipeline tool will need to make a few additional changes in the Env repo. Specifically, it will need to create at least the following resources: A candidate deployment An Iter8 experiment (Optionally) A workload generator These are the same resources you would need to create even in an non-GitOps setting. To simplify this step in the tutorial, we included a runCI.sh script that creates these three resources. Run the following: ( cd $ITER8 /samples/istio/gitops ; ./runCI.sh ) To start an Iter8 experiment, you need to commit these changes to the Env repo for Argo CD to sync them to the cluster. Run the following: git add -A . ; git commit -m \"iter8 experiment\" ; git push origin head By default Argo CD is configured to run every three minutes, so if you don't want to wait, you can use the Argo CD UI to force a manual refresh so the changes to the Env repo can be immediately synced to the cluster. More about runCI.sh runCI.sh (shown as below) creates resource files from templates using sed . 1 2 3 4 5 6 7 8 9 10 11 12 13 # give fortio deployment a random name so it restarts on a new experiment RANDOM = ` od -An -N4 -i /dev/random ` sed \"s| name: fortio-| name: fortio- $RANDOM |\" templates/fortio.yaml > ./fortio.yaml # give experiment a random name so CI triggers new experiment each time a new app version is available sed \"s|name: gitops-exp|name: gitops-exp- $RANDOM |\" templates/experiment.yaml > ./experiment.yaml # use a random color for a new experiment candidate declare -a colors =( \"red\" \"orange\" \"blue\" \"green\" \"yellow\" \"violet\" \"brown\" ) color = ` expr $RANDOM % ${# colors [@] } ` version = ` git rev-parse --short HEAD ` sed \"s|value: COLOR|value: \\\" ${ colors [ $color ] } \\\"|\" templates/productpage-candidate.yaml | \\ sed \"s|version: v.*|version: v $version |\" > ./productpage-candidate.yaml Both fortio.yaml and experiment.yaml are almost identical to their templates in the templates/ subdirectory. The only change we made was to give it a random name because we want each new experiment to preempt any running experiment and workload generator when Argo CD syncs. We could have used a simpler cp command if Argo CD supported generateName field better. However, in its current version, it cannot correctly associate resources with generateName field in the Env repo with those created in the cluster, so we had to resort to using sed . The candidate deployment is also templated from the templates/ subdirectory, and we simply use the current commit ID as its version and assign it a random color to use. 7. Finish experiment \u00b6 The experiment should run for a few minutes once it starts, and one can run the following command to track its progress: watch kubectl get experiments.iter8.tools Once the experiment finishes, check https://github.com/[YOUR_ORG]/iter8/pulls . Iter8 should have created a new PR titled Iter8 GitOps . File diffs from the PR should show clearly what Iter8 is proposing to change in the Env repo. Regardless which version is the winner, Iter8 will always clean up the Env repo after an experiment is finished. Specifically, the files created by the CI pipeline at the start of the experiment will be deleted, i.e., experiment.yaml , fortio.yaml , and productpage-candidate.yaml , to essentially put the Env repo back to its initial state. Additionally, if the candidate meets all the success criteria of the experiment, productpage.yaml will be updated to reflect the new baseline version. You can now merge the PR that Iter8 just created. Argo CD should detect the change and sync the cluster to the new desired states. If the experiment succeeded, the candidate version will become the new baseline version for future experiments. More about Iter8 GitOps task Iter8 operating in the GitOps mode is very similar to how it works normally. One key difference is that at the end of the experiment, it will need to perform an additional step to modify the desired state of the Env repo to reflect the outcome of the experiment. This can be done by specifying a finish task that runs at the end of an experiment. The specific task we are using in this tutorial is written in a shell script as shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 actions : # when the experiment completes, promote the winning version in the Env repo finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"apt-get install -y git jq curl;\\ REPO=github.com/huang195/iter8;\\ BRANCH=gitops;\\ USER=huang195;\\ TOKEN=`kubectl -n {{ .namespace }} get secret iter8-token -o json | jq -r .data.token | base64 -d`;\\ git config --global user.email 'iter8@iter8.tools';\\ git config --global user.name 'Iter8';\\ git clone https://${USER}:${TOKEN}@${REPO} --branch=${BRANCH};\\ cd iter8/samples/istio/gitops;\\ TMP=`mktemp`;\\ sed 's/candidate/stable/g' {{ .filepath }} > $TMP;\\ mv $TMP productpage.yaml;\\ rm -f productpage-candidate.yaml;\\ rm -f fortio.yaml;\\ rm -f experiment.yaml;\\ if [ `git status -s | wc -l` != 0 ];\\ then \\ git checkout -b iter8_exp;\\ git commit -a -m 'update baseline';\\ git push -f origin iter8_exp;\\ curl -u${USER}:${TOKEN} -XPOST https://api.github.com/repos/iter8-tools/iter8/pulls -s -d '{\\\"head\\\":\\\"iter8_exp\\\", \\\"base\\\":\\\"gitops\\\", \\\"body\\\":\\\"update baseline\\\", \\\"title\\\":\\\"Iter8 GitOps\\\"}';\\ fi;\\ kubectl -n {{ .namespace }} apply -f istio-vs.yaml;\\ \" ] For prototyping, one can write these tasks as shell scripts and inline them within an Experiment CR. This makes writing these tasks efficient and easy to debug. However, the down side is it makes the Experiment CR a lot more complicated and scary to read. We are currently working to simplify this interface, so stay tuned. 8. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/istio/gitops/ kubectl delete ns istio-system kubectl delete ns iter8-system kubectl delete ns argocd 9. Additional details \u00b6 Env repo setup \u00b6 In GitOps, it's generally a good idea to use multiple repos separating code from environment configurations. In cases where the same repo is being used for both, one needs to be careful when configuring CI/CD pipeline tools so code changes can be differentiated from configuration changes, so that one doesn't inadvertently create infinite loops. The Env repo can be organized in many different ways. With tools such as Helm and Kustomize becoming widely used, it becomes even simpler for CI pipeline tools to update an Env repo to roll out new app versions. In this tutorial, we consciously decided to use the simplest directory structure (i.e., all YAML files within a single directory without subdirectories) without the use of any higher level templating tools. Adapting the basic directory structure to Helm/Kustomize should be fairly straight forward. When organizing the directory structure, one needs to keep in mind that the CI pipeline tool will be creating new resources in the Env repo to start an Iter8 experiment. And when the experiment finishes, Iter8 (specifically, Iter8 tasks) will delete the added resources and update the baseline version in the Env repo. In other words, the invariant here is the directory structure, which should stay the same before and after an experiment. GitOps support for multiple environments \u00b6 Some users might use GitOps to manage multiple environments, e.g., dev, staging, prod, so changes can always propagate from environment to environment, minimizing the chance of defects from reaching the prod environment. In this setup, the Iter8 GitOps task would need to be modified so that Env repo changes are done at the correct places. For example, if different environments are managed by different Env repos, the task would need to make multiple git commits, one for each of the repos. This could be done all within a single task, or across multiple tasks. Caveats \u00b6 Both CI pipeline tools and Iter8 need to write to the Env repo in GitOps, and if not coordinated, race conditions could occur. In this tutorial, we assume repo changes are done via PRs, which is a common practice, so the chance of having a race condition is minimized, if not eliminated. However, other means to coordinate writes to the Env repo by different entities can be done so Iter8 can operate in fully automated pipelines. When a new app version becomes available while an experiment is still running, Iter8 will preempt the existing experiment with the new one. We currently don't support test-every-commit behavior by queuing new experiments, but this could be supported in the future if it turrned out to be more common than we are currently expecting. Iter8 task could fail, just like everything else. Iter8 tasks are currently fail-stop without retries. Please take this into account when writing Iter8 tasks and error handling code.","title":"Argo CD + Istio"},{"location":"tutorials/istio/gitops/argocd/#gitops-with-argo-cd","text":"Scenario: GitOps GitOps methodology is being widely used in CI/CD pipelines in Kubernetes-based environments to ease cluster management tasks. When using this methodology, the desired states of one or more clusters are kept in a Git repo, and a CD pipeline tool will continuously monitor for changes in the repo and sync them to the clusters. Additionally, it is preferred that Git repos are structured in a certain way so that the code repo is separated from the environment (Env) repo. Commits to the code repo trigger the CI pipeline tool to build, test, lint, and eventually push newly built images to an image repository. The Env repo contains configuration files that describe how various resources should be deployed in the cluster. Subsequently, configurations in the Env repo are updated to point to the newly built images. And finally, the CD pipeline tool will sync the new desired states to the clusters. This process is shown below: Scenario: Iter8+Gitops Iter8 can be used in the context of GitOps (shown below) so that new versions of an application can be progressively rolled out, or even rolled back when problems are detected. In this tutorial, we will use Argo CD as the CD pipeline tool and Istio as the underlying service mesh, We assume the reader has at least a basic understanding of how Iter8 works from the quick start tutorial . Since the Env repo is at the heart of GitOps, we will focus mainly on how to setup and manage the Env repo during application updates. In this tutorial, we will cover the following topics. How to setup an Env repo to work with Iter8+GitOps How to update the Env repo to start an Iter8 experiment How to cleanup the Env repo after an Iter8 experiment is finished Iter8 GitOps Guarantees Unlike other progressive delivery tools, Iter8 adheres to GitOps' guarantees by ensuring the actual state is always in sync with the desired state. App versions that fail promotion criteria will never get promoted, even if the cluster had to be recreated from scratch. This important GitOps property is often not guaranteed by other tools!","title":"GitOps with Argo CD"},{"location":"tutorials/istio/gitops/argocd/#step-1-create-k8s-cluster","text":"If you don't already have a K8s cluster, create a Minikube or Kind K8s cluster locally .","title":"Step 1. Create K8s cluster"},{"location":"tutorials/istio/gitops/argocd/#step-2-fork-repo","text":"As you will need to make changes to the Env repo to test new app versions, you will need your own copy of the repo. Fork this repo: https://github.com/iter8-tools/iter8 Now you should have your own Env repo at: https://github.com/[YOUR_ORG]/iter8","title":"Step 2. Fork repo"},{"location":"tutorials/istio/gitops/argocd/#step-3-install-platform-components","text":"In your K8s cluster, you will need to install Istio, Prometheus, Iter8, and Argo CD. Replace [YOUR_ORG] with your GitHub organization or username and run the following script to install these: git clone https://github.com/ [ YOUR_ORG ] /iter8.git cd iter8 export ITER8 = $( pwd ) $ITER8 /samples/istio/gitops/platformsetup.sh Now, do the same replacement operation to update references in the repo so they will point at your forked repo. MacOS find $ITER8 /samples/istio/gitops -name \"*\" -type f | xargs sed -i '' \"s/MY_ORG/YOUR_ORG/\" git commit -a -m \"update references\" git push origin head Linux find $ITER8 /samples/istio/gitops -name \"*\" -type f | xargs sed -i \"s/MY_ORG/YOUR_ORG/\" git commit -a -m \"update references\" git push origin head","title":"Step 3. Install platform components"},{"location":"tutorials/istio/gitops/argocd/#step-4-argo-cd-setup","text":"The output from the previous step will provide instructions on how to access the Argo CD UI to setup your Argo CD app. You might see something similar to: Your Argo CD installation is complete Run the following commands: 1 . kubectl port-forward svc/argocd-server -n argocd 8080 :443 2 . Open a browser with URL: http://localhost:8080 with the following credential Username: 'admin' , Password: 'xxxxxxxxxx' Start the port-forward in a new terminal, and access the Argo CD UI from your browser. After logging in, you should see Argo CD showing no application is currently installed. To install the bookinfo application we will use for this tutorial, run the following: kubectl apply -f $ITER8 /samples/istio/gitops/argocd-app.yaml Now the Argo CD UI should show that a new app called gitops has been created. Make sure it is showing both Healthy and Synced - this might take a few minutes.","title":"Step 4. Argo CD Setup"},{"location":"tutorials/istio/gitops/argocd/#step-5-setup-github-token","text":"At the end of an experiment, Iter8 will need to update the Env repo so the winner of the experiment becomes the new baseline (it will also need to perform various clean up tasks in the Env repo -- we will discuss these later). To accomplish this, Iter8 will need to have access to your Env repo, so it can make the necessary changes by creating PRs. First, login to GitHub , and from the upper right corner of the page, go to Settings > Developer settings > Personal access token > Generate new token. Make sure the token is granted access for repo.public_repo . Now create a K8s secret from the token so that Iter8 can use it at runtime. Run the following (replace the token string with your own token): kubectl create secret generic iter8-token --from-literal = token = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx","title":"Step 5. Setup GitHub token"},{"location":"tutorials/istio/gitops/argocd/#step-6-start-experiment","text":"When new images become available and/or new configurations need to be tested, the CI pipeline tool (or some other entity) will make changes to the Env repo, so the new desired states can be deployed into the cluster. To use Iter8 to perform progressive traffic shifts, the CI pipeline tool will need to make a few additional changes in the Env repo. Specifically, it will need to create at least the following resources: A candidate deployment An Iter8 experiment (Optionally) A workload generator These are the same resources you would need to create even in an non-GitOps setting. To simplify this step in the tutorial, we included a runCI.sh script that creates these three resources. Run the following: ( cd $ITER8 /samples/istio/gitops ; ./runCI.sh ) To start an Iter8 experiment, you need to commit these changes to the Env repo for Argo CD to sync them to the cluster. Run the following: git add -A . ; git commit -m \"iter8 experiment\" ; git push origin head By default Argo CD is configured to run every three minutes, so if you don't want to wait, you can use the Argo CD UI to force a manual refresh so the changes to the Env repo can be immediately synced to the cluster. More about runCI.sh runCI.sh (shown as below) creates resource files from templates using sed . 1 2 3 4 5 6 7 8 9 10 11 12 13 # give fortio deployment a random name so it restarts on a new experiment RANDOM = ` od -An -N4 -i /dev/random ` sed \"s| name: fortio-| name: fortio- $RANDOM |\" templates/fortio.yaml > ./fortio.yaml # give experiment a random name so CI triggers new experiment each time a new app version is available sed \"s|name: gitops-exp|name: gitops-exp- $RANDOM |\" templates/experiment.yaml > ./experiment.yaml # use a random color for a new experiment candidate declare -a colors =( \"red\" \"orange\" \"blue\" \"green\" \"yellow\" \"violet\" \"brown\" ) color = ` expr $RANDOM % ${# colors [@] } ` version = ` git rev-parse --short HEAD ` sed \"s|value: COLOR|value: \\\" ${ colors [ $color ] } \\\"|\" templates/productpage-candidate.yaml | \\ sed \"s|version: v.*|version: v $version |\" > ./productpage-candidate.yaml Both fortio.yaml and experiment.yaml are almost identical to their templates in the templates/ subdirectory. The only change we made was to give it a random name because we want each new experiment to preempt any running experiment and workload generator when Argo CD syncs. We could have used a simpler cp command if Argo CD supported generateName field better. However, in its current version, it cannot correctly associate resources with generateName field in the Env repo with those created in the cluster, so we had to resort to using sed . The candidate deployment is also templated from the templates/ subdirectory, and we simply use the current commit ID as its version and assign it a random color to use.","title":"Step 6. Start experiment"},{"location":"tutorials/istio/gitops/argocd/#7-finish-experiment","text":"The experiment should run for a few minutes once it starts, and one can run the following command to track its progress: watch kubectl get experiments.iter8.tools Once the experiment finishes, check https://github.com/[YOUR_ORG]/iter8/pulls . Iter8 should have created a new PR titled Iter8 GitOps . File diffs from the PR should show clearly what Iter8 is proposing to change in the Env repo. Regardless which version is the winner, Iter8 will always clean up the Env repo after an experiment is finished. Specifically, the files created by the CI pipeline at the start of the experiment will be deleted, i.e., experiment.yaml , fortio.yaml , and productpage-candidate.yaml , to essentially put the Env repo back to its initial state. Additionally, if the candidate meets all the success criteria of the experiment, productpage.yaml will be updated to reflect the new baseline version. You can now merge the PR that Iter8 just created. Argo CD should detect the change and sync the cluster to the new desired states. If the experiment succeeded, the candidate version will become the new baseline version for future experiments. More about Iter8 GitOps task Iter8 operating in the GitOps mode is very similar to how it works normally. One key difference is that at the end of the experiment, it will need to perform an additional step to modify the desired state of the Env repo to reflect the outcome of the experiment. This can be done by specifying a finish task that runs at the end of an experiment. The specific task we are using in this tutorial is written in a shell script as shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 actions : # when the experiment completes, promote the winning version in the Env repo finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"apt-get install -y git jq curl;\\ REPO=github.com/huang195/iter8;\\ BRANCH=gitops;\\ USER=huang195;\\ TOKEN=`kubectl -n {{ .namespace }} get secret iter8-token -o json | jq -r .data.token | base64 -d`;\\ git config --global user.email 'iter8@iter8.tools';\\ git config --global user.name 'Iter8';\\ git clone https://${USER}:${TOKEN}@${REPO} --branch=${BRANCH};\\ cd iter8/samples/istio/gitops;\\ TMP=`mktemp`;\\ sed 's/candidate/stable/g' {{ .filepath }} > $TMP;\\ mv $TMP productpage.yaml;\\ rm -f productpage-candidate.yaml;\\ rm -f fortio.yaml;\\ rm -f experiment.yaml;\\ if [ `git status -s | wc -l` != 0 ];\\ then \\ git checkout -b iter8_exp;\\ git commit -a -m 'update baseline';\\ git push -f origin iter8_exp;\\ curl -u${USER}:${TOKEN} -XPOST https://api.github.com/repos/iter8-tools/iter8/pulls -s -d '{\\\"head\\\":\\\"iter8_exp\\\", \\\"base\\\":\\\"gitops\\\", \\\"body\\\":\\\"update baseline\\\", \\\"title\\\":\\\"Iter8 GitOps\\\"}';\\ fi;\\ kubectl -n {{ .namespace }} apply -f istio-vs.yaml;\\ \" ] For prototyping, one can write these tasks as shell scripts and inline them within an Experiment CR. This makes writing these tasks efficient and easy to debug. However, the down side is it makes the Experiment CR a lot more complicated and scary to read. We are currently working to simplify this interface, so stay tuned.","title":"7. Finish experiment"},{"location":"tutorials/istio/gitops/argocd/#8-cleanup","text":"kubectl delete -f $ITER8 /samples/istio/gitops/ kubectl delete ns istio-system kubectl delete ns iter8-system kubectl delete ns argocd","title":"8. Cleanup"},{"location":"tutorials/istio/gitops/argocd/#9-additional-details","text":"","title":"9. Additional details"},{"location":"tutorials/istio/gitops/argocd/#env-repo-setup","text":"In GitOps, it's generally a good idea to use multiple repos separating code from environment configurations. In cases where the same repo is being used for both, one needs to be careful when configuring CI/CD pipeline tools so code changes can be differentiated from configuration changes, so that one doesn't inadvertently create infinite loops. The Env repo can be organized in many different ways. With tools such as Helm and Kustomize becoming widely used, it becomes even simpler for CI pipeline tools to update an Env repo to roll out new app versions. In this tutorial, we consciously decided to use the simplest directory structure (i.e., all YAML files within a single directory without subdirectories) without the use of any higher level templating tools. Adapting the basic directory structure to Helm/Kustomize should be fairly straight forward. When organizing the directory structure, one needs to keep in mind that the CI pipeline tool will be creating new resources in the Env repo to start an Iter8 experiment. And when the experiment finishes, Iter8 (specifically, Iter8 tasks) will delete the added resources and update the baseline version in the Env repo. In other words, the invariant here is the directory structure, which should stay the same before and after an experiment.","title":"Env repo setup"},{"location":"tutorials/istio/gitops/argocd/#gitops-support-for-multiple-environments","text":"Some users might use GitOps to manage multiple environments, e.g., dev, staging, prod, so changes can always propagate from environment to environment, minimizing the chance of defects from reaching the prod environment. In this setup, the Iter8 GitOps task would need to be modified so that Env repo changes are done at the correct places. For example, if different environments are managed by different Env repos, the task would need to make multiple git commits, one for each of the repos. This could be done all within a single task, or across multiple tasks.","title":"GitOps support for multiple environments"},{"location":"tutorials/istio/gitops/argocd/#caveats","text":"Both CI pipeline tools and Iter8 need to write to the Env repo in GitOps, and if not coordinated, race conditions could occur. In this tutorial, we assume repo changes are done via PRs, which is a common practice, so the chance of having a race condition is minimized, if not eliminated. However, other means to coordinate writes to the Env repo by different entities can be done so Iter8 can operate in fully automated pipelines. When a new app version becomes available while an experiment is still running, Iter8 will preempt the existing experiment with the new one. We currently don't support test-every-commit behavior by queuing new experiments, but this could be supported in the future if it turrned out to be more common than we are currently expecting. Iter8 task could fail, just like everything else. Iter8 tasks are currently fail-stop without retries. Please take this into account when writing Iter8 tasks and error handling code.","title":"Caveats"},{"location":"tutorials/istio/rollout-strategies/fixed-split/","text":"Fixed % Split \u00b6 Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster. 1. Create versions and fix traffic split \u00b6 kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/fixed-split/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : bookinfo spec : gateways : - mesh - bookinfo-gateway hosts : - productpage - \"bookinfo.example.com\" http : - match : - uri : exact : /productpage - uri : prefix : /static - uri : exact : /login - uri : exact : /logout - uri : prefix : /api/v1/products route : - destination : host : productpage port : number : 9080 subset : productpage-v1 weight : 60 - destination : host : productpage port : number : 9080 subset : productpage-v2 weight : 40 2. Steps 2 and 3 \u00b6 Please follow Steps 2 and 3 of the quick start tutorial . 4. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/istio/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will not shift traffic during iterations deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 300 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/istio/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete ns bookinfo-iter8","title":"Fixed-%-split"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#fixed-split","text":"Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster.","title":"Fixed % Split"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#1-create-versions-and-fix-traffic-split","text":"kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/fixed-split/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : bookinfo spec : gateways : - mesh - bookinfo-gateway hosts : - productpage - \"bookinfo.example.com\" http : - match : - uri : exact : /productpage - uri : prefix : /static - uri : exact : /login - uri : exact : /logout - uri : prefix : /api/v1/products route : - destination : host : productpage port : number : 9080 subset : productpage-v1 weight : 60 - destination : host : productpage port : number : 9080 subset : productpage-v2 weight : 40","title":"1. Create versions and fix traffic split"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#2-steps-2-and-3","text":"Please follow Steps 2 and 3 of the quick start tutorial .","title":"2. Steps 2 and 3"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#4-launch-experiment","text":"kubectl apply -f $ITER8 /samples/istio/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will not shift traffic during iterations deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 300 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml","title":"4. Launch experiment"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/istio/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete ns bookinfo-iter8","title":"6. Cleanup"},{"location":"tutorials/istio/rollout-strategies/progressive/","text":"Progressive Traffic Shift \u00b6 Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below. Tutorials with progressive traffic shift \u00b6 The hybrid testing (quick start) and the SLO validation tutorials demonstrate progressive traffic shift. Specifying weightObjRef \u00b6 Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The hybrid (A/B + SLOs) testing quick start tutorial for Istio uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this virtual service and the traffic fields within the virtual service corresponding to the versions. versionInfo : baseline : name : productpage-v1 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight Traffic controls \u00b6 You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Progressive traffic shift"},{"location":"tutorials/istio/rollout-strategies/progressive/#progressive-traffic-shift","text":"Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below.","title":"Progressive Traffic Shift"},{"location":"tutorials/istio/rollout-strategies/progressive/#tutorials-with-progressive-traffic-shift","text":"The hybrid testing (quick start) and the SLO validation tutorials demonstrate progressive traffic shift.","title":"Tutorials with progressive traffic shift"},{"location":"tutorials/istio/rollout-strategies/progressive/#specifying-weightobjref","text":"Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The hybrid (A/B + SLOs) testing quick start tutorial for Istio uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this virtual service and the traffic fields within the virtual service corresponding to the versions. versionInfo : baseline : name : productpage-v1 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight","title":"Specifying weightObjRef"},{"location":"tutorials/istio/rollout-strategies/progressive/#traffic-controls","text":"You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Traffic controls"},{"location":"tutorials/istio/testing-strategies/conformance/","text":"SLO Validation with a single version \u00b6 Scenario: SLO validation with a single version Iter8 enables you to perform SLO validation with a single version of your application (a.k.a. conformance testing ). In this tutorial, you will: Perform conformance testing. Specify latency and error-rate based service-level objectives (SLOs). If your version satisfies SLOs, Iter8 will declare it as the winner. Use Prometheus as the provider for latency and error-rate metrics. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster. 1. Create application version \u00b6 Deploy bookinfo app: kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/conformance/bookinfo-app.yaml Look inside productpage-v1 defined in bookinfo-app.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v1 labels : app : productpage version : v1 spec : replicas : 1 selector : matchLabels : app : productpage version : v1 template : metadata : annotations : sidecar.istio.io/inject : \"true\" prometheus.io/scrape : \"true\" prometheus.io/path : /metrics prometheus.io/port : \"9080\" labels : app : productpage version : v1 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v1\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"red\" - name : reward_min value : \"0\" - name : reward_max value : \"5\" - name : port value : \"9080\" 2. Generate requests \u00b6 Generate requests using Fortio as follows. kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 3. Define metrics \u00b6 Please follow step 3 of the quick start tutorial . 4. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/istio/conformance/experiment.yaml Look inside experiment.yaml apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : conformance-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Conformance test testingPattern : Conformance criteria : objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/istio/conformance/fortio.yaml kubectl delete -f $ITER8 /samples/istio/conformance/experiment.yaml kubectl delete ns bookinfo-iter8","title":"SLO validation (single version)"},{"location":"tutorials/istio/testing-strategies/conformance/#slo-validation-with-a-single-version","text":"Scenario: SLO validation with a single version Iter8 enables you to perform SLO validation with a single version of your application (a.k.a. conformance testing ). In this tutorial, you will: Perform conformance testing. Specify latency and error-rate based service-level objectives (SLOs). If your version satisfies SLOs, Iter8 will declare it as the winner. Use Prometheus as the provider for latency and error-rate metrics. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster.","title":"SLO Validation with a single version"},{"location":"tutorials/istio/testing-strategies/conformance/#1-create-application-version","text":"Deploy bookinfo app: kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/conformance/bookinfo-app.yaml Look inside productpage-v1 defined in bookinfo-app.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v1 labels : app : productpage version : v1 spec : replicas : 1 selector : matchLabels : app : productpage version : v1 template : metadata : annotations : sidecar.istio.io/inject : \"true\" prometheus.io/scrape : \"true\" prometheus.io/path : /metrics prometheus.io/port : \"9080\" labels : app : productpage version : v1 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v1\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"red\" - name : reward_min value : \"0\" - name : reward_max value : \"5\" - name : port value : \"9080\"","title":"1. Create application version"},{"location":"tutorials/istio/testing-strategies/conformance/#2-generate-requests","text":"Generate requests using Fortio as follows. kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"2. Generate requests"},{"location":"tutorials/istio/testing-strategies/conformance/#3-define-metrics","text":"Please follow step 3 of the quick start tutorial .","title":"3. Define metrics"},{"location":"tutorials/istio/testing-strategies/conformance/#4-launch-experiment","text":"kubectl apply -f $ITER8 /samples/istio/conformance/experiment.yaml Look inside experiment.yaml apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : conformance-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Conformance test testingPattern : Conformance criteria : objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8","title":"4. Launch experiment"},{"location":"tutorials/istio/testing-strategies/conformance/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/istio/testing-strategies/conformance/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/istio/conformance/fortio.yaml kubectl delete -f $ITER8 /samples/istio/conformance/experiment.yaml kubectl delete ns bookinfo-iter8","title":"6. Cleanup"},{"location":"tutorials/istio/testing-strategies/hybrid/","text":"Hybrid (A/B + SLOs) Testing \u00b6 Hybrid (A/B + SLOs) testing is documented as part of the Istio quick start tutorial .","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/istio/testing-strategies/hybrid/#hybrid-ab-slos-testing","text":"Hybrid (A/B + SLOs) testing is documented as part of the Istio quick start tutorial .","title":"Hybrid (A/B + SLOs) Testing"},{"location":"tutorials/istio/testing-strategies/slovalidation/","text":"SLO Validation \u00b6 Scenario: SLO validation with progressive traffic shift This tutorial illustrates an SLO validation experiment with two versions ; the candidate version will be promoted after Iter8 validates that it satisfies service-level objectives (SLOs). You will: Specify latency and error-rate based service-level objectives (SLOs). If the candidate version satisfies SLOs, Iter8 will declare it as the winner. Use Prometheus as the provider for latency and error-rate metrics. Combine SLO validation with progressive traffic shifting . Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster. Steps 1 to 3 \u00b6 Follow Steps 1 to 3 of the Iter8 quick start tutorial . 4. Launch experiment \u00b6 Launch the SLO validation experiment. kubectl apply -f $ITER8 /samples/istio/slovalidation/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : slovalidation-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Canary test testingPattern : Canary # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : objectives : # metrics used to validate versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used in metric queries value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used in metric queries value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/slovalidation/experiment.yaml kubectl delete namespace bookinfo-iter8","title":"SLO validation"},{"location":"tutorials/istio/testing-strategies/slovalidation/#slo-validation","text":"Scenario: SLO validation with progressive traffic shift This tutorial illustrates an SLO validation experiment with two versions ; the candidate version will be promoted after Iter8 validates that it satisfies service-level objectives (SLOs). You will: Specify latency and error-rate based service-level objectives (SLOs). If the candidate version satisfies SLOs, Iter8 will declare it as the winner. Use Prometheus as the provider for latency and error-rate metrics. Combine SLO validation with progressive traffic shifting . Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster.","title":"SLO Validation"},{"location":"tutorials/istio/testing-strategies/slovalidation/#steps-1-to-3","text":"Follow Steps 1 to 3 of the Iter8 quick start tutorial .","title":"Steps 1 to 3"},{"location":"tutorials/istio/testing-strategies/slovalidation/#4-launch-experiment","text":"Launch the SLO validation experiment. kubectl apply -f $ITER8 /samples/istio/slovalidation/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : slovalidation-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Canary test testingPattern : Canary # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl -n bookinfo-iter8 apply -f {{ .promote }}\" ] criteria : objectives : # metrics used to validate versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used in metric queries value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used in metric queries value : bookinfo-iter8 - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight","title":"4. Launch experiment"},{"location":"tutorials/istio/testing-strategies/slovalidation/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/istio/testing-strategies/slovalidation/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/slovalidation/experiment.yaml kubectl delete namespace bookinfo-iter8","title":"6. Cleanup"},{"location":"tutorials/kfserving/platform-setup/","text":"Platform Setup for KFServing \u00b6 1. Create Kubernetes cluster \u00b6 Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288 2. Clone Iter8 repo \u00b6 git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) 3. Install KFServing, Iter8 and Telemetry \u00b6 Setup KFServing, Iter8, a mock New Relic service, and Prometheus add-on within your cluster. $ITER8 /samples/kfserving/quickstart/platformsetup.sh","title":"Platform setup"},{"location":"tutorials/kfserving/platform-setup/#platform-setup-for-kfserving","text":"","title":"Platform Setup for KFServing"},{"location":"tutorials/kfserving/platform-setup/#1-create-kubernetes-cluster","text":"Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288","title":"1. Create Kubernetes cluster"},{"location":"tutorials/kfserving/platform-setup/#2-clone-iter8-repo","text":"git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd )","title":"2. Clone Iter8 repo"},{"location":"tutorials/kfserving/platform-setup/#3-install-kfserving-iter8-and-telemetry","text":"Setup KFServing, Iter8, a mock New Relic service, and Prometheus add-on within your cluster. $ITER8 /samples/kfserving/quickstart/platformsetup.sh","title":"3. Install KFServing, Iter8 and Telemetry"},{"location":"tutorials/kfserving/quick-start/","text":"A/B Testing \u00b6 Scenario: A/B testing and progressive traffic shift for KFServing models A/B testing enables you to compare two versions of an ML model, and select a winner based on a (business) reward metric. In this tutorial, you will: Perform A/B testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Combine A/B testing with progressive traffic shifting . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster. 1. Create ML model versions \u00b6 Deploy two KFServing inference services corresponding to two versions of a TensorFlow classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-baseline spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers\" Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-candidate spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers-2\" Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 100 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 0 2. Generate requests \u00b6 Generate requests for your model as follows. Port forward Istio ingress in terminal one INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Send requests in terminal two curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json watch --interval 0 .2 -x curl -v -H \"Host: example.com\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json 3. Define metrics \u00b6 Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. For the purpose of this tutorial, you will mock the user-engagement metric as follows. kubectl apply -f $ITER8 /samples/kfserving/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : v1 kind : Namespace metadata : name : iter8-kfserving --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-kfserving spec : mock : - name : flowers-v1 level : \"15.0\" - name : flowers-v2 level : \"20.0\" Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked metric used in this tutorial with any custom metric you wish to optimize in the A/B test. Documentation on defining custom metrics is here . 4. Launch experiment \u00b6 Launch the A/B testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/kfserving/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 20 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/kfserving/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml","title":"Quick start"},{"location":"tutorials/kfserving/quick-start/#ab-testing","text":"Scenario: A/B testing and progressive traffic shift for KFServing models A/B testing enables you to compare two versions of an ML model, and select a winner based on a (business) reward metric. In this tutorial, you will: Perform A/B testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Combine A/B testing with progressive traffic shifting . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster.","title":"A/B Testing"},{"location":"tutorials/kfserving/quick-start/#1-create-ml-model-versions","text":"Deploy two KFServing inference services corresponding to two versions of a TensorFlow classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-baseline spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers\" Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-candidate spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers-2\" Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 100 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 0","title":"1. Create ML model versions"},{"location":"tutorials/kfserving/quick-start/#2-generate-requests","text":"Generate requests for your model as follows. Port forward Istio ingress in terminal one INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Send requests in terminal two curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json watch --interval 0 .2 -x curl -v -H \"Host: example.com\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json","title":"2. Generate requests"},{"location":"tutorials/kfserving/quick-start/#3-define-metrics","text":"Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. For the purpose of this tutorial, you will mock the user-engagement metric as follows. kubectl apply -f $ITER8 /samples/kfserving/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : v1 kind : Namespace metadata : name : iter8-kfserving --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-kfserving spec : mock : - name : flowers-v1 level : \"15.0\" - name : flowers-v2 level : \"20.0\" Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked metric used in this tutorial with any custom metric you wish to optimize in the A/B test. Documentation on defining custom metrics is here .","title":"3. Define metrics"},{"location":"tutorials/kfserving/quick-start/#4-launch-experiment","text":"Launch the A/B testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/kfserving/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 20 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml","title":"4. Launch experiment"},{"location":"tutorials/kfserving/quick-start/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/kfserving/quick-start/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/kfserving/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml","title":"6. Cleanup"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/","text":"Fixed % Split \u00b6 Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster. 1. Create ML model versions \u00b6 kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 60 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 40 2. Steps 2 and 3 \u00b6 Please follow Steps 2 and 3 of the quick start tutorial . 4. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 20 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml","title":"Fixed-%-split"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#fixed-split","text":"Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster.","title":"Fixed % Split"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#1-create-ml-model-versions","text":"kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 60 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 40","title":"1. Create ML model versions"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#2-steps-2-and-3","text":"Please follow Steps 2 and 3 of the quick start tutorial .","title":"2. Steps 2 and 3"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#4-launch-experiment","text":"kubectl apply -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 20 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml","title":"4. Launch experiment"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml","title":"6. Cleanup"},{"location":"tutorials/kfserving/rollout-strategies/progressive/","text":"Progressive Traffic Shift \u00b6 Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below. Tutorials with progressive traffic shift \u00b6 The A/B testing (quick start) and hybrid (A/B + SLOs) testing tutorials demonstrate progressive traffic shift. Specifying weightObjRef \u00b6 Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The A/B testing quick start tutorial uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight Traffic controls \u00b6 You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Progressive traffic shift"},{"location":"tutorials/kfserving/rollout-strategies/progressive/#progressive-traffic-shift","text":"Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below.","title":"Progressive Traffic Shift"},{"location":"tutorials/kfserving/rollout-strategies/progressive/#tutorials-with-progressive-traffic-shift","text":"The A/B testing (quick start) and hybrid (A/B + SLOs) testing tutorials demonstrate progressive traffic shift.","title":"Tutorials with progressive traffic shift"},{"location":"tutorials/kfserving/rollout-strategies/progressive/#specifying-weightobjref","text":"Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The A/B testing quick start tutorial uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight","title":"Specifying weightObjRef"},{"location":"tutorials/kfserving/rollout-strategies/progressive/#traffic-controls","text":"You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Traffic controls"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/","text":"Session Affinity \u00b6 Scenario: Canary rollout with session affinity Session affinity ensures that the version to which a particular user's request is routed remains consistent throughout the duration of the experiment. In this tutorial, you will use an experiment involving two user groups, 1 and 2. Reqeusts from user group 1 will have a userhash header value prefixed with 111 and will be routed to the baseline version. Requests from user group 2 will have a userhash header value prefixed with 101 and will be routed to the candidate version. The experiment is shown below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster. 1. Create ML model versions \u00b6 Deploy two KFServing inference services corresponding to two versions of a TensorFlow classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Istio virtual service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - match : - headers : userhash : # user hash is a 10-digit random binary string prefix : \"101\" # in expectation, 1/8th of user hashes will match this prefix route : # matching users will always go to v2 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 - route : # non-matching users will always go to v1 - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 2. Generate requests \u00b6 Generate requests to your model as follows. Port forward (terminal one) INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Baseline requests (terminal two) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1111100000\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 0 .29 done Candidate requests (terminal three) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1010101010\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 2 .0 done 3. Define metrics \u00b6 Please follow Step 3 of the quick start tutorial . 4. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : session-affinity-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-kfserving/request-count rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-kfserving/mean-latency upperLimit : 2000 - metric : iter8-kfserving/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-kfserving/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml","title":"Session affinity"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#session-affinity","text":"Scenario: Canary rollout with session affinity Session affinity ensures that the version to which a particular user's request is routed remains consistent throughout the duration of the experiment. In this tutorial, you will use an experiment involving two user groups, 1 and 2. Reqeusts from user group 1 will have a userhash header value prefixed with 111 and will be routed to the baseline version. Requests from user group 2 will have a userhash header value prefixed with 101 and will be routed to the candidate version. The experiment is shown below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster.","title":"Session Affinity"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#1-create-ml-model-versions","text":"Deploy two KFServing inference services corresponding to two versions of a TensorFlow classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Istio virtual service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - match : - headers : userhash : # user hash is a 10-digit random binary string prefix : \"101\" # in expectation, 1/8th of user hashes will match this prefix route : # matching users will always go to v2 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 - route : # non-matching users will always go to v1 - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1","title":"1. Create ML model versions"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#2-generate-requests","text":"Generate requests to your model as follows. Port forward (terminal one) INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Baseline requests (terminal two) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1111100000\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 0 .29 done Candidate requests (terminal three) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1010101010\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 2 .0 done","title":"2. Generate requests"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#3-define-metrics","text":"Please follow Step 3 of the quick start tutorial .","title":"3. Define metrics"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#4-launch-experiment","text":"kubectl apply -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : session-affinity-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-kfserving/request-count rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-kfserving/mean-latency upperLimit : 2000 - metric : iter8-kfserving/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-kfserving/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml","title":"4. Launch experiment"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml","title":"6. Cleanup"},{"location":"tutorials/kfserving/testing-strategies/ab/","text":"A/B Testing \u00b6 The quick start tutorial for KFServing demonstrates A/B testing.","title":"A/B testing (quick start)"},{"location":"tutorials/kfserving/testing-strategies/ab/#ab-testing","text":"The quick start tutorial for KFServing demonstrates A/B testing.","title":"A/B Testing"},{"location":"tutorials/kfserving/testing-strategies/hybrid/","text":"Hybrid (A/B + SLOs) testing \u00b6 Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift of KFServing models Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. Specify latency and error-rate based objectives, for which data will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. 1. Steps 1, 2, and 3 \u00b6 Follow Steps 1, 2, and 3 of the KFServing quick start tutorial . 4. Define metrics \u00b6 kubectl apply -f $ITER8 /samples/kfserving/hybrid/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 apiVersion : v1 kind : Namespace metadata : name : iter8-kfserving --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-kfserving spec : mock : - name : flowers-v1 level : \"15.0\" - name : flowers-v2 level : \"20.0\" --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-kfserving spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(revision_app_request_latencies_bucket{namespace_name='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-kfserving spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-kfserving spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-kfserving spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_sum{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-kfserving spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query 5. Launch experiment \u00b6 Launch the hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. kubectl apply -f $ITER8 /samples/kfserving/hybrid/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : hybrid-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 20 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml 6. Observe experiment \u00b6 Follow these steps to observe your experiment. 7. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/kfserving/hybrid/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#hybrid-ab-slos-testing","text":"Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift of KFServing models Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. Specify latency and error-rate based objectives, for which data will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below.","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#1-steps-1-2-and-3","text":"Follow Steps 1, 2, and 3 of the KFServing quick start tutorial .","title":"1. Steps 1, 2, and 3"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#4-define-metrics","text":"kubectl apply -f $ITER8 /samples/kfserving/hybrid/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 apiVersion : v1 kind : Namespace metadata : name : iter8-kfserving --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-kfserving spec : mock : - name : flowers-v1 level : \"15.0\" - name : flowers-v2 level : \"20.0\" --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-kfserving spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(revision_app_request_latencies_bucket{namespace_name='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-kfserving spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-kfserving spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-kfserving spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_sum{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-kfserving spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query","title":"4. Define metrics"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#5-launch-experiment","text":"Launch the hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. kubectl apply -f $ITER8 /samples/kfserving/hybrid/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : hybrid-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 20 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml","title":"5. Launch experiment"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#6-observe-experiment","text":"Follow these steps to observe your experiment.","title":"6. Observe experiment"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#7-cleanup","text":"kubectl delete -f $ITER8 /samples/kfserving/hybrid/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml","title":"7. Cleanup"},{"location":"tutorials/knative/setup-for-tutorials/","text":"Setup For Tutorials \u00b6 Install Knative \u00b6 For production installation of Knative, refer to the official Knative instructions . For exercising Iter8 tutorials, install Knative as follows. Clone Iter8 repo git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) Install Knative serving Unless Istio is especially mentioned as a requirement, we recommend Kourier as the Knative networking layer for Iter8 tutorials. Kourier $ITER8 /samples/knative/quickstart/platform-setup.sh kourier Istio $ITER8 /samples/knative/quickstart/platform-setup.sh istio","title":"Setup for tutorials"},{"location":"tutorials/knative/setup-for-tutorials/#setup-for-tutorials","text":"","title":"Setup For Tutorials"},{"location":"tutorials/knative/setup-for-tutorials/#install-knative","text":"For production installation of Knative, refer to the official Knative instructions . For exercising Iter8 tutorials, install Knative as follows. Clone Iter8 repo git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) Install Knative serving Unless Istio is especially mentioned as a requirement, we recommend Kourier as the Knative networking layer for Iter8 tutorials. Kourier $ITER8 /samples/knative/quickstart/platform-setup.sh kourier Istio $ITER8 /samples/knative/quickstart/platform-setup.sh istio","title":"Install Knative"},{"location":"tutorials/knative/slovalidation-helmex-gitops/","text":"SLO Validation (Helmex, GitOps) \u00b6 Scenario: Safely rollout new version of a Knative app with SLO validation This tutorial builds on the Knative Helmex tutorial for SLO validation , and illustrates the Helmex pattern in the context of GitOps. Setup K8s cluster with Knative and local environment Follow the setup in the Knative Helmex tutorial for SLO validation . If you haven't done so already, try the Knative Helmex tutorial for SLO validation , and cleanup. This will promote a better understanding of the current tutorial. 1. Fork and clone \u00b6 Fork the Iter8 GitHub repo . Clone your fork and set the $ITER8 environment variable as follows: export USERNAME = <your GitHub username> git clone git@github.com: $USERNAME /iter8.git cd iter8 export BRANCH=gitops-test git checkout -b $BRANCH export ITER8=$(pwd) 2. Create baseline version \u00b6 Deploy the baseline version of the hello world Knative app using Helm. helm install my-app iter8/knslo-gitops \\ -f https://raw.githubusercontent.com/ $USERNAME /iter8/master/samples/knative/second-exp/values.yaml Verify that baseline version is 1.0.0 as in this tutorial . 3. Enable Iter8 to update Git \u00b6 3.a) Create GitHub token \u00b6 Create a personal access token on GitHub . In Step 8 of this process, select repo . This will ensure that the token can be used by Iter8 to update the values.yaml file in GitHub. 3.b) Create K8s secret \u00b6 # replace $GHTOKEN with GitHub token created above kubectl create secret generic ghtoken --from-literal = token = $GHTOKEN 3.c) Provide RBAC permission \u00b6 kubectl create role ghtoken-reader \\ --verb = get \\ --resource = secrets \\ --resource-name = ghtoken kubectl create rolebinding ghtoken-reader-binding \\ --role = ghtoken-reader \\ --serviceaccount = iter8-system:iter8-handlers Iter8 can now read the GitHub token. 4. Create candidate version \u00b6 When a new candidate arrives, a deployment pipeline would typically update the values.yaml file in the GitHub repo. In this tutorial, simulate this pipeline as follows. 4.a) Update values.yaml locally \u00b6 cat <<EOF > $ITER8/samples/knative/second-exp/values.yaml common: application: hello repo: \"gcr.io/google-samples/hello-app\" baseline: dynamic: tag: \"1.0\" id: \"v1\" candidate: dynamic: tag: \"2.0\" id: \"v2\" knslo-gitops-exp: experiment: # Iter8 will update this values.yaml file in the $BRANCH branch of your repo helmexGitOps: gitRepo: \"https://github.com/$USERNAME/iter8.git\" filePath: \"samples/knative/second-exp/values.yaml\" username: $USERNAME branch: $BRANCH EOF 4.b) Git push \u00b6 git commit -a -m \"update values.yaml with candidate version\" --allow-empty git push origin $BRANCH -f 4.c) Helm upgrade \u00b6 Deploy the candidate version of the hello world application using Helm. helm upgrade my-app iter8/knslo-gitops \\ -f https://raw.githubusercontent.com/ $USERNAME /iter8/ $BRANCH /samples/knative/second-exp/values.yaml \\ --install View application and experiment resources, and verify candidate as in this tutorial . 5. Observe experiment \u00b6 Describe the results of the Iter8 experiment as in this tutorial . 6. Promote winner \u00b6 6.a) Assert winner \u00b6 Assert that the experiment completed and found a winning version. If the conditions are not satisfied, try again after a few seconds. iter8ctl assert -c completed -c winnerFound When the Iter8 experiment completes, assuming the candidate version is the winner (this is the expected case), Iter8 would have automatically promoted the candidate version in the GitHub values.yaml file as follows. Content of values.yaml after promotion by Iter8 common : application : hello repo : \"gcr.io/google-samples/hello-app\" baseline : dynamic : tag : \"2.0\" id : \"v2\" experiment : # Iter8 will update this values.yaml file in the $BRANCH branch of your repo helmexGitops : repo : \"https://github.com/$USERNAME/iter8.git\" path : \"samples/knative/second-exp/values.yaml\" branch : $BRANCH username : $USERNAME 6.b) Helm upgrade \u00b6 # replace $USERNAME with your GitHub username helm upgrade my-app iter8/knslo-gitops \\ -f https://raw.githubusercontent.com/ $USERNAME /iter8/gitops-test/samples/first-exp/helm/values.yaml --install Verify that baseline version is 2.0.0 as in this tutorial . 7. Cleanup \u00b6 helm uninstall my-app git branch -D gitops-test git push -D origin gitops-test Next Steps Use in production The knslo-gitops Helm chart comprises of the kn-hello-world and knslo-gitops-exp sub-charts. These sub-charts are located in the $ITER8/helm folder. Modify them as needed by your application for production usage. Use with ArgoCD The knslo-gitops Helm chart comprises of the kn-hello-world and knslo-gitops-exp sub-charts. These sub-charts are located in the $ITER8/helm folder. Modify them as needed by your application for production usage. Use with Flux The knslo-gitops Helm chart comprises of the kn-hello-world and knslo-gitops-exp sub-charts. These sub-charts are located in the $ITER8/helm folder. Modify them as needed by your application for production usage. Use with GitHub Actions The knslo-gitops Helm chart comprises of the kn-hello-world and knslo-gitops-exp sub-charts. These sub-charts are located in the $ITER8/helm folder. Modify them as needed by your application for production usage. Try other Iter8 Knative tutorials SLO validation with progressive traffic shift Hybrid testing Fixed traffic split User segmentation based on HTTP headers","title":"SLO validation (Helmex, GitOps)"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#slo-validation-helmex-gitops","text":"Scenario: Safely rollout new version of a Knative app with SLO validation This tutorial builds on the Knative Helmex tutorial for SLO validation , and illustrates the Helmex pattern in the context of GitOps. Setup K8s cluster with Knative and local environment Follow the setup in the Knative Helmex tutorial for SLO validation . If you haven't done so already, try the Knative Helmex tutorial for SLO validation , and cleanup. This will promote a better understanding of the current tutorial.","title":"SLO Validation (Helmex, GitOps)"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#1-fork-and-clone","text":"Fork the Iter8 GitHub repo . Clone your fork and set the $ITER8 environment variable as follows: export USERNAME = <your GitHub username> git clone git@github.com: $USERNAME /iter8.git cd iter8 export BRANCH=gitops-test git checkout -b $BRANCH export ITER8=$(pwd)","title":"1. Fork and clone"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#2-create-baseline-version","text":"Deploy the baseline version of the hello world Knative app using Helm. helm install my-app iter8/knslo-gitops \\ -f https://raw.githubusercontent.com/ $USERNAME /iter8/master/samples/knative/second-exp/values.yaml Verify that baseline version is 1.0.0 as in this tutorial .","title":"2. Create baseline version"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#3-enable-iter8-to-update-git","text":"","title":"3. Enable Iter8 to update Git"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#3a-create-github-token","text":"Create a personal access token on GitHub . In Step 8 of this process, select repo . This will ensure that the token can be used by Iter8 to update the values.yaml file in GitHub.","title":"3.a) Create GitHub token"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#3b-create-k8s-secret","text":"# replace $GHTOKEN with GitHub token created above kubectl create secret generic ghtoken --from-literal = token = $GHTOKEN","title":"3.b) Create K8s secret"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#3c-provide-rbac-permission","text":"kubectl create role ghtoken-reader \\ --verb = get \\ --resource = secrets \\ --resource-name = ghtoken kubectl create rolebinding ghtoken-reader-binding \\ --role = ghtoken-reader \\ --serviceaccount = iter8-system:iter8-handlers Iter8 can now read the GitHub token.","title":"3.c) Provide RBAC permission"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#4-create-candidate-version","text":"When a new candidate arrives, a deployment pipeline would typically update the values.yaml file in the GitHub repo. In this tutorial, simulate this pipeline as follows.","title":"4. Create candidate version"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#4a-update-valuesyaml-locally","text":"cat <<EOF > $ITER8/samples/knative/second-exp/values.yaml common: application: hello repo: \"gcr.io/google-samples/hello-app\" baseline: dynamic: tag: \"1.0\" id: \"v1\" candidate: dynamic: tag: \"2.0\" id: \"v2\" knslo-gitops-exp: experiment: # Iter8 will update this values.yaml file in the $BRANCH branch of your repo helmexGitOps: gitRepo: \"https://github.com/$USERNAME/iter8.git\" filePath: \"samples/knative/second-exp/values.yaml\" username: $USERNAME branch: $BRANCH EOF","title":"4.a) Update values.yaml locally"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#4b-git-push","text":"git commit -a -m \"update values.yaml with candidate version\" --allow-empty git push origin $BRANCH -f","title":"4.b) Git push"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#4c-helm-upgrade","text":"Deploy the candidate version of the hello world application using Helm. helm upgrade my-app iter8/knslo-gitops \\ -f https://raw.githubusercontent.com/ $USERNAME /iter8/ $BRANCH /samples/knative/second-exp/values.yaml \\ --install View application and experiment resources, and verify candidate as in this tutorial .","title":"4.c) Helm upgrade"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#5-observe-experiment","text":"Describe the results of the Iter8 experiment as in this tutorial .","title":"5. Observe experiment"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#6-promote-winner","text":"","title":"6. Promote winner"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#6a-assert-winner","text":"Assert that the experiment completed and found a winning version. If the conditions are not satisfied, try again after a few seconds. iter8ctl assert -c completed -c winnerFound When the Iter8 experiment completes, assuming the candidate version is the winner (this is the expected case), Iter8 would have automatically promoted the candidate version in the GitHub values.yaml file as follows. Content of values.yaml after promotion by Iter8 common : application : hello repo : \"gcr.io/google-samples/hello-app\" baseline : dynamic : tag : \"2.0\" id : \"v2\" experiment : # Iter8 will update this values.yaml file in the $BRANCH branch of your repo helmexGitops : repo : \"https://github.com/$USERNAME/iter8.git\" path : \"samples/knative/second-exp/values.yaml\" branch : $BRANCH username : $USERNAME","title":"6.a) Assert winner"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#6b-helm-upgrade","text":"# replace $USERNAME with your GitHub username helm upgrade my-app iter8/knslo-gitops \\ -f https://raw.githubusercontent.com/ $USERNAME /iter8/gitops-test/samples/first-exp/helm/values.yaml --install Verify that baseline version is 2.0.0 as in this tutorial .","title":"6.b) Helm upgrade"},{"location":"tutorials/knative/slovalidation-helmex-gitops/#7-cleanup","text":"helm uninstall my-app git branch -D gitops-test git push -D origin gitops-test Next Steps Use in production The knslo-gitops Helm chart comprises of the kn-hello-world and knslo-gitops-exp sub-charts. These sub-charts are located in the $ITER8/helm folder. Modify them as needed by your application for production usage. Use with ArgoCD The knslo-gitops Helm chart comprises of the kn-hello-world and knslo-gitops-exp sub-charts. These sub-charts are located in the $ITER8/helm folder. Modify them as needed by your application for production usage. Use with Flux The knslo-gitops Helm chart comprises of the kn-hello-world and knslo-gitops-exp sub-charts. These sub-charts are located in the $ITER8/helm folder. Modify them as needed by your application for production usage. Use with GitHub Actions The knslo-gitops Helm chart comprises of the kn-hello-world and knslo-gitops-exp sub-charts. These sub-charts are located in the $ITER8/helm folder. Modify them as needed by your application for production usage. Try other Iter8 Knative tutorials SLO validation with progressive traffic shift Hybrid testing Fixed traffic split User segmentation based on HTTP headers","title":"7. Cleanup"},{"location":"tutorials/knative/slovalidation-helmex/","text":"SLO validation (Helmex) \u00b6 Scenario: Safely rollout new version of a Knative app with SLO validation Dark launch a candidate version of your Knative application, validate that the candidate satisfies latency and error-based objectives (SLOs) , and promote the candidate. This tutorial illustrates the Helmex pattern . Setup K8s cluster with Knative and local environment Get Helm 3+ Setup K8s cluster . If you wish to use the Istio networking layer for Knative, ensure that the cluster has sufficient resources Install Knative in K8s cluster . This tutorial assumes Knative with Kourier networking layer. Install Iter8 in K8s cluster Get iter8ctl Get the Iter8 Helm repo 1. Create baseline version \u00b6 Deploy the baseline version of the hello world Knative app using Helm. helm install my-app iter8/knslo \\ --set baseline.dynamic.tag = \"1.0\" \\ --set baseline.dynamic.id = \"v1\" \\ --set candidate = null Verify that baseline version is 1.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready Port-forward the ingress service for Knative. Choose the networking layer used by your Knative installation. Kourier # do this in a separate terminal kubectl port-forward svc/kourier -n knative-serving 8080 :80 Istio # do this in a separate terminal kubectl port-forward svc/istio-ingressgateway -n istio-system 8080 :80 curl localhost:8080 -H \"Host: hello.default.example.com\" # output will be similar to the following (notice 1.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 1.0.0 Hostname: hello-bc95d9b56-xp9kv 2. Create candidate version \u00b6 Deploy the candidate version of the hello world application using Helm. helm upgrade my-app iter8/knslo \\ --set baseline.dynamic.tag = \"1.0\" \\ --set baseline.dynamic.id = \"v1\" \\ --set candidate.dynamic.tag = \"2.0\" \\ --set candidate.dynamic.id = \"v2\" \\ --install The above command creates an Iter8 experiment alongside the candidate version of the hello world application. The experiment will collect latency and error rate metrics for the candidate, and verify that it satisfies the mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. View application and experiment resources Use the command below to view your application and Iter8 experiment resources. helm get manifest my-app Verify that candidate version is 2.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready # this command reuses the port-forward from the first step curl localhost:8080 -H \"Host: candidate-hello.default.example.com\" # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv 3. Observe experiment \u00b6 Describe the results of the Iter8 experiment. Wait ~1 min before trying the following command. If the output is not as expected, try again after a few seconds. iter8ctl describe Experiment results will look similar to this ... ****** Overview ****** Experiment name: my-experiment Experiment namespace: default Target: my-app Testing pattern: Conformance Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Completed Number of completed iterations: 1 ****** Winner Assessment ****** > If the version being validated ; i.e., the baseline version, satisfies the experiment objectives, it is the winner. > Otherwise, there is no winner. Winning version: my-app ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------+--------+ | OBJECTIVE | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency < = | true | | 50 .000 | | +--------------------------------------+--------+ | iter8-system/error-rate < = | true | | 0 .000 | | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | true | | < = 100 .000 | | +--------------------------------------+--------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------+--------+ | METRIC | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency | 1 .233 | +--------------------------------------+--------+ | iter8-system/error-rate | 0 .000 | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | 2 .311 | +--------------------------------------+--------+ | iter8-system/request-count | 40 .000 | +--------------------------------------+--------+ | iter8-system/error-count | 0 .000 | +--------------------------------------+--------+ 4. Promote winner \u00b6 Assert that the experiment completed and found a winning version. If the conditions are not satisfied, try again after a few seconds. iter8ctl assert -c completed -c winnerFound Promote the winner as follows. helm upgrade my-app iter8/knslo \\ --set baseline.dynamic.tag = \"2.0\" \\ --set baseline.dynamic.id = \"v2\" \\ --set candidate = null \\ --install Verify that baseline version is 2.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready curl localhost:8080 -H \"Host: hello.default.example.com\" # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv 5. Cleanup \u00b6 helm uninstall my-app Next Steps Use in production The source for the Hello World Helm application chart is located below. #ITER8 is the root folder for the Iter8 GitHub repo $ITER8 /helm/knslo The source for the Iter8 experiment sub-chart used by the above chart is located below. $ITER8 /helm/knslo-exp Modify the application chart, and optionally modify the experiment chart, for production usage. Try other Iter8 Knative tutorials SLO validation with progressive traffic shift Hybrid testing Fixed traffic split User segmentation based on HTTP headers","title":"SLO validation (Helmex)"},{"location":"tutorials/knative/slovalidation-helmex/#slo-validation-helmex","text":"Scenario: Safely rollout new version of a Knative app with SLO validation Dark launch a candidate version of your Knative application, validate that the candidate satisfies latency and error-based objectives (SLOs) , and promote the candidate. This tutorial illustrates the Helmex pattern . Setup K8s cluster with Knative and local environment Get Helm 3+ Setup K8s cluster . If you wish to use the Istio networking layer for Knative, ensure that the cluster has sufficient resources Install Knative in K8s cluster . This tutorial assumes Knative with Kourier networking layer. Install Iter8 in K8s cluster Get iter8ctl Get the Iter8 Helm repo","title":"SLO validation (Helmex)"},{"location":"tutorials/knative/slovalidation-helmex/#1-create-baseline-version","text":"Deploy the baseline version of the hello world Knative app using Helm. helm install my-app iter8/knslo \\ --set baseline.dynamic.tag = \"1.0\" \\ --set baseline.dynamic.id = \"v1\" \\ --set candidate = null Verify that baseline version is 1.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready Port-forward the ingress service for Knative. Choose the networking layer used by your Knative installation. Kourier # do this in a separate terminal kubectl port-forward svc/kourier -n knative-serving 8080 :80 Istio # do this in a separate terminal kubectl port-forward svc/istio-ingressgateway -n istio-system 8080 :80 curl localhost:8080 -H \"Host: hello.default.example.com\" # output will be similar to the following (notice 1.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 1.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"1. Create baseline version"},{"location":"tutorials/knative/slovalidation-helmex/#2-create-candidate-version","text":"Deploy the candidate version of the hello world application using Helm. helm upgrade my-app iter8/knslo \\ --set baseline.dynamic.tag = \"1.0\" \\ --set baseline.dynamic.id = \"v1\" \\ --set candidate.dynamic.tag = \"2.0\" \\ --set candidate.dynamic.id = \"v2\" \\ --install The above command creates an Iter8 experiment alongside the candidate version of the hello world application. The experiment will collect latency and error rate metrics for the candidate, and verify that it satisfies the mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. View application and experiment resources Use the command below to view your application and Iter8 experiment resources. helm get manifest my-app Verify that candidate version is 2.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready # this command reuses the port-forward from the first step curl localhost:8080 -H \"Host: candidate-hello.default.example.com\" # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"2. Create candidate version"},{"location":"tutorials/knative/slovalidation-helmex/#3-observe-experiment","text":"Describe the results of the Iter8 experiment. Wait ~1 min before trying the following command. If the output is not as expected, try again after a few seconds. iter8ctl describe Experiment results will look similar to this ... ****** Overview ****** Experiment name: my-experiment Experiment namespace: default Target: my-app Testing pattern: Conformance Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Completed Number of completed iterations: 1 ****** Winner Assessment ****** > If the version being validated ; i.e., the baseline version, satisfies the experiment objectives, it is the winner. > Otherwise, there is no winner. Winning version: my-app ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------+--------+ | OBJECTIVE | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency < = | true | | 50 .000 | | +--------------------------------------+--------+ | iter8-system/error-rate < = | true | | 0 .000 | | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | true | | < = 100 .000 | | +--------------------------------------+--------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------+--------+ | METRIC | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency | 1 .233 | +--------------------------------------+--------+ | iter8-system/error-rate | 0 .000 | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | 2 .311 | +--------------------------------------+--------+ | iter8-system/request-count | 40 .000 | +--------------------------------------+--------+ | iter8-system/error-count | 0 .000 | +--------------------------------------+--------+","title":"3. Observe experiment"},{"location":"tutorials/knative/slovalidation-helmex/#4-promote-winner","text":"Assert that the experiment completed and found a winning version. If the conditions are not satisfied, try again after a few seconds. iter8ctl assert -c completed -c winnerFound Promote the winner as follows. helm upgrade my-app iter8/knslo \\ --set baseline.dynamic.tag = \"2.0\" \\ --set baseline.dynamic.id = \"v2\" \\ --set candidate = null \\ --install Verify that baseline version is 2.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready curl localhost:8080 -H \"Host: hello.default.example.com\" # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"4. Promote winner"},{"location":"tutorials/knative/slovalidation-helmex/#5-cleanup","text":"helm uninstall my-app Next Steps Use in production The source for the Hello World Helm application chart is located below. #ITER8 is the root folder for the Iter8 GitHub repo $ITER8 /helm/knslo The source for the Iter8 experiment sub-chart used by the above chart is located below. $ITER8 /helm/knslo-exp Modify the application chart, and optionally modify the experiment chart, for production usage. Try other Iter8 Knative tutorials SLO validation with progressive traffic shift Hybrid testing Fixed traffic split User segmentation based on HTTP headers","title":"5. Cleanup"},{"location":"tutorials/knative/rollout-strategies/fixed-split/","text":"Fixed % Split \u00b6 Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster. 1. Create versions and fix traffic split \u00b6 kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Knative service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 60 - tag : candidate latestRevision : true percent : 40 2. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/knative/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : Canary deploymentPattern : FixedSplit actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-system/request-count objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" duration : maxLoops : 10 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 variables : - name : promote value : baseline candidates : - name : sample-app-v2 variables : - name : promote value : candidate 3. Observe experiment \u00b6 Follow these steps to observe your experiment. 4. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/fixed-split/experiment.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml","title":"Fixed-%-split"},{"location":"tutorials/knative/rollout-strategies/fixed-split/#fixed-split","text":"Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster.","title":"Fixed % Split"},{"location":"tutorials/knative/rollout-strategies/fixed-split/#1-create-versions-and-fix-traffic-split","text":"kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Knative service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 60 - tag : candidate latestRevision : true percent : 40","title":"1. Create versions and fix traffic split"},{"location":"tutorials/knative/rollout-strategies/fixed-split/#2-launch-experiment","text":"kubectl apply -f $ITER8 /samples/knative/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : Canary deploymentPattern : FixedSplit actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-system/request-count objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" duration : maxLoops : 10 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 variables : - name : promote value : baseline candidates : - name : sample-app-v2 variables : - name : promote value : candidate","title":"2. Launch experiment"},{"location":"tutorials/knative/rollout-strategies/fixed-split/#3-observe-experiment","text":"Follow these steps to observe your experiment.","title":"3. Observe experiment"},{"location":"tutorials/knative/rollout-strategies/fixed-split/#4-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/fixed-split/experiment.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml","title":"4. Cleanup"},{"location":"tutorials/knative/rollout-strategies/progressive/","text":"Progressive Traffic Shift \u00b6 Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below. Tutorials with progressive traffic shift \u00b6 The SLO validation and hybrid testing tutorials demonstrate progressive traffic shift. Specifying weightObjRef \u00b6 Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The SLO validation experiment uses a Knative service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Knative service and the traffic fields within the Knative service corresponding to the versions. versionInfo : baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent Traffic controls \u00b6 You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Progressive traffic shift"},{"location":"tutorials/knative/rollout-strategies/progressive/#progressive-traffic-shift","text":"Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below.","title":"Progressive Traffic Shift"},{"location":"tutorials/knative/rollout-strategies/progressive/#tutorials-with-progressive-traffic-shift","text":"The SLO validation and hybrid testing tutorials demonstrate progressive traffic shift.","title":"Tutorials with progressive traffic shift"},{"location":"tutorials/knative/rollout-strategies/progressive/#specifying-weightobjref","text":"Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The SLO validation experiment uses a Knative service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Knative service and the traffic fields within the Knative service corresponding to the versions. versionInfo : baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent","title":"Specifying weightObjRef"},{"location":"tutorials/knative/rollout-strategies/progressive/#traffic-controls","text":"You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Traffic controls"},{"location":"tutorials/knative/rollout-strategies/user-segmentation/","text":"User Segmentation \u00b6 Scenario: SLO validation with user segmentation and built-in metrics User segmentation is the ability to carve out a specific segment of users for an experiment, leaving the rest of the users unaffected by the experiment. In this tutorial, you will: Segment users into two groups: those from Wakanda, and others. Users from Wakanda will participate in the experiment: specifically, requests originating from Wakanda may be routed to baseline or candidate versions; requests that are originating from outside Wakanda will not participate in the experiment and will be routed to the baseline only. The experiment is shown below. Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster. Note: Knative needs to be installed with the Istio networking layer for this tutorial. 1. Create app versions \u00b6 kubectl apply -f $ITER8 /samples/knative/user-segmentation/services.yaml Look inside services.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v1 namespace : default spec : template : metadata : name : sample-app-v1-blue annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v2 namespace : default spec : template : metadata : name : sample-app-v2-green annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" 2. Create Istio virtual service to segment users \u00b6 kubectl apply -f $ITER8 /samples/knative/user-segmentation/routing-rule.yaml Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-for-wakanda spec : gateways : - mesh - knative-serving/knative-ingress-gateway - knative-serving/knative-local-gateway hosts : - example.com http : - match : - headers : country : exact : wakanda route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default weight : 100 - destination : host : sample-app-v2.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v2-green Host : sample-app-v2.default weight : 0 - route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default 3. Generate requests \u00b6 TEMP_DIR = $( mktemp -d ) cd $TEMP_DIR curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .7.0 sh - istio-1.7.0/bin/istioctl kube-inject -f $ITER8 /samples/knative/user-segmentation/curl.yaml | kubectl create -f - cd $ITER8 Look inside curl.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : batch/v1 kind : Job metadata : name : curl spec : template : spec : activeDeadlineSeconds : 6000 containers : - name : curl-from-gondor image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS example.com -H \"country: gondor\" sleep 1.0 done - name : curl-from-wakanda image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS example.com -H \"country: wakanda\" sleep 0.25 done restartPolicy : Never 4. Create Iter8 experiment \u00b6 kubectl wait --for = condition = Ready ksvc/sample-app-v1 kubectl wait --for = condition = Ready ksvc/sample-app-v2 kubectl apply -f $ITER8 /samples/knative/user-segmentation/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : user-segmentation-exp spec : # this experiment uses the fully-qualified name of the Istio virtual service as the target target : default/routing-for-wakanda strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-count upperLimit : \"0.01\" duration : maxLoops : 10 intervalSeconds : 2 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : sample-app-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[1].weight 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/user-segmentation/experiment.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/curl.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/routing-rule.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/services.yaml","title":"User segmentation"},{"location":"tutorials/knative/rollout-strategies/user-segmentation/#user-segmentation","text":"Scenario: SLO validation with user segmentation and built-in metrics User segmentation is the ability to carve out a specific segment of users for an experiment, leaving the rest of the users unaffected by the experiment. In this tutorial, you will: Segment users into two groups: those from Wakanda, and others. Users from Wakanda will participate in the experiment: specifically, requests originating from Wakanda may be routed to baseline or candidate versions; requests that are originating from outside Wakanda will not participate in the experiment and will be routed to the baseline only. The experiment is shown below. Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster. Note: Knative needs to be installed with the Istio networking layer for this tutorial.","title":"User Segmentation"},{"location":"tutorials/knative/rollout-strategies/user-segmentation/#1-create-app-versions","text":"kubectl apply -f $ITER8 /samples/knative/user-segmentation/services.yaml Look inside services.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v1 namespace : default spec : template : metadata : name : sample-app-v1-blue annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app-v2 namespace : default spec : template : metadata : name : sample-app-v2-green annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"10m\" spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\"","title":"1. Create app versions"},{"location":"tutorials/knative/rollout-strategies/user-segmentation/#2-create-istio-virtual-service-to-segment-users","text":"kubectl apply -f $ITER8 /samples/knative/user-segmentation/routing-rule.yaml Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-for-wakanda spec : gateways : - mesh - knative-serving/knative-ingress-gateway - knative-serving/knative-local-gateway hosts : - example.com http : - match : - headers : country : exact : wakanda route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default weight : 100 - destination : host : sample-app-v2.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v2-green Host : sample-app-v2.default weight : 0 - route : - destination : host : sample-app-v1.default.svc.cluster.local headers : request : set : Knative-Serving-Namespace : default Knative-Serving-Revision : sample-app-v1-blue Host : sample-app-v1.default","title":"2. Create Istio virtual service to segment users"},{"location":"tutorials/knative/rollout-strategies/user-segmentation/#3-generate-requests","text":"TEMP_DIR = $( mktemp -d ) cd $TEMP_DIR curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .7.0 sh - istio-1.7.0/bin/istioctl kube-inject -f $ITER8 /samples/knative/user-segmentation/curl.yaml | kubectl create -f - cd $ITER8 Look inside curl.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : batch/v1 kind : Job metadata : name : curl spec : template : spec : activeDeadlineSeconds : 6000 containers : - name : curl-from-gondor image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS example.com -H \"country: gondor\" sleep 1.0 done - name : curl-from-wakanda image : tutum/curl command : - /bin/sh - -c - | while true; do curl -sS example.com -H \"country: wakanda\" sleep 0.25 done restartPolicy : Never","title":"3. Generate requests"},{"location":"tutorials/knative/rollout-strategies/user-segmentation/#4-create-iter8-experiment","text":"kubectl wait --for = condition = Ready ksvc/sample-app-v1 kubectl wait --for = condition = Ready ksvc/sample-app-v2 kubectl apply -f $ITER8 /samples/knative/user-segmentation/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : user-segmentation-exp spec : # this experiment uses the fully-qualified name of the Istio virtual service as the target target : default/routing-for-wakanda strategy : # this experiment will perform a canary test testingPattern : Canary deploymentPattern : Progressive actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local criteria : # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-count upperLimit : \"0.01\" duration : maxLoops : 10 intervalSeconds : 2 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : sample-app-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-for-wakanda namespace : default fieldPath : .spec.http[0].route[1].weight","title":"4. Create Iter8 experiment"},{"location":"tutorials/knative/rollout-strategies/user-segmentation/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/knative/rollout-strategies/user-segmentation/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/user-segmentation/experiment.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/curl.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/routing-rule.yaml kubectl delete -f $ITER8 /samples/knative/user-segmentation/services.yaml","title":"6. Cleanup"},{"location":"tutorials/knative/testing-strategies/hybrid/","text":"Hybrid (A/B + SLOs) testing \u00b6 Scenario: Hybrid (A/B + SLOs) testing and progressive rollout of Knative services Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. Specify latency and error-rate based objectives; these metrics will be collected using Iter8's built-in metrics collection feature. Combine hybrid (A/B + SLOs) testing with progressive rollout . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster. 1. Create app versions \u00b6 Deploy two versions of a Knative app. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0 2. Define metrics \u00b6 kubectl apply -f $ITER8 /samples/knative/hybrid/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion : v1 kind : Namespace metadata : name : iter8-knative --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-knative spec : params : - name : nrql value : | SELECT average(duration) FROM Sessions WHERE version='$name' SINCE $elapsedTime sec ago description : Average duration of a session type : Gauge headerTemplates : - name : X-Query-Key value : t0p-secret-api-key provider : newrelic jqExpression : \".results[0] | .[] | tonumber\" urlTemplate : https://my-newrelic-service.com mock : - name : sample-app-v1 level : 15.0 - name : sample-app-v2 level : 20.0 Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked user-engagement metric used in this tutorial with any custom metric you wish to optimize in the hybrid (A/B + SLOs) test. Documentation on defining custom metrics is here . 3. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/knative/hybrid/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : hybrid-exp spec : target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" requestCount : iter8-system/request-count duration : maxLoops : 10 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate 4. Observe experiment \u00b6 Follow these steps to observe your experiment. 5. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/hybrid/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/knative/testing-strategies/hybrid/#hybrid-ab-slos-testing","text":"Scenario: Hybrid (A/B + SLOs) testing and progressive rollout of Knative services Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. Specify latency and error-rate based objectives; these metrics will be collected using Iter8's built-in metrics collection feature. Combine hybrid (A/B + SLOs) testing with progressive rollout . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster.","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/knative/testing-strategies/hybrid/#1-create-app-versions","text":"Deploy two versions of a Knative app. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0","title":"1. Create app versions"},{"location":"tutorials/knative/testing-strategies/hybrid/#2-define-metrics","text":"kubectl apply -f $ITER8 /samples/knative/hybrid/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion : v1 kind : Namespace metadata : name : iter8-knative --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-knative spec : params : - name : nrql value : | SELECT average(duration) FROM Sessions WHERE version='$name' SINCE $elapsedTime sec ago description : Average duration of a session type : Gauge headerTemplates : - name : X-Query-Key value : t0p-secret-api-key provider : newrelic jqExpression : \".results[0] | .[] | tonumber\" urlTemplate : https://my-newrelic-service.com mock : - name : sample-app-v1 level : 15.0 - name : sample-app-v2 level : 20.0 Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked user-engagement metric used in this tutorial with any custom metric you wish to optimize in the hybrid (A/B + SLOs) test. Documentation on defining custom metrics is here .","title":"2. Define metrics"},{"location":"tutorials/knative/testing-strategies/hybrid/#3-launch-experiment","text":"kubectl apply -f $ITER8 /samples/knative/hybrid/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : hybrid-exp spec : target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" requestCount : iter8-system/request-count duration : maxLoops : 10 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate","title":"3. Launch experiment"},{"location":"tutorials/knative/testing-strategies/hybrid/#4-observe-experiment","text":"Follow these steps to observe your experiment.","title":"4. Observe experiment"},{"location":"tutorials/knative/testing-strategies/hybrid/#5-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/hybrid/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"5. Cleanup"},{"location":"tutorials/knative/testing-strategies/slovalidation/","text":"SLO Validation \u00b6 Scenario: SLO validation with progressive traffic shift This tutorial illustrates an SLO validation experiment with two versions ; the candidate version will be promoted after Iter8 validates that it satisfies service-level objectives (SLOs). You will: Specify latency and error-rate based service-level objectives (SLOs). If the candidate version satisfies SLOs, Iter8 will declare it as the winner. Use Iter8's built-in capabilities for collecting latency and error-rate metrics. Combine SLO validation with progressive traffic shifting . Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster. 1. Create app versions \u00b6 Deploy two versions of a Knative app. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0 2. Launch experiment \u00b6 Launch the SLO validation experiment. This experiment will generate requests for your application versions, collect latency and error-rate metrics, and progressively shift traffic and promote the candidate version after verifying that it satisfies SLOs. kubectl apply -f $ITER8 /samples/knative/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : default/sample-app strategy : testingPattern : Canary deploymentPattern : Progressive actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://current-sample-app.default.svc.cluster.local - name : sample-app-v2 url : http://candidate-sample-app.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-system/request-count objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" duration : maxLoops : 10 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate 3. Observe experiment \u00b6 Follow these steps to observe your experiment. 4. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"SLO validation"},{"location":"tutorials/knative/testing-strategies/slovalidation/#slo-validation","text":"Scenario: SLO validation with progressive traffic shift This tutorial illustrates an SLO validation experiment with two versions ; the candidate version will be promoted after Iter8 validates that it satisfies service-level objectives (SLOs). You will: Specify latency and error-rate based service-level objectives (SLOs). If the candidate version satisfies SLOs, Iter8 will declare it as the winner. Use Iter8's built-in capabilities for collecting latency and error-rate metrics. Combine SLO validation with progressive traffic shifting . Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster.","title":"SLO Validation"},{"location":"tutorials/knative/testing-strategies/slovalidation/#1-create-app-versions","text":"Deploy two versions of a Knative app. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0","title":"1. Create app versions"},{"location":"tutorials/knative/testing-strategies/slovalidation/#2-launch-experiment","text":"Launch the SLO validation experiment. This experiment will generate requests for your application versions, collect latency and error-rate metrics, and progressively shift traffic and promote the candidate version after verifying that it satisfies SLOs. kubectl apply -f $ITER8 /samples/knative/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : default/sample-app strategy : testingPattern : Canary deploymentPattern : Progressive actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://current-sample-app.default.svc.cluster.local - name : sample-app-v2 url : http://candidate-sample-app.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - task : common/exec # promote the winning version with : cmd : /bin/sh args : - \"-c\" - | kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/{{ .promote }}.yaml criteria : requestCount : iter8-system/request-count objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" duration : maxLoops : 10 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent variables : - name : promote value : baseline candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent variables : - name : promote value : candidate","title":"2. Launch experiment"},{"location":"tutorials/knative/testing-strategies/slovalidation/#3-observe-experiment","text":"Follow these steps to observe your experiment.","title":"3. Observe experiment"},{"location":"tutorials/knative/testing-strategies/slovalidation/#4-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"4. Cleanup"},{"location":"tutorials/linkerd/platform-setup/","text":"Platform Setup for Linkerd \u00b6 1. Create Kubernetes cluster \u00b6 Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288 2. Clone Iter8 repo \u00b6 git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) 3. Install Linkerd and Iter8 \u00b6 Setup Linkerd, Iter8, and the Linkerd Viz extension. $ITER8 /samples/linkerd/quickstart/platformsetup.sh","title":"Platform setup"},{"location":"tutorials/linkerd/platform-setup/#platform-setup-for-linkerd","text":"","title":"Platform Setup for Linkerd"},{"location":"tutorials/linkerd/platform-setup/#1-create-kubernetes-cluster","text":"Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288","title":"1. Create Kubernetes cluster"},{"location":"tutorials/linkerd/platform-setup/#2-clone-iter8-repo","text":"git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd )","title":"2. Clone Iter8 repo"},{"location":"tutorials/linkerd/platform-setup/#3-install-linkerd-and-iter8","text":"Setup Linkerd, Iter8, and the Linkerd Viz extension. $ITER8 /samples/linkerd/quickstart/platformsetup.sh","title":"3. Install Linkerd and Iter8"},{"location":"tutorials/linkerd/quick-start/","text":"A/B Testing \u00b6 Scenario: A/B testing and progressive traffic shift for KFServing models A/B testing enables you to compare two versions of an ML model, and select a winner based on a (business) reward metric. In this tutorial, you will: Perform A/B testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Combine A/B testing with progressive traffic shifting . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8 and Linkerd in your K8s cluster. 1. Create application versions \u00b6 Create a new namespace, enable Linkerd proxy injection, deploy two Hello World applications, and create a traffic split. kubectl create ns test kubectl annotate namespace test linkerd.io/inject = enabled kubectl create deployment web --image = gcr.io/google-samples/hello-app:1.0 -n test kubectl expose deployment web --type = NodePort --port = 8080 -n test kubectl create deployment web2 --image = gcr.io/google-samples/hello-app:2.0 -n test kubectl expose deployment web2 --type = NodePort --port = 8080 -n test kubectl wait --for = condition = Ready pods --all -n test kubectl apply -f $ITER8 /samples/linkerd/quickstart/traffic-split.yaml -n test Look inside traffic-split.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit metadata : name : web-traffic-split spec : service : web backends : - service : web weight : 100 - service : web2 weight : 0 2. Generate requests \u00b6 Generate requests to your app using Fortio as follows. kubectl apply -f $ITER8 /samples/linkerd/quickstart/fortio.yaml Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-allow-initial-errors\" , \"-t\" , \"6000s\" , \"-qps\" , \"16\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : web.test:8080 volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 3. Define metrics \u00b6 Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. For the purpose of this tutorial, you will mock a number of metrics as follows. kubectl apply -f $ITER8 /samples/linkerd/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 apiVersion : v1 kind : Namespace metadata : labels : creator : iter8 stack : linkerd name : iter8-linkerd --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : user-engagement namespace : iter8-linkerd spec : description : Number of error responses type : Gauge mock : - name : web level : 5 - name : web2 level : 10 --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count namespace : iter8-linkerd spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(response_total{status_code=~'5..',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query # (sum(increase(request_total{namespace='$namespace',deployment='$name',direction='inbound',tls='true'}[${elapsedTime}s]))) - (sum(increase(response_total{classification='success',namespace='$namespace',deployment='$name',direction='inbound',tls='true'}[${elapsedTime}s]))) --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate namespace : iter8-linkerd spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_total{status_code=~'5..',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : le5ms-latency-percentile namespace : iter8-linkerd spec : description : Less than 5 ms latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_latency_ms_bucket{le='5',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / sum(increase(response_latency_ms_bucket{le='+Inf',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0) provider : prometheus sampleSize : iter8-linkerd/request-count type : Gauge urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency namespace : iter8-linkerd spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_latency_ms_sum{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : iter8-linkerd spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) provider : prometheus type : Counter urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query 4. Launch experiment \u00b6 Launch the A/B testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/linkerd/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the service under experimentation using its fully qualified name target : test/web-traffic-split strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/bash with : script : kubectl -n test apply -f {{ .promote }} criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-linkerd/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-linkerd/mean-latency upperLimit : 300 - metric : iter8-linkerd/error-rate upperLimit : \"0.01\" requestCount : iter8-linkerd/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : web variables : - name : namespace # used by final action if this version is the winner value : test - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/alan-cha/iter8/linkerd/samples/linkerd/quickstart/vs-for-v1.yaml weightObjRef : apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit namespace : test name : web-traffic-split fieldPath : .spec.backends[0].weight candidates : - name : web2 variables : - name : namespace # used by final action if this version is the winner value : test - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/alan-cha/iter8/linkerd/samples/linkerd/quickstart/vs-for-v2.yaml weightObjRef : apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit namespace : test name : web-traffic-split fieldPath : .spec.backends[1].weight 3. Observe experiment \u00b6 Follow these steps to observe your experiment. 4. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/linkerd/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/linkerd/quickstart/experiment.yaml kubectl delete namespace test","title":"Quick start"},{"location":"tutorials/linkerd/quick-start/#ab-testing","text":"Scenario: A/B testing and progressive traffic shift for KFServing models A/B testing enables you to compare two versions of an ML model, and select a winner based on a (business) reward metric. In this tutorial, you will: Perform A/B testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Combine A/B testing with progressive traffic shifting . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8 and Linkerd in your K8s cluster.","title":"A/B Testing"},{"location":"tutorials/linkerd/quick-start/#1-create-application-versions","text":"Create a new namespace, enable Linkerd proxy injection, deploy two Hello World applications, and create a traffic split. kubectl create ns test kubectl annotate namespace test linkerd.io/inject = enabled kubectl create deployment web --image = gcr.io/google-samples/hello-app:1.0 -n test kubectl expose deployment web --type = NodePort --port = 8080 -n test kubectl create deployment web2 --image = gcr.io/google-samples/hello-app:2.0 -n test kubectl expose deployment web2 --type = NodePort --port = 8080 -n test kubectl wait --for = condition = Ready pods --all -n test kubectl apply -f $ITER8 /samples/linkerd/quickstart/traffic-split.yaml -n test Look inside traffic-split.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit metadata : name : web-traffic-split spec : service : web backends : - service : web weight : 100 - service : web2 weight : 0","title":"1. Create application versions"},{"location":"tutorials/linkerd/quick-start/#2-generate-requests","text":"Generate requests to your app using Fortio as follows. kubectl apply -f $ITER8 /samples/linkerd/quickstart/fortio.yaml Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-allow-initial-errors\" , \"-t\" , \"6000s\" , \"-qps\" , \"16\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : web.test:8080 volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"2. Generate requests"},{"location":"tutorials/linkerd/quick-start/#3-define-metrics","text":"Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. For the purpose of this tutorial, you will mock a number of metrics as follows. kubectl apply -f $ITER8 /samples/linkerd/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 apiVersion : v1 kind : Namespace metadata : labels : creator : iter8 stack : linkerd name : iter8-linkerd --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : user-engagement namespace : iter8-linkerd spec : description : Number of error responses type : Gauge mock : - name : web level : 5 - name : web2 level : 10 --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count namespace : iter8-linkerd spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(response_total{status_code=~'5..',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query # (sum(increase(request_total{namespace='$namespace',deployment='$name',direction='inbound',tls='true'}[${elapsedTime}s]))) - (sum(increase(response_total{classification='success',namespace='$namespace',deployment='$name',direction='inbound',tls='true'}[${elapsedTime}s]))) --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate namespace : iter8-linkerd spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_total{status_code=~'5..',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : le5ms-latency-percentile namespace : iter8-linkerd spec : description : Less than 5 ms latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_latency_ms_bucket{le='5',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / sum(increase(response_latency_ms_bucket{le='+Inf',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0) provider : prometheus sampleSize : iter8-linkerd/request-count type : Gauge urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency namespace : iter8-linkerd spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_latency_ms_sum{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : iter8-linkerd spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) provider : prometheus type : Counter urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query","title":"3. Define metrics"},{"location":"tutorials/linkerd/quick-start/#4-launch-experiment","text":"Launch the A/B testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/linkerd/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the service under experimentation using its fully qualified name target : test/web-traffic-split strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/bash with : script : kubectl -n test apply -f {{ .promote }} criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-linkerd/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-linkerd/mean-latency upperLimit : 300 - metric : iter8-linkerd/error-rate upperLimit : \"0.01\" requestCount : iter8-linkerd/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about the app versions used in this experiment baseline : name : web variables : - name : namespace # used by final action if this version is the winner value : test - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/alan-cha/iter8/linkerd/samples/linkerd/quickstart/vs-for-v1.yaml weightObjRef : apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit namespace : test name : web-traffic-split fieldPath : .spec.backends[0].weight candidates : - name : web2 variables : - name : namespace # used by final action if this version is the winner value : test - name : promote # used by final action if this version is the winner value : https://raw.githubusercontent.com/alan-cha/iter8/linkerd/samples/linkerd/quickstart/vs-for-v2.yaml weightObjRef : apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit namespace : test name : web-traffic-split fieldPath : .spec.backends[1].weight","title":"4. Launch experiment"},{"location":"tutorials/linkerd/quick-start/#3-observe-experiment","text":"Follow these steps to observe your experiment.","title":"3. Observe experiment"},{"location":"tutorials/linkerd/quick-start/#4-cleanup","text":"kubectl delete -f $ITER8 /samples/linkerd/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/linkerd/quickstart/experiment.yaml kubectl delete namespace test","title":"4. Cleanup"},{"location":"tutorials/seldon/platform-setup/","text":"Platform Setup for Seldon \u00b6 1. Create Kubernetes cluster \u00b6 Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288 2. Clone Iter8 repo \u00b6 git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) 3. Install Seldon and Iter8 \u00b6 Setup Seldon Core, Seldon Analytics and Iter8 within your cluster. $ITER8 /samples/seldon/quickstart/platformsetup.sh","title":"Platform setup"},{"location":"tutorials/seldon/platform-setup/#platform-setup-for-seldon","text":"","title":"Platform Setup for Seldon"},{"location":"tutorials/seldon/platform-setup/#1-create-kubernetes-cluster","text":"Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288","title":"1. Create Kubernetes cluster"},{"location":"tutorials/seldon/platform-setup/#2-clone-iter8-repo","text":"git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd )","title":"2. Clone Iter8 repo"},{"location":"tutorials/seldon/platform-setup/#3-install-seldon-and-iter8","text":"Setup Seldon Core, Seldon Analytics and Iter8 within your cluster. $ITER8 /samples/seldon/quickstart/platformsetup.sh","title":"3. Install Seldon and Iter8"},{"location":"tutorials/seldon/quick-start/","text":"Hybrid (A/B + SLOs) testing \u00b6 Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift of Seldon models Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric; data for this metric will be provided by Prometheus. Specify latency and error-rate based objectives; data for these metrics will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Seldon and Iter8 in your K8s cluster. 1. Create ML model versions \u00b6 Deploy two Seldon Deployments corresponding to two versions of an Iris classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/routing-rule.yaml kubectl wait --for = condition = Ready --timeout = 600s pods --all -n ns-baseline kubectl wait --for = condition = Ready --timeout = 600s pods --all -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-baseline spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/sklearn/iris implementation : SKLEARN_SERVER Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-candidate spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/xgboost/iris implementation : XGBOOST_SERVER Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - istio-system/seldon-gateway hosts : - iris.example.com http : - route : - destination : host : iris-default.ns-baseline.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v1 weight : 100 - destination : host : iris-default.ns-candidate.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v2 weight : 0 2. Generate requests \u00b6 Generate requests using Fortio as follows. URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/seldon/quickstart/fortio.yaml | sed \"s/6000s/600s/g\" | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 apiVersion : batch/v1 kind : Job metadata : name : fortio-requests spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"5\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: iris.example.com' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"data\": {\"ndarray\":[[6.8,2.8,4.8,1.4]]}}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/api/v1.0/predictions volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv1-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"0.7\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-baseline/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv2-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"1\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-candidate/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 3. Define metrics \u00b6 Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. Define the Iter8 metrics used in this experiment as follows. kubectl apply -f $ITER8 /samples/seldon/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 apiVersion : v1 kind : Namespace metadata : name : iter8-seldon --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-seldon spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(seldon_api_executor_client_requests_seconds_bucket{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-seldon spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-seldon spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_server_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-seldon spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_client_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-seldon spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-seldon spec : description : Number of feedback requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{service='feedback',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked user-engagement metric used in this tutorial with any custom metric you wish to optimize in the hybrid (A/B + SLOs) test. Documentation on defining custom metrics is here . 4. Launch experiment \u00b6 Launch the hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/seldon/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : iris strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-seldon/request-count rewards : # Business rewards - metric : iter8-seldon/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-seldon/mean-latency upperLimit : 2000 - metric : iter8-seldon/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-seldon/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about model versions used in this experiment baseline : name : iris-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : sid value : iris - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v1.yaml candidates : - name : iris-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : sid value : iris - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v2.yaml 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/seldon/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/candidate.yaml","title":"Quick start"},{"location":"tutorials/seldon/quick-start/#hybrid-ab-slos-testing","text":"Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift of Seldon models Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric; data for this metric will be provided by Prometheus. Specify latency and error-rate based objectives; data for these metrics will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Seldon and Iter8 in your K8s cluster.","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/seldon/quick-start/#1-create-ml-model-versions","text":"Deploy two Seldon Deployments corresponding to two versions of an Iris classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/routing-rule.yaml kubectl wait --for = condition = Ready --timeout = 600s pods --all -n ns-baseline kubectl wait --for = condition = Ready --timeout = 600s pods --all -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-baseline spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/sklearn/iris implementation : SKLEARN_SERVER Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-candidate spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/xgboost/iris implementation : XGBOOST_SERVER Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - istio-system/seldon-gateway hosts : - iris.example.com http : - route : - destination : host : iris-default.ns-baseline.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v1 weight : 100 - destination : host : iris-default.ns-candidate.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v2 weight : 0","title":"1. Create ML model versions"},{"location":"tutorials/seldon/quick-start/#2-generate-requests","text":"Generate requests using Fortio as follows. URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/seldon/quickstart/fortio.yaml | sed \"s/6000s/600s/g\" | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 apiVersion : batch/v1 kind : Job metadata : name : fortio-requests spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"5\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: iris.example.com' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"data\": {\"ndarray\":[[6.8,2.8,4.8,1.4]]}}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/api/v1.0/predictions volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv1-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"0.7\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-baseline/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv2-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"1\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-candidate/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"2. Generate requests"},{"location":"tutorials/seldon/quick-start/#3-define-metrics","text":"Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. Define the Iter8 metrics used in this experiment as follows. kubectl apply -f $ITER8 /samples/seldon/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 apiVersion : v1 kind : Namespace metadata : name : iter8-seldon --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-seldon spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(seldon_api_executor_client_requests_seconds_bucket{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-seldon spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-seldon spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_server_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-seldon spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_client_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-seldon spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-seldon spec : description : Number of feedback requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{service='feedback',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked user-engagement metric used in this tutorial with any custom metric you wish to optimize in the hybrid (A/B + SLOs) test. Documentation on defining custom metrics is here .","title":"3. Define metrics"},{"location":"tutorials/seldon/quick-start/#4-launch-experiment","text":"Launch the hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/seldon/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : iris strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - task : common/exec with : cmd : /bin/bash args : [ \"-c\" , \"kubectl apply -f {{ .promote }}\" ] criteria : requestCount : iter8-seldon/request-count rewards : # Business rewards - metric : iter8-seldon/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-seldon/mean-latency upperLimit : 2000 - metric : iter8-seldon/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-seldon/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 10 versionInfo : # information about model versions used in this experiment baseline : name : iris-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : sid value : iris - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v1.yaml candidates : - name : iris-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : sid value : iris - name : promote value : https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v2.yaml","title":"4. Launch experiment"},{"location":"tutorials/seldon/quick-start/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/seldon/quick-start/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/seldon/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/candidate.yaml","title":"6. Cleanup"},{"location":"tutorials/seldon/rollout-strategies/progressive/","text":"Progressive Traffic Shift \u00b6 Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below. Tutorials with progressive traffic shift \u00b6 The hybrid (A/B + SLOs) testing tutorial demonstrates progressive traffic shift. Specifying weightObjRef \u00b6 Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The hybrid (A/B + SLOs) testing tutorial uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : iris-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : iris-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight Traffic controls \u00b6 You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Progressive traffic shift"},{"location":"tutorials/seldon/rollout-strategies/progressive/#progressive-traffic-shift","text":"Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below.","title":"Progressive Traffic Shift"},{"location":"tutorials/seldon/rollout-strategies/progressive/#tutorials-with-progressive-traffic-shift","text":"The hybrid (A/B + SLOs) testing tutorial demonstrates progressive traffic shift.","title":"Tutorials with progressive traffic shift"},{"location":"tutorials/seldon/rollout-strategies/progressive/#specifying-weightobjref","text":"Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The hybrid (A/B + SLOs) testing tutorial uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : iris-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : iris-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight","title":"Specifying weightObjRef"},{"location":"tutorials/seldon/rollout-strategies/progressive/#traffic-controls","text":"You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Traffic controls"},{"location":"tutorials/seldon/testing-strategies/hybrid/","text":"Hybrid (A/B + SLOs) testing \u00b6 The quick start tutorial for Seldon demonstrates hybrid (A/B + SLOs) testing.","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/seldon/testing-strategies/hybrid/#hybrid-ab-slos-testing","text":"The quick start tutorial for Seldon demonstrates hybrid (A/B + SLOs) testing.","title":"Hybrid (A/B + SLOs) testing"}]}